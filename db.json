{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1528209352008},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1528209352008},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1528209352008},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1528209352008},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1528209352008},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1528209352008},{"_id":"source/_posts/archlinux-hibernate.md","hash":"7df95e139e796af4e1316c6cbcd2c891bcdc0a2a","modified":1528211238002},{"_id":"source/_posts/deploy-blog-by-github-hexo.md","hash":"0718fe54ac5b9a22639aa477463bfe5e9a3a498c","modified":1528211238002},{"_id":"source/_posts/deploy-kubernetes.md","hash":"db6a0fd4e00b611829a6d8132ccdcfc94dffd731","modified":1528211238002},{"_id":"source/_posts/deploy-traefik.md","hash":"3f090a94536cf74400f5bf035a9ad322f5c46389","modified":1528211238002},{"_id":"source/_posts/docker-thin-pool.md","hash":"8c05b0051cbb8418f2f292d3a2066e0bd546a33b","modified":1528211238002},{"_id":"source/_posts/docker-var-lib-docker-directory-move.md","hash":"44b9e7c19a2b93de8c4f8f1540cb4be5e439c7e5","modified":1528211238002},{"_id":"source/_posts/execute-docker-without-sudo.md","hash":"5b469ba56db693427c24714032dbf01532568701","modified":1528211238002},{"_id":"source/_posts/harbor-deploy.md","hash":"ba9af4896555d59272fc2f8d549c54752eb2f375","modified":1528211238002},{"_id":"source/_posts/markdown-sample.md","hash":"38af1d6b3f66e4b6ddd81cb50580c383c90d4a56","modified":1528211238002},{"_id":"source/_posts/kuberntes-rollout-update.md","hash":"37725b035326a52d3d79b00bbc88b3a50e78bc93","modified":1528211238002},{"_id":"source/_posts/kuberntes-heapster-cannot-write-to-influxdb.md","hash":"38422cc1985ea09c0de60f979a0c22e4a47742f3","modified":1528211238002},{"_id":"source/_posts/tuning_conntrack.md","hash":"f94ef77909a392f831b355967decf2f4b5cb3104","modified":1528211238002},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1528209352008},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1528209352008},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1528209352008},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1528209352008},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1528209352008},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1528209352008},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1528209352008},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1528209352008},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1528209352008},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1528209352008},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1528209352008},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1528209352008},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1528209352008},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1528209352008},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1528209352008},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1528209352008},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1528209352008},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1528209352008},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1528209352008},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1528209352008},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1528209352008},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1528209352008},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1528209352008},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1528209352008},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1528209352008},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1528209352008},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1528209352008},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1528209352015},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1528209352015},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1528209352008},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1528209352008},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1528209352008},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1528209352008},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1528209352008},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1528209352011},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1528209352011},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1528209352011},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1528209352015},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1528209352015},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1528209352011},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1528209352011},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1528209352015},{"_id":"public/2018/06/05/archlinux-hibernate/index.html","hash":"e5938cd46f7870b57df40cf508b9faeeea776f91","modified":1528212253412},{"_id":"public/2017/03/09/markdown-sample/index.html","hash":"51e88657ccee5876c64c5853d6fb3d59b882a7fd","modified":1528212253412},{"_id":"public/2017/02/22/docker-var-lib-docker-directory-move/index.html","hash":"48dc612c3fe610749be442931f6a2ea738d7a495","modified":1528212253413},{"_id":"public/2017/02/06/kuberntes-heapster-cannot-write-to-influxdb/index.html","hash":"40ac945b9587cdb96676724cd34ec14070ff1c3b","modified":1528212253413},{"_id":"public/2017/01/18/docker-thin-pool/index.html","hash":"42800656e9710595201f4ccb65804d46b4325fd6","modified":1528212253413},{"_id":"public/2017/01/18/execute-docker-without-sudo/index.html","hash":"95171daf270ce472c577e25f1f8df7621c4331bc","modified":1528212253413},{"_id":"public/archives/index.html","hash":"db5684fb1f9fd4e2134767f3ca85ad1a6954f027","modified":1528212253413},{"_id":"public/archives/page/2/index.html","hash":"7d9e8eaf75c5795d1b90b5b9045c08c70836545b","modified":1528212253413},{"_id":"public/archives/2017/index.html","hash":"22d67f15acd91e6b8010a758c6ddc4c62b08eb53","modified":1528212253413},{"_id":"public/archives/2017/page/2/index.html","hash":"82072c6e679e72c4c4e638f907864ec473ddd333","modified":1528212253413},{"_id":"public/archives/2017/01/index.html","hash":"4f856e16f3a0e7e7e8fb37549571fce3bbfe3010","modified":1528212253414},{"_id":"public/archives/2017/02/index.html","hash":"a3fabbf9087869bd379e443d31c9827b7fb9d20d","modified":1528212253414},{"_id":"public/archives/2017/03/index.html","hash":"e3f8975df8791461ba8c15d2c2889b87d888d65e","modified":1528212253414},{"_id":"public/archives/2017/04/index.html","hash":"3d7ed10beeab54ca5ed0bbdcd02de5c41c5b94cb","modified":1528212253414},{"_id":"public/archives/2017/05/index.html","hash":"c664498043340eedc1c3cc08f874e0be4651ce86","modified":1528212253414},{"_id":"public/archives/2018/index.html","hash":"2fb9a2e72d8eed0a9bf4f24784a2265f9f266413","modified":1528212253414},{"_id":"public/archives/2018/06/index.html","hash":"83eb0e709d749892328437ef12360f00a4a4b72f","modified":1528212253414},{"_id":"public/tags/arch/index.html","hash":"9e6ee742c679152354f88a4515168a6e105984e4","modified":1528212253414},{"_id":"public/tags/linux/index.html","hash":"02dbdb72bbe00a184f46e17e3c4c2c12bc5dc7c6","modified":1528212253414},{"_id":"public/tags/hibernate/index.html","hash":"191e8b9d16b41207e294f649d65dcdb4618b98c0","modified":1528212253414},{"_id":"public/tags/github/index.html","hash":"8f4a3390637eff2a0bd98c667071e57b37ba7b72","modified":1528212253414},{"_id":"public/tags/hexo/index.html","hash":"518585f9538bab5b98e950c071b8e6021354a59f","modified":1528212253414},{"_id":"public/tags/blog/index.html","hash":"2773172ee2febe1b4dc679f89f8cceddf1b60f3f","modified":1528212253415},{"_id":"public/tags/docker/index.html","hash":"3bad25587c8b6203624a3d9e3a9f883782fc2682","modified":1528212253415},{"_id":"public/tags/kubernetes/index.html","hash":"669ae9667f9fdc7d88ee34621ac39a7e6f6a7254","modified":1528212253415},{"_id":"public/tags/traefik/index.html","hash":"98d7bf6610ed5be88a6055e3abdb3ffcf3b6f765","modified":1528212253415},{"_id":"public/tags/harbor/index.html","hash":"f820397e0fee90463a0f9f217dc895142415c15f","modified":1528212253415},{"_id":"public/tags/registry/index.html","hash":"2ac8a1899e2ce7815203f87d5ede7bdf9c94a463","modified":1528212253415},{"_id":"public/tags/markdown/index.html","hash":"e0e7c41c12067ff4925f7e3e258111267fdbe118","modified":1528212253415},{"_id":"public/tags/rollout/index.html","hash":"53531502318e93b27c73a0e7a7ebc4d17be92dfd","modified":1528212253415},{"_id":"public/tags/heapster/index.html","hash":"27de642782ed5d39b28d7e3645aced2f280b3a25","modified":1528212253415},{"_id":"public/tags/iptables/index.html","hash":"b792e4a1579f7f044c8e99e2fc155daf9ce06615","modified":1528212253415},{"_id":"public/tags/conntrack/index.html","hash":"bd9d1a618a91e7caf4ba94559801f70eb46ee01f","modified":1528212253415},{"_id":"public/2017/05/16/tuning_conntrack/index.html","hash":"826e5ed68a131ecc94fe469f40c5f618c5981394","modified":1528212253416},{"_id":"public/2017/04/19/harbor-deploy/index.html","hash":"0c77d551ac3e75f441c0700a3ae2866f3dcd0602","modified":1528212253416},{"_id":"public/2017/03/08/kuberntes-rollout-update/index.html","hash":"36b09767be78fc365af2fe48e978983cd2c67bc7","modified":1528212253416},{"_id":"public/2017/03/03/deploy-traefik/index.html","hash":"ae0fc72c172de58a32cd51bed1298d00a784f01b","modified":1528212253416},{"_id":"public/2017/01/18/deploy-kubernetes/index.html","hash":"45872db34438a335a718ba6a74f7616c26c70372","modified":1528212253416},{"_id":"public/2017/01/18/deploy-blog-by-github-hexo/index.html","hash":"4ffee16383068b130f2a8adb6b394c319382e0f9","modified":1528212253416},{"_id":"public/index.html","hash":"d3e6c418f5ada2111226804387dd9a15d972c2f9","modified":1528212253416},{"_id":"public/page/2/index.html","hash":"3e9b9ac90e9b441c669bc57b4d391d303cf6ccc9","modified":1528212253416},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1528212253423},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1528212253423},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1528212253423},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1528212253423},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1528212253423},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1528212253423},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1528212253423},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1528212253423},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1528212253424},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1528212253425},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1528212253793},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1528212253806},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1528212253807},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1528212253807},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1528212253807},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1528212253807},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1528212253808},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1528212253808},{"_id":"public/css/style.css","hash":"5f8dadd37d0052c557061018fe6f568f64fced9b","modified":1528212253808},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1528212253808},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1528212253808},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1528212253809},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1528212253809}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Archlinux Hibernate","date":"2018-06-04T16:00:00.000Z","_content":"\n\n# 修改swap partition/file size\n\nimage_size的容量用于控制休眠的时候内存dump到swap的最大大小，设置成0不受大小限制\n\n```\nsudo tee /sys/power/image_size <<< 0\n```\n\nimage_size重启后会被恢复，可以采用\n\n```\nsu - root -c 'echo \"w /sys/power/image_size - - - - 0\" > /etc/tmpfiles.d/modify_power_image_size.conf'\n```\n\n# 修改grub2\n\n在文件`/etc/grub.d/40_custom`添加下面内容\n\n```\nmenuentry 'My Arch Linux' {\n        load_video\n        set gfxpayload=keep\n        insmod gzio\n        insmod part_gpt\n        insmod xfs\n        set root='hd0,gpt2'\n        if [ x$feature_platform_search_hint = xy ]; then\n          search --no-floppy --fs-uuid --set=root --hint-bios=hd0,gpt2 --hint-efi=hd0,gpt2 --hint-baremetal=ahci0,gpt2  cda52f03-ea43-44dd-bf62-6defe65cc765\n        else\n          search --no-floppy --fs-uuid --set=root cda52f03-ea43-44dd-bf62-6defe65cc765\n        fi\n        echo    'Loading Linux linux ...'\n        linux   /vmlinuz-linux root=UUID=412d830c-e11e-44b4-abb9-3b59f885b803 rw quiet resume=/dev/sda5\n        echo    'Loading initial ramdisk ...'\n        initrd  /initramfs-linux.img\n}\n```\n\n其中增加了resume参数，该参数指定了swap所在分区，如果不清楚swap是哪个分区可以使用`lsblk`查看\n\n将配置信息独立在40_custom的好处是不破坏其他默认配置文件和原有启动菜单\n\ngrub配置信息每台机器不一样，建议从`/boot/grub/grub.cfg`复制出对应的菜单进行修改\n\n生成grub.cfg\n\n```\nsudo grub-mkconfig -o /boot/grub/grub.cfg\n```\n\n# 修改mkinitcpio.conf\n\n在`/etc/mkinitcpio.conf`的HOOKS中增加`resume`\n\n```\nHOOKS=(base udev autodetect modconf block resume filesystems keyboard fsck)\n```\n\n`resume`最好增加在`filesystems`前面\n\n生成initramfs\n\n```\nsudo mkinitcpio -p linux\n```\n\n# 测试休眠\n\n使用systemctl进入休眠\n\n```\nsystemctl hibernate\n```\n\n命令执行后等待电脑关机，关机后使用电源键选择增加的grub菜单进行启动即可恢复原有环境\n\n# 参考\n\n[Power management/Suspend and hibernate](https://wiki.archlinux.org/index.php/Power_management/Suspend_and_hibernate#Hibernation)\n[Linux GRUB2: How to resume from hibernation?](https://superuser.com/questions/383140/linux-grub2-how-to-resume-from-hibernation)\n[Archlinux休眠设置](http://www.cnblogs.com/xiaozhang9/p/6443478.html)\n","source":"_posts/archlinux-hibernate.md","raw":"title: Archlinux Hibernate\ndate: 2018-06-05\ntags:\n- arch\n- linux\n- hibernate\n---\n\n\n# 修改swap partition/file size\n\nimage_size的容量用于控制休眠的时候内存dump到swap的最大大小，设置成0不受大小限制\n\n```\nsudo tee /sys/power/image_size <<< 0\n```\n\nimage_size重启后会被恢复，可以采用\n\n```\nsu - root -c 'echo \"w /sys/power/image_size - - - - 0\" > /etc/tmpfiles.d/modify_power_image_size.conf'\n```\n\n# 修改grub2\n\n在文件`/etc/grub.d/40_custom`添加下面内容\n\n```\nmenuentry 'My Arch Linux' {\n        load_video\n        set gfxpayload=keep\n        insmod gzio\n        insmod part_gpt\n        insmod xfs\n        set root='hd0,gpt2'\n        if [ x$feature_platform_search_hint = xy ]; then\n          search --no-floppy --fs-uuid --set=root --hint-bios=hd0,gpt2 --hint-efi=hd0,gpt2 --hint-baremetal=ahci0,gpt2  cda52f03-ea43-44dd-bf62-6defe65cc765\n        else\n          search --no-floppy --fs-uuid --set=root cda52f03-ea43-44dd-bf62-6defe65cc765\n        fi\n        echo    'Loading Linux linux ...'\n        linux   /vmlinuz-linux root=UUID=412d830c-e11e-44b4-abb9-3b59f885b803 rw quiet resume=/dev/sda5\n        echo    'Loading initial ramdisk ...'\n        initrd  /initramfs-linux.img\n}\n```\n\n其中增加了resume参数，该参数指定了swap所在分区，如果不清楚swap是哪个分区可以使用`lsblk`查看\n\n将配置信息独立在40_custom的好处是不破坏其他默认配置文件和原有启动菜单\n\ngrub配置信息每台机器不一样，建议从`/boot/grub/grub.cfg`复制出对应的菜单进行修改\n\n生成grub.cfg\n\n```\nsudo grub-mkconfig -o /boot/grub/grub.cfg\n```\n\n# 修改mkinitcpio.conf\n\n在`/etc/mkinitcpio.conf`的HOOKS中增加`resume`\n\n```\nHOOKS=(base udev autodetect modconf block resume filesystems keyboard fsck)\n```\n\n`resume`最好增加在`filesystems`前面\n\n生成initramfs\n\n```\nsudo mkinitcpio -p linux\n```\n\n# 测试休眠\n\n使用systemctl进入休眠\n\n```\nsystemctl hibernate\n```\n\n命令执行后等待电脑关机，关机后使用电源键选择增加的grub菜单进行启动即可恢复原有环境\n\n# 参考\n\n[Power management/Suspend and hibernate](https://wiki.archlinux.org/index.php/Power_management/Suspend_and_hibernate#Hibernation)\n[Linux GRUB2: How to resume from hibernation?](https://superuser.com/questions/383140/linux-grub2-how-to-resume-from-hibernation)\n[Archlinux休眠设置](http://www.cnblogs.com/xiaozhang9/p/6443478.html)\n","slug":"archlinux-hibernate","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48to00002tsb4eia7l0d","content":"<h1 id=\"修改swap-partition-file-size\"><a href=\"#修改swap-partition-file-size\" class=\"headerlink\" title=\"修改swap partition/file size\"></a>修改swap partition/file size</h1><p>image_size的容量用于控制休眠的时候内存dump到swap的最大大小，设置成0不受大小限制</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo tee /sys/power/image_size &lt;&lt;&lt; 0</span><br></pre></td></tr></table></figure>\n<p>image_size重启后会被恢复，可以采用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su - root -c &apos;echo &quot;w /sys/power/image_size - - - - 0&quot; &gt; /etc/tmpfiles.d/modify_power_image_size.conf&apos;</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改grub2\"><a href=\"#修改grub2\" class=\"headerlink\" title=\"修改grub2\"></a>修改grub2</h1><p>在文件<code>/etc/grub.d/40_custom</code>添加下面内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">menuentry &apos;My Arch Linux&apos; &#123;</span><br><span class=\"line\">        load_video</span><br><span class=\"line\">        set gfxpayload=keep</span><br><span class=\"line\">        insmod gzio</span><br><span class=\"line\">        insmod part_gpt</span><br><span class=\"line\">        insmod xfs</span><br><span class=\"line\">        set root=&apos;hd0,gpt2&apos;</span><br><span class=\"line\">        if [ x$feature_platform_search_hint = xy ]; then</span><br><span class=\"line\">          search --no-floppy --fs-uuid --set=root --hint-bios=hd0,gpt2 --hint-efi=hd0,gpt2 --hint-baremetal=ahci0,gpt2  cda52f03-ea43-44dd-bf62-6defe65cc765</span><br><span class=\"line\">        else</span><br><span class=\"line\">          search --no-floppy --fs-uuid --set=root cda52f03-ea43-44dd-bf62-6defe65cc765</span><br><span class=\"line\">        fi</span><br><span class=\"line\">        echo    &apos;Loading Linux linux ...&apos;</span><br><span class=\"line\">        linux   /vmlinuz-linux root=UUID=412d830c-e11e-44b4-abb9-3b59f885b803 rw quiet resume=/dev/sda5</span><br><span class=\"line\">        echo    &apos;Loading initial ramdisk ...&apos;</span><br><span class=\"line\">        initrd  /initramfs-linux.img</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中增加了resume参数，该参数指定了swap所在分区，如果不清楚swap是哪个分区可以使用<code>lsblk</code>查看</p>\n<p>将配置信息独立在40_custom的好处是不破坏其他默认配置文件和原有启动菜单</p>\n<p>grub配置信息每台机器不一样，建议从<code>/boot/grub/grub.cfg</code>复制出对应的菜单进行修改</p>\n<p>生成grub.cfg</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo grub-mkconfig -o /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改mkinitcpio-conf\"><a href=\"#修改mkinitcpio-conf\" class=\"headerlink\" title=\"修改mkinitcpio.conf\"></a>修改mkinitcpio.conf</h1><p>在<code>/etc/mkinitcpio.conf</code>的HOOKS中增加<code>resume</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HOOKS=(base udev autodetect modconf block resume filesystems keyboard fsck)</span><br></pre></td></tr></table></figure>\n<p><code>resume</code>最好增加在<code>filesystems</code>前面</p>\n<p>生成initramfs</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkinitcpio -p linux</span><br></pre></td></tr></table></figure>\n<h1 id=\"测试休眠\"><a href=\"#测试休眠\" class=\"headerlink\" title=\"测试休眠\"></a>测试休眠</h1><p>使用systemctl进入休眠</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl hibernate</span><br></pre></td></tr></table></figure>\n<p>命令执行后等待电脑关机，关机后使用电源键选择增加的grub菜单进行启动即可恢复原有环境</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://wiki.archlinux.org/index.php/Power_management/Suspend_and_hibernate#Hibernation\" target=\"_blank\" rel=\"noopener\">Power management/Suspend and hibernate</a><br><a href=\"https://superuser.com/questions/383140/linux-grub2-how-to-resume-from-hibernation\" target=\"_blank\" rel=\"noopener\">Linux GRUB2: How to resume from hibernation?</a><br><a href=\"http://www.cnblogs.com/xiaozhang9/p/6443478.html\" target=\"_blank\" rel=\"noopener\">Archlinux休眠设置</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"修改swap-partition-file-size\"><a href=\"#修改swap-partition-file-size\" class=\"headerlink\" title=\"修改swap partition/file size\"></a>修改swap partition/file size</h1><p>image_size的容量用于控制休眠的时候内存dump到swap的最大大小，设置成0不受大小限制</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo tee /sys/power/image_size &lt;&lt;&lt; 0</span><br></pre></td></tr></table></figure>\n<p>image_size重启后会被恢复，可以采用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su - root -c &apos;echo &quot;w /sys/power/image_size - - - - 0&quot; &gt; /etc/tmpfiles.d/modify_power_image_size.conf&apos;</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改grub2\"><a href=\"#修改grub2\" class=\"headerlink\" title=\"修改grub2\"></a>修改grub2</h1><p>在文件<code>/etc/grub.d/40_custom</code>添加下面内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">menuentry &apos;My Arch Linux&apos; &#123;</span><br><span class=\"line\">        load_video</span><br><span class=\"line\">        set gfxpayload=keep</span><br><span class=\"line\">        insmod gzio</span><br><span class=\"line\">        insmod part_gpt</span><br><span class=\"line\">        insmod xfs</span><br><span class=\"line\">        set root=&apos;hd0,gpt2&apos;</span><br><span class=\"line\">        if [ x$feature_platform_search_hint = xy ]; then</span><br><span class=\"line\">          search --no-floppy --fs-uuid --set=root --hint-bios=hd0,gpt2 --hint-efi=hd0,gpt2 --hint-baremetal=ahci0,gpt2  cda52f03-ea43-44dd-bf62-6defe65cc765</span><br><span class=\"line\">        else</span><br><span class=\"line\">          search --no-floppy --fs-uuid --set=root cda52f03-ea43-44dd-bf62-6defe65cc765</span><br><span class=\"line\">        fi</span><br><span class=\"line\">        echo    &apos;Loading Linux linux ...&apos;</span><br><span class=\"line\">        linux   /vmlinuz-linux root=UUID=412d830c-e11e-44b4-abb9-3b59f885b803 rw quiet resume=/dev/sda5</span><br><span class=\"line\">        echo    &apos;Loading initial ramdisk ...&apos;</span><br><span class=\"line\">        initrd  /initramfs-linux.img</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中增加了resume参数，该参数指定了swap所在分区，如果不清楚swap是哪个分区可以使用<code>lsblk</code>查看</p>\n<p>将配置信息独立在40_custom的好处是不破坏其他默认配置文件和原有启动菜单</p>\n<p>grub配置信息每台机器不一样，建议从<code>/boot/grub/grub.cfg</code>复制出对应的菜单进行修改</p>\n<p>生成grub.cfg</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo grub-mkconfig -o /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure>\n<h1 id=\"修改mkinitcpio-conf\"><a href=\"#修改mkinitcpio-conf\" class=\"headerlink\" title=\"修改mkinitcpio.conf\"></a>修改mkinitcpio.conf</h1><p>在<code>/etc/mkinitcpio.conf</code>的HOOKS中增加<code>resume</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">HOOKS=(base udev autodetect modconf block resume filesystems keyboard fsck)</span><br></pre></td></tr></table></figure>\n<p><code>resume</code>最好增加在<code>filesystems</code>前面</p>\n<p>生成initramfs</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mkinitcpio -p linux</span><br></pre></td></tr></table></figure>\n<h1 id=\"测试休眠\"><a href=\"#测试休眠\" class=\"headerlink\" title=\"测试休眠\"></a>测试休眠</h1><p>使用systemctl进入休眠</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl hibernate</span><br></pre></td></tr></table></figure>\n<p>命令执行后等待电脑关机，关机后使用电源键选择增加的grub菜单进行启动即可恢复原有环境</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://wiki.archlinux.org/index.php/Power_management/Suspend_and_hibernate#Hibernation\" target=\"_blank\" rel=\"noopener\">Power management/Suspend and hibernate</a><br><a href=\"https://superuser.com/questions/383140/linux-grub2-how-to-resume-from-hibernation\" target=\"_blank\" rel=\"noopener\">Linux GRUB2: How to resume from hibernation?</a><br><a href=\"http://www.cnblogs.com/xiaozhang9/p/6443478.html\" target=\"_blank\" rel=\"noopener\">Archlinux休眠设置</a></p>\n"},{"title":"Deploy private blog using Github and Hexo","date":"2017-01-17T16:00:00.000Z","_content":"\n## Install nodejs\n\n### Download and Install nodejs\n\n先找个地方，用于存放你的node程序\n\n一般会使用固定的目录，如：/data/daen-bin\n\n```\nmkdir -p /data/dean-bin\ncd /data/dean-bin\n```\n\n下载并解压程序文件到dean-bin目录\n\n```\nwget https://nodejs.org/dist/v4.7.2/node-v4.7.2-linux-x64.tar.xz\nxz -d node-v4.7.2-linux-x64.tar.xz\ntar -xf node-v4.7.2-linux-x64.tar\nrm -f node-v4.7.2-linux-x64.tar\n```\n\n创建node链接，方便后续更新版本不需要更新配置\n\n```\nln -s node-v4.7.2-linux-x64 node\n```\n\n修改profile文件，让其可以读取到node和npm两个命令\n\n```\ncat << EOF >> ~/.bash_profile\nexport MY_BIN=/data/dean-bin\nexport NODEROOT=\\$MY_BIN/node\nexport PATH=\\$PATH:\\$NODEROOT/bin\nEOF\n```\n\n## Install hexo\n\n创建hexo文件夹，并安装hexo，同时进行初始化\n\n```\nmkdir hexo\ncd hexo\nnpm install -g hexo-cli\nhexo init\n```\n\n安装hexo插件\n\n```\nnpm install hexo-generator-index --save\nnpm install hexo-generator-archive --save\nnpm install hexo-generator-category --save\nnpm install hexo-generator-tag --save\nnpm install hexo-server --save\nnpm install hexo-deployer-git --save\nnpm install hexo-deployer-heroku --save\nnpm install hexo-deployer-rsync --save\nnpm install hexo-deployer-openshift --save\nnpm install hexo-renderer-marked --save\nnpm install hexo-renderer-stylus --save\nnpm install hexo-generator-feed --save\nnpm install hexo-generator-sitemap --save\n```\n\n启动hexo服务进行本地测试\n\n```\n$ hexo s\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.\n```\n\n如果出现上面提示，你就可以通过本地的4000端口访问页面，访问成功说明hexo可正常使用\n\n\n## 创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\n\n在github上创建仓库：xxx.github.io.git\n\n其中xxx为你的git用户名，如：git@github.com:dean/dean.github.io.git\n\n## 修改hexo配置\n\n其中title, subtitle, description, author根据按字面理解进行修改\nlanguage设置成zh_CN\nurl设置成你自己的域名，如果不进行域名执行可不修改\n其中deploy参照下列进行修改\n\n最终的配置文件：\n\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: Dean's Blog\nsubtitle: to going...\ndescription:\nauthor: Dean.wu\nlanguage: zh_CN\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://yoursite.com\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:xxx/xxx.github.io.git\n  branch: master\n  message: update :{{now(\"YYYY-MM-DD HH/mm/ss\")}}\n```\n\n## 测试hexo部署\n\n```\nhexo g\nhexo d\n```\n\n如果上述命令执行正常，你将可以打开你的博客页面：[https://xxx.github.io/](https://xxx.github.io/)\n\nDone\n","source":"_posts/deploy-blog-by-github-hexo.md","raw":"title: Deploy private blog using Github and Hexo\ndate: 2017-01-18\ntags:\n- github\n- hexo\n- blog\n---\n\n## Install nodejs\n\n### Download and Install nodejs\n\n先找个地方，用于存放你的node程序\n\n一般会使用固定的目录，如：/data/daen-bin\n\n```\nmkdir -p /data/dean-bin\ncd /data/dean-bin\n```\n\n下载并解压程序文件到dean-bin目录\n\n```\nwget https://nodejs.org/dist/v4.7.2/node-v4.7.2-linux-x64.tar.xz\nxz -d node-v4.7.2-linux-x64.tar.xz\ntar -xf node-v4.7.2-linux-x64.tar\nrm -f node-v4.7.2-linux-x64.tar\n```\n\n创建node链接，方便后续更新版本不需要更新配置\n\n```\nln -s node-v4.7.2-linux-x64 node\n```\n\n修改profile文件，让其可以读取到node和npm两个命令\n\n```\ncat << EOF >> ~/.bash_profile\nexport MY_BIN=/data/dean-bin\nexport NODEROOT=\\$MY_BIN/node\nexport PATH=\\$PATH:\\$NODEROOT/bin\nEOF\n```\n\n## Install hexo\n\n创建hexo文件夹，并安装hexo，同时进行初始化\n\n```\nmkdir hexo\ncd hexo\nnpm install -g hexo-cli\nhexo init\n```\n\n安装hexo插件\n\n```\nnpm install hexo-generator-index --save\nnpm install hexo-generator-archive --save\nnpm install hexo-generator-category --save\nnpm install hexo-generator-tag --save\nnpm install hexo-server --save\nnpm install hexo-deployer-git --save\nnpm install hexo-deployer-heroku --save\nnpm install hexo-deployer-rsync --save\nnpm install hexo-deployer-openshift --save\nnpm install hexo-renderer-marked --save\nnpm install hexo-renderer-stylus --save\nnpm install hexo-generator-feed --save\nnpm install hexo-generator-sitemap --save\n```\n\n启动hexo服务进行本地测试\n\n```\n$ hexo s\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.\n```\n\n如果出现上面提示，你就可以通过本地的4000端口访问页面，访问成功说明hexo可正常使用\n\n\n## 创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\n\n在github上创建仓库：xxx.github.io.git\n\n其中xxx为你的git用户名，如：git@github.com:dean/dean.github.io.git\n\n## 修改hexo配置\n\n其中title, subtitle, description, author根据按字面理解进行修改\nlanguage设置成zh_CN\nurl设置成你自己的域名，如果不进行域名执行可不修改\n其中deploy参照下列进行修改\n\n最终的配置文件：\n\n```\n# Hexo Configuration\n## Docs: https://hexo.io/docs/configuration.html\n## Source: https://github.com/hexojs/hexo/\n\n# Site\ntitle: Dean's Blog\nsubtitle: to going...\ndescription:\nauthor: Dean.wu\nlanguage: zh_CN\ntimezone:\n\n# URL\n## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'\nurl: http://yoursite.com\nroot: /\npermalink: :year/:month/:day/:title/\npermalink_defaults:\n\n# Directory\nsource_dir: source\npublic_dir: public\ntag_dir: tags\narchive_dir: archives\ncategory_dir: categories\ncode_dir: downloads/code\ni18n_dir: :lang\nskip_render:\n\n# Writing\nnew_post_name: :title.md # File name of new posts\ndefault_layout: post\ntitlecase: false # Transform title into titlecase\nexternal_link: true # Open external links in new tab\nfilename_case: 0\nrender_drafts: false\npost_asset_folder: false\nrelative_link: false\nfuture: true\nhighlight:\n  enable: true\n  line_number: true\n  auto_detect: false\n  tab_replace:\n\n# Category & Tag\ndefault_category: uncategorized\ncategory_map:\ntag_map:\n\n# Date / Time format\n## Hexo uses Moment.js to parse and display date\n## You can customize the date format as defined in\n## http://momentjs.com/docs/#/displaying/format/\ndate_format: YYYY-MM-DD\ntime_format: HH:mm:ss\n\n# Pagination\n## Set per_page to 0 to disable pagination\nper_page: 10\npagination_dir: page\n\n# Extensions\n## Plugins: https://hexo.io/plugins/\n## Themes: https://hexo.io/themes/\ntheme: landscape\n\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: git@github.com:xxx/xxx.github.io.git\n  branch: master\n  message: update :{{now(\"YYYY-MM-DD HH/mm/ss\")}}\n```\n\n## 测试hexo部署\n\n```\nhexo g\nhexo d\n```\n\n如果上述命令执行正常，你将可以打开你的博客页面：[https://xxx.github.io/](https://xxx.github.io/)\n\nDone\n","slug":"deploy-blog-by-github-hexo","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48tv00012tsbrxvyae6x","content":"<h2 id=\"Install-nodejs\"><a href=\"#Install-nodejs\" class=\"headerlink\" title=\"Install nodejs\"></a>Install nodejs</h2><h3 id=\"Download-and-Install-nodejs\"><a href=\"#Download-and-Install-nodejs\" class=\"headerlink\" title=\"Download and Install nodejs\"></a>Download and Install nodejs</h3><p>先找个地方，用于存放你的node程序</p>\n<p>一般会使用固定的目录，如：/data/daen-bin</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /data/dean-bin</span><br><span class=\"line\">cd /data/dean-bin</span><br></pre></td></tr></table></figure>\n<p>下载并解压程序文件到dean-bin目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://nodejs.org/dist/v4.7.2/node-v4.7.2-linux-x64.tar.xz</span><br><span class=\"line\">xz -d node-v4.7.2-linux-x64.tar.xz</span><br><span class=\"line\">tar -xf node-v4.7.2-linux-x64.tar</span><br><span class=\"line\">rm -f node-v4.7.2-linux-x64.tar</span><br></pre></td></tr></table></figure>\n<p>创建node链接，方便后续更新版本不需要更新配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln -s node-v4.7.2-linux-x64 node</span><br></pre></td></tr></table></figure>\n<p>修改profile文件，让其可以读取到node和npm两个命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt;&gt; ~/.bash_profile</span><br><span class=\"line\">export MY_BIN=/data/dean-bin</span><br><span class=\"line\">export NODEROOT=\\$MY_BIN/node</span><br><span class=\"line\">export PATH=\\$PATH:\\$NODEROOT/bin</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h2 id=\"Install-hexo\"><a href=\"#Install-hexo\" class=\"headerlink\" title=\"Install hexo\"></a>Install hexo</h2><p>创建hexo文件夹，并安装hexo，同时进行初始化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir hexo</span><br><span class=\"line\">cd hexo</span><br><span class=\"line\">npm install -g hexo-cli</span><br><span class=\"line\">hexo init</span><br></pre></td></tr></table></figure>\n<p>安装hexo插件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo-generator-index --save</span><br><span class=\"line\">npm install hexo-generator-archive --save</span><br><span class=\"line\">npm install hexo-generator-category --save</span><br><span class=\"line\">npm install hexo-generator-tag --save</span><br><span class=\"line\">npm install hexo-server --save</span><br><span class=\"line\">npm install hexo-deployer-git --save</span><br><span class=\"line\">npm install hexo-deployer-heroku --save</span><br><span class=\"line\">npm install hexo-deployer-rsync --save</span><br><span class=\"line\">npm install hexo-deployer-openshift --save</span><br><span class=\"line\">npm install hexo-renderer-marked --save</span><br><span class=\"line\">npm install hexo-renderer-stylus --save</span><br><span class=\"line\">npm install hexo-generator-feed --save</span><br><span class=\"line\">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure>\n<p>启动hexo服务进行本地测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo s</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure>\n<p>如果出现上面提示，你就可以通过本地的4000端口访问页面，访问成功说明hexo可正常使用</p>\n<h2 id=\"创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\"><a href=\"#创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\" class=\"headerlink\" title=\"创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\"></a>创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）</h2><p>在github上创建仓库：xxx.github.io.git</p>\n<p>其中xxx为你的git用户名，如：<a href=\"mailto:git@github.com\" target=\"_blank\" rel=\"noopener\">git@github.com</a>:dean/dean.github.io.git</p>\n<h2 id=\"修改hexo配置\"><a href=\"#修改hexo配置\" class=\"headerlink\" title=\"修改hexo配置\"></a>修改hexo配置</h2><p>其中title, subtitle, description, author根据按字面理解进行修改<br>language设置成zh_CN<br>url设置成你自己的域名，如果不进行域名执行可不修改<br>其中deploy参照下列进行修改</p>\n<p>最终的配置文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Hexo Configuration</span><br><span class=\"line\">## Docs: https://hexo.io/docs/configuration.html</span><br><span class=\"line\">## Source: https://github.com/hexojs/hexo/</span><br><span class=\"line\"></span><br><span class=\"line\"># Site</span><br><span class=\"line\">title: Dean&apos;s Blog</span><br><span class=\"line\">subtitle: to going...</span><br><span class=\"line\">description:</span><br><span class=\"line\">author: Dean.wu</span><br><span class=\"line\">language: zh_CN</span><br><span class=\"line\">timezone:</span><br><span class=\"line\"></span><br><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://yoursite.com</span><br><span class=\"line\">root: /</span><br><span class=\"line\">permalink: :year/:month/:day/:title/</span><br><span class=\"line\">permalink_defaults:</span><br><span class=\"line\"></span><br><span class=\"line\"># Directory</span><br><span class=\"line\">source_dir: source</span><br><span class=\"line\">public_dir: public</span><br><span class=\"line\">tag_dir: tags</span><br><span class=\"line\">archive_dir: archives</span><br><span class=\"line\">category_dir: categories</span><br><span class=\"line\">code_dir: downloads/code</span><br><span class=\"line\">i18n_dir: :lang</span><br><span class=\"line\">skip_render:</span><br><span class=\"line\"></span><br><span class=\"line\"># Writing</span><br><span class=\"line\">new_post_name: :title.md # File name of new posts</span><br><span class=\"line\">default_layout: post</span><br><span class=\"line\">titlecase: false # Transform title into titlecase</span><br><span class=\"line\">external_link: true # Open external links in new tab</span><br><span class=\"line\">filename_case: 0</span><br><span class=\"line\">render_drafts: false</span><br><span class=\"line\">post_asset_folder: false</span><br><span class=\"line\">relative_link: false</span><br><span class=\"line\">future: true</span><br><span class=\"line\">highlight:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  line_number: true</span><br><span class=\"line\">  auto_detect: false</span><br><span class=\"line\">  tab_replace:</span><br><span class=\"line\"></span><br><span class=\"line\"># Category &amp; Tag</span><br><span class=\"line\">default_category: uncategorized</span><br><span class=\"line\">category_map:</span><br><span class=\"line\">tag_map:</span><br><span class=\"line\"></span><br><span class=\"line\"># Date / Time format</span><br><span class=\"line\">## Hexo uses Moment.js to parse and display date</span><br><span class=\"line\">## You can customize the date format as defined in</span><br><span class=\"line\">## http://momentjs.com/docs/#/displaying/format/</span><br><span class=\"line\">date_format: YYYY-MM-DD</span><br><span class=\"line\">time_format: HH:mm:ss</span><br><span class=\"line\"></span><br><span class=\"line\"># Pagination</span><br><span class=\"line\">## Set per_page to 0 to disable pagination</span><br><span class=\"line\">per_page: 10</span><br><span class=\"line\">pagination_dir: page</span><br><span class=\"line\"></span><br><span class=\"line\"># Extensions</span><br><span class=\"line\">## Plugins: https://hexo.io/plugins/</span><br><span class=\"line\">## Themes: https://hexo.io/themes/</span><br><span class=\"line\">theme: landscape</span><br><span class=\"line\"></span><br><span class=\"line\"># Deployment</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:xxx/xxx.github.io.git</span><br><span class=\"line\">  branch: master</span><br><span class=\"line\">  message: update :&#123;&#123;now(&quot;YYYY-MM-DD HH/mm/ss&quot;)&#125;&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试hexo部署\"><a href=\"#测试hexo部署\" class=\"headerlink\" title=\"测试hexo部署\"></a>测试hexo部署</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>如果上述命令执行正常，你将可以打开你的博客页面：<a href=\"https://xxx.github.io/\" target=\"_blank\" rel=\"noopener\">https://xxx.github.io/</a></p>\n<p>Done</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Install-nodejs\"><a href=\"#Install-nodejs\" class=\"headerlink\" title=\"Install nodejs\"></a>Install nodejs</h2><h3 id=\"Download-and-Install-nodejs\"><a href=\"#Download-and-Install-nodejs\" class=\"headerlink\" title=\"Download and Install nodejs\"></a>Download and Install nodejs</h3><p>先找个地方，用于存放你的node程序</p>\n<p>一般会使用固定的目录，如：/data/daen-bin</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /data/dean-bin</span><br><span class=\"line\">cd /data/dean-bin</span><br></pre></td></tr></table></figure>\n<p>下载并解压程序文件到dean-bin目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://nodejs.org/dist/v4.7.2/node-v4.7.2-linux-x64.tar.xz</span><br><span class=\"line\">xz -d node-v4.7.2-linux-x64.tar.xz</span><br><span class=\"line\">tar -xf node-v4.7.2-linux-x64.tar</span><br><span class=\"line\">rm -f node-v4.7.2-linux-x64.tar</span><br></pre></td></tr></table></figure>\n<p>创建node链接，方便后续更新版本不需要更新配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln -s node-v4.7.2-linux-x64 node</span><br></pre></td></tr></table></figure>\n<p>修改profile文件，让其可以读取到node和npm两个命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt;&gt; ~/.bash_profile</span><br><span class=\"line\">export MY_BIN=/data/dean-bin</span><br><span class=\"line\">export NODEROOT=\\$MY_BIN/node</span><br><span class=\"line\">export PATH=\\$PATH:\\$NODEROOT/bin</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h2 id=\"Install-hexo\"><a href=\"#Install-hexo\" class=\"headerlink\" title=\"Install hexo\"></a>Install hexo</h2><p>创建hexo文件夹，并安装hexo，同时进行初始化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir hexo</span><br><span class=\"line\">cd hexo</span><br><span class=\"line\">npm install -g hexo-cli</span><br><span class=\"line\">hexo init</span><br></pre></td></tr></table></figure>\n<p>安装hexo插件</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo-generator-index --save</span><br><span class=\"line\">npm install hexo-generator-archive --save</span><br><span class=\"line\">npm install hexo-generator-category --save</span><br><span class=\"line\">npm install hexo-generator-tag --save</span><br><span class=\"line\">npm install hexo-server --save</span><br><span class=\"line\">npm install hexo-deployer-git --save</span><br><span class=\"line\">npm install hexo-deployer-heroku --save</span><br><span class=\"line\">npm install hexo-deployer-rsync --save</span><br><span class=\"line\">npm install hexo-deployer-openshift --save</span><br><span class=\"line\">npm install hexo-renderer-marked --save</span><br><span class=\"line\">npm install hexo-renderer-stylus --save</span><br><span class=\"line\">npm install hexo-generator-feed --save</span><br><span class=\"line\">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure>\n<p>启动hexo服务进行本地测试</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo s</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure>\n<p>如果出现上面提示，你就可以通过本地的4000端口访问页面，访问成功说明hexo可正常使用</p>\n<h2 id=\"创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\"><a href=\"#创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\" class=\"headerlink\" title=\"创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）\"></a>创建github仓库（注册自行解决，并确定已经将自己的私有key放在github上。也就是可以免密码使用git）</h2><p>在github上创建仓库：xxx.github.io.git</p>\n<p>其中xxx为你的git用户名，如：<a href=\"mailto:git@github.com\" target=\"_blank\" rel=\"noopener\">git@github.com</a>:dean/dean.github.io.git</p>\n<h2 id=\"修改hexo配置\"><a href=\"#修改hexo配置\" class=\"headerlink\" title=\"修改hexo配置\"></a>修改hexo配置</h2><p>其中title, subtitle, description, author根据按字面理解进行修改<br>language设置成zh_CN<br>url设置成你自己的域名，如果不进行域名执行可不修改<br>其中deploy参照下列进行修改</p>\n<p>最终的配置文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Hexo Configuration</span><br><span class=\"line\">## Docs: https://hexo.io/docs/configuration.html</span><br><span class=\"line\">## Source: https://github.com/hexojs/hexo/</span><br><span class=\"line\"></span><br><span class=\"line\"># Site</span><br><span class=\"line\">title: Dean&apos;s Blog</span><br><span class=\"line\">subtitle: to going...</span><br><span class=\"line\">description:</span><br><span class=\"line\">author: Dean.wu</span><br><span class=\"line\">language: zh_CN</span><br><span class=\"line\">timezone:</span><br><span class=\"line\"></span><br><span class=\"line\"># URL</span><br><span class=\"line\">## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;</span><br><span class=\"line\">url: http://yoursite.com</span><br><span class=\"line\">root: /</span><br><span class=\"line\">permalink: :year/:month/:day/:title/</span><br><span class=\"line\">permalink_defaults:</span><br><span class=\"line\"></span><br><span class=\"line\"># Directory</span><br><span class=\"line\">source_dir: source</span><br><span class=\"line\">public_dir: public</span><br><span class=\"line\">tag_dir: tags</span><br><span class=\"line\">archive_dir: archives</span><br><span class=\"line\">category_dir: categories</span><br><span class=\"line\">code_dir: downloads/code</span><br><span class=\"line\">i18n_dir: :lang</span><br><span class=\"line\">skip_render:</span><br><span class=\"line\"></span><br><span class=\"line\"># Writing</span><br><span class=\"line\">new_post_name: :title.md # File name of new posts</span><br><span class=\"line\">default_layout: post</span><br><span class=\"line\">titlecase: false # Transform title into titlecase</span><br><span class=\"line\">external_link: true # Open external links in new tab</span><br><span class=\"line\">filename_case: 0</span><br><span class=\"line\">render_drafts: false</span><br><span class=\"line\">post_asset_folder: false</span><br><span class=\"line\">relative_link: false</span><br><span class=\"line\">future: true</span><br><span class=\"line\">highlight:</span><br><span class=\"line\">  enable: true</span><br><span class=\"line\">  line_number: true</span><br><span class=\"line\">  auto_detect: false</span><br><span class=\"line\">  tab_replace:</span><br><span class=\"line\"></span><br><span class=\"line\"># Category &amp; Tag</span><br><span class=\"line\">default_category: uncategorized</span><br><span class=\"line\">category_map:</span><br><span class=\"line\">tag_map:</span><br><span class=\"line\"></span><br><span class=\"line\"># Date / Time format</span><br><span class=\"line\">## Hexo uses Moment.js to parse and display date</span><br><span class=\"line\">## You can customize the date format as defined in</span><br><span class=\"line\">## http://momentjs.com/docs/#/displaying/format/</span><br><span class=\"line\">date_format: YYYY-MM-DD</span><br><span class=\"line\">time_format: HH:mm:ss</span><br><span class=\"line\"></span><br><span class=\"line\"># Pagination</span><br><span class=\"line\">## Set per_page to 0 to disable pagination</span><br><span class=\"line\">per_page: 10</span><br><span class=\"line\">pagination_dir: page</span><br><span class=\"line\"></span><br><span class=\"line\"># Extensions</span><br><span class=\"line\">## Plugins: https://hexo.io/plugins/</span><br><span class=\"line\">## Themes: https://hexo.io/themes/</span><br><span class=\"line\">theme: landscape</span><br><span class=\"line\"></span><br><span class=\"line\"># Deployment</span><br><span class=\"line\">## Docs: https://hexo.io/docs/deployment.html</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:xxx/xxx.github.io.git</span><br><span class=\"line\">  branch: master</span><br><span class=\"line\">  message: update :&#123;&#123;now(&quot;YYYY-MM-DD HH/mm/ss&quot;)&#125;&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"测试hexo部署\"><a href=\"#测试hexo部署\" class=\"headerlink\" title=\"测试hexo部署\"></a>测试hexo部署</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>如果上述命令执行正常，你将可以打开你的博客页面：<a href=\"https://xxx.github.io/\" target=\"_blank\" rel=\"noopener\">https://xxx.github.io/</a></p>\n<p>Done</p>\n"},{"title":"Deploy kubernetes by manual on centos 7","date":"2017-01-17T16:00:00.000Z","_content":"\n\n\n# 环境信息\n\n|ip|role|hostname|\n|--|----|--------|\n|10.10.0.1|master&minion|k8s-master-1|\n|10.10.0.2|minion|k8s-node-1|\n|10.10.0.3|minion|k8s-node-2|\n\n# on all machine\n\n*Note: 部署的时候最好关闭selinux，否则像 nfs会挂载失败(nfs用于使用pv，不是必须的)*\n\n## 修改hostname\n\n用户名建议使用中横杠，因为后面在进行kubernetes配置的时候可以直接使用hostname作为节点名称\n\n```shell\necho \"k8s-node-xx\" > /etc/hostname\n```\n\n## 基本配置\n\n关闭防火墙\n安装ntp\n\n```shell\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\n\nsudo yum -y install ntp\nsudo systemctl start ntpd\nsudo systemctl enable ntpd\nsudo systemctl status ntpd\n```\n\n创建k8s帐号和sudo\n\n```\nsudo useradd -u 1008 k8s -m && sudo passwd k8s\n\nsudo echo \"\" >> /etc/sudoers\nsudo echo \"#add k8s user\" >> /etc/sudoers\nsudo echo \"k8s ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n```\n\n从下面开始所以操作都是在k8s这个用户下进行\n\n# on master\n\n## etcd\n\n### Download && Install etcd\n\netcd是类似与zookeeper的分布式配置存储解决方案，特意加了“配置”两个字是因为它的设计目标主要是用来存储配置文件\n\n下载地址：[etcd v3.0.9](https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz)\n\n```shell\nwget https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz\ntar -zxf etcd-v3.0.9-linux-amd64.tar.gz\nln -s etcd-v3.0.9-linux-amd64 etcd\n```\n\n### 启动配置脚本\n\n因为这是实验环境，所以这里的etcd只使用一个节点，如果你是用于生产环境建议配置3个以上的etcd节点\n\n```\ncd ~/etcd\ncat <<EOF > etcd-start.sh\n#! /bin/sh\n\nTHIS_IP=10.10.0.1\n\n./etcd \\\n--name=infra1 \\\n--advertise-client-urls http://${THIS_IP}:2379 \\\n--listen-client-urls http://${THIS_IP}:2379,http://127.0.0.1:2379 \\\n--data-dir=data  \\\n1> etcd.log 2>&1\nEOF\nchmod a+x etcd-start.sh\n```\n\n### 使用配置脚本启动etcd\n\n```\n./etcd-start.sh\n```\n\n### 测试etcd\n\n```\n./etcdctl ls\n```\n\n如果上述命令成功，那么etcd处于正常运行中\n\n### 添加flannel的网络地址范围到etcd中\n\n```\ncd ~/etcd\n./etcdctl mk /flannel/network/config '{\"Network\":\"172.1.0.0/16\"}'\n```\n\n其中 /flannel/network/config这个地址是可以自己指定的，需要配置到flannel中\n\n## Kubernetes\n\n### 下载并配置kubernetes\n\nkubernetes因为发布节奏很快，所以各种仓库都不会及时更新，所以本文采用手动安装方式，安装包可以直接在github上下载\n\n下载地址：[kubernetes v1.4.6](https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz)\n\n这里需要注意，kubernetes v1.5以后不再提供bin文件，所以你如果下载1.5以后的版本需要另外下载bin文件\n\n```shell\nwget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\ntar zxf kubernetes.tar.gz\ncd kubernetes/server\ntar zxf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes/server/bin\nmkdir -p ~/kubernetes-master/log\ncp kube-apiserver kube-controller-manager kubectl kube-dns kube-scheduler ~/kubernetes-master/\n\n# 将bin添加到path中\necho \"export PATH=~/kubernetes-master:\\$PATH\" >> ~/.bash_profile\n\nsource ~/.bash_profile\n```\n\n### init配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > init.sh\n#! /bin/sh\n\nKUBE_MASTER=10.10.0.1\n\n# Change to root auth\nsudo chown root.root kube-dns\nsudo chmod u+s kube-dns\nEOF\nchmod a+x init.sh\n./init.sh\n```\n\nkube-dns需要使用root权限\n\n### api启动配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_api.sh\n#! /bin/sh\n\n# start the main server of k8s master\nsudo ./kube-apiserver \\\n    --bind-address=0.0.0.0 \\\n    --insecure-bind-address=0.0.0.0 \\\n    --insecure-port=8080 \\\n    --etcd_servers=http://10.10.0.1:2379 \\\n    --allow-privileged=true \\\n    --service-cluster-ip-range=182.1.0.0/16 \\\n    --log-dir=\"log\" \\\n1> k8s-api.log 2>&1\nEOF\nchmod a+x start_k8s_api.sh\n```\n\n### controller-manager配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_cm.sh\n#! /bin/sh\n\n./kube-controller-manager \\\n    --master=\"http://127.0.0.1:8080\" \\\n    --log-dir=\"log\" \\\n1> k8s-cm.log 2>&1\nEOF\nchmod a+x start_k8s_cm.sh\n```\n\n### dns配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_dns.sh\n#! /bin/sh\n\nKUBE_DOMAIN=\"cluster.local\"\n\nsudo ./kube-dns \\\n    --domain=${KUBE_DOMAIN} \\\n    --kube-master-url=\"http://127.0.0.1:8080\" \\\n    --dns-port=53 \\\n    --log-dir=\"log\" \\\n1> k8s-dns.log 2>&1\nEOF\nchmod a+x start_k8s_dns.sh\n```\n\n### scheduler配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_scd.sh\n#! /bin/sh\n\n./kube-scheduler \\\n    --master=\"http://127.0.0.1:8080\" \\\n    --log-dir=\"log\" \\\n1> k8s-scd.log 2>&1\nEOF\nchmod a+x start_k8s_scd.sh\n```\n\n### start master\n\n```shell\ncd ~/etcd\n./start_etcd.sh\n\ncd ~/kubernetes-master\n./start_k8s_api.sh\n./start_k8s_dns.sh\n./start_k8s_cm.sh\n./start_k8s_scd.sh\n```\n\n# on minions\n\nminion需要部署所有机器上\nmaster上也需要部署minion\n\n## flannel\n\n### Download && Install flannel\n\n[flannel v0.6.1](https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz)\n\n```shell\ncd ~\nwget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz\ntar -zxf flannel-v0.6.1-linux-amd64.tar.gz\nmkdir flannel\nmv flannel-v0.6.1-linux-amd64.tar.gz flannel\ncd ~/flannel/\ntar -zxf flannel-v0.6.1-linux-amd64.tar.gz\n```\n\n### 启动配置脚本\n\n```\ncd ~/flannel\ncat <<EOF > flanneld-start.sh\n#! /bin/sh\n\nETCD_PREFIX=/flannel/network\n\n# delete docker0. it will recreate in docker starting\nrc=0\nip link show docker0 >/dev/null 2>&1 || rc=\"$?\"\nif [[ \"$rc\" -eq \"0\" ]]; then\n  sudo ip link set dev docker0 down\n  sudo ip link delete docker0\nfi\n\nsudo ./flanneld \\\n    --ip-masq \\\n    --subnet-file=\"run/subnet.env\" \\\n    --etcd-endpoints=http://10.10.0.1:2379 \\\n    --etcd-prefix=$ETCD_PREFIX > flanneld.log 2>&1\nEOF\nchmod a+x flanneld-start.sh\n```\n## kubernetes\n\n```shell\nwget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\ntar zxf kubernetes.tar.gz\ncd kubernetes/server\ntar zxf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes/server/bin\nmkdir -p ~/kubernetes/log\nmkdir -p ~/kubernetes/cert\nmv kubelet kube-proxy ~/kubernetes/\n```\n\n### kubeconfig配置文件\n\n```\ncat << EOF > /home/k8s/kubernetes/kubeconfig\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://10.10.0.1:8080\n  name: k8s-cluster\ncontexts:\n- context:\n    cluster: k8s-cluster\n    namespace: default\n  name: dft\n- context:\n    cluster: k8s-cluster\n    namespace: kube-system\n  name: sys\ncurrent-context: dft\nkind: Config\npreferences: {}\nEOF\n```\n\n### kubelet配置文件\n\nNODE_IP分别指定两台minion机器，分别是10.10.0.2和10.10.0.3\n\n```\ncd ~/kubernetes\ncat <<EOF > k8s-let-start.sh\n#! /bin/sh\n\nKUBE_MASTER=10.10.0.1\nNODE_IP=10.10.0.1\nCLUSTER_DOMAIN=cluster.local\n\nsudo ./kubelet \\\n    --address=0.0.0.0 \\\n    --port=10250 \\\n    --hostname_override=$NODE_IP \\\n    --require-kubeconfig=true \\\n    --kubeconfig=\"/home/k8s/kubernetes/kubeconfig\" \\\n    --cni-bin-dir=cni/bin \\\n    --cni-conf-dir=cni/net.d \\\n    --logtostderr=false \\\n    --allow-privileged=true \\\n    --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\\n    --cluster-dns=$KUBE_MASTER \\\n    --cluster-domain=$CLUSTER_DOMAIN \\\n    --pod-manifest-path=\"manifests\" \\\n    --log-dir=\"log\" \\\n1> k8s-let.log 2>&1\nEOF\nchmod a+x k8s-let-start.sh\n```\n\n### kube-proxy配置文件\n\n```\ncd ~/kubernetes\ncat <<EOF > k8s-proxy-start.sh\n#! /bin/sh\n\nNODE_IP=10.10.0.1\n\nsudo ./kube-proxy \\\n    --kubeconfig=\"/home/k8s/kubernetes/kubeconfig\" \\\n    --hostname_override=$NODE_IP \\\n    --proxy-mode=iptables \\\n    --log-dir=\"log\" \\\n1> k8s-proxy.log 2>&1\nEOF\nchmod a+x k8s-proxy-start.sh\n```\n\n## Init & Start minion\n\n### 配置flannel和docker0\n\n启动flannel\n\n```shell\ncd ~/flannel\n./start_flanneld.sh\n```\n\n如果是第一次启动需要进行docker的ip配置\n\n```\nsudo ./mk-docker-opts.sh -f ./run/subnet.env -d ./run/docker.env\ncat ./run/docker.env | grep -i DOCKER_OPTS\n```\n\n记录下上面命令输出的内容，内容大概如下：\n\n```\n--bip=172.17.95.1/24 --mtu=1472\n```\n\n编辑docker-network，并修改DOCKER_NETWORK_OPTIONS参数，填入上面输出的内容\n\n类似与下面的结果：\n\n```\nsudo vi /etc/sysconfig/docker-network\nDOCKER_NETWORK_OPTIONS=\" --bip=172.17.95.1/24 --mtu=1472\"\n```\n\n上述步骤是用于配置docker0中可分配的网段，如果使用yum install flannel在启动时会自动配置\n\n重新启动docker\n\n```\nsudo systemctl restart docker\n```\n\n通过ip a确定docker0和flannel0是在同一个网段\n\n```shell\nip a\n```\n\n### start kubelet\n\n```shell\ncd ~/kubernetes\n./start_k8s_let.sh\n./start_k8s_proxy.sh\n```\n\n相同的步骤配置10.10.0.2和10.10.0.3\n配置完成后三个节点都会加入到集群中\n\n# 验证\n\n登录到10.10.0.1上\n\n```\nkubectl get no\n```\n\n检查上个节点是否已配置完成\n\nDone\n","source":"_posts/deploy-kubernetes.md","raw":"title: Deploy kubernetes by manual on centos 7\ndate: 2017-01-18\ntags:\n- docker\n- kubernetes\n---\n\n\n\n# 环境信息\n\n|ip|role|hostname|\n|--|----|--------|\n|10.10.0.1|master&minion|k8s-master-1|\n|10.10.0.2|minion|k8s-node-1|\n|10.10.0.3|minion|k8s-node-2|\n\n# on all machine\n\n*Note: 部署的时候最好关闭selinux，否则像 nfs会挂载失败(nfs用于使用pv，不是必须的)*\n\n## 修改hostname\n\n用户名建议使用中横杠，因为后面在进行kubernetes配置的时候可以直接使用hostname作为节点名称\n\n```shell\necho \"k8s-node-xx\" > /etc/hostname\n```\n\n## 基本配置\n\n关闭防火墙\n安装ntp\n\n```shell\nsudo systemctl stop firewalld\nsudo systemctl disable firewalld\n\nsudo yum -y install ntp\nsudo systemctl start ntpd\nsudo systemctl enable ntpd\nsudo systemctl status ntpd\n```\n\n创建k8s帐号和sudo\n\n```\nsudo useradd -u 1008 k8s -m && sudo passwd k8s\n\nsudo echo \"\" >> /etc/sudoers\nsudo echo \"#add k8s user\" >> /etc/sudoers\nsudo echo \"k8s ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n```\n\n从下面开始所以操作都是在k8s这个用户下进行\n\n# on master\n\n## etcd\n\n### Download && Install etcd\n\netcd是类似与zookeeper的分布式配置存储解决方案，特意加了“配置”两个字是因为它的设计目标主要是用来存储配置文件\n\n下载地址：[etcd v3.0.9](https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz)\n\n```shell\nwget https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz\ntar -zxf etcd-v3.0.9-linux-amd64.tar.gz\nln -s etcd-v3.0.9-linux-amd64 etcd\n```\n\n### 启动配置脚本\n\n因为这是实验环境，所以这里的etcd只使用一个节点，如果你是用于生产环境建议配置3个以上的etcd节点\n\n```\ncd ~/etcd\ncat <<EOF > etcd-start.sh\n#! /bin/sh\n\nTHIS_IP=10.10.0.1\n\n./etcd \\\n--name=infra1 \\\n--advertise-client-urls http://${THIS_IP}:2379 \\\n--listen-client-urls http://${THIS_IP}:2379,http://127.0.0.1:2379 \\\n--data-dir=data  \\\n1> etcd.log 2>&1\nEOF\nchmod a+x etcd-start.sh\n```\n\n### 使用配置脚本启动etcd\n\n```\n./etcd-start.sh\n```\n\n### 测试etcd\n\n```\n./etcdctl ls\n```\n\n如果上述命令成功，那么etcd处于正常运行中\n\n### 添加flannel的网络地址范围到etcd中\n\n```\ncd ~/etcd\n./etcdctl mk /flannel/network/config '{\"Network\":\"172.1.0.0/16\"}'\n```\n\n其中 /flannel/network/config这个地址是可以自己指定的，需要配置到flannel中\n\n## Kubernetes\n\n### 下载并配置kubernetes\n\nkubernetes因为发布节奏很快，所以各种仓库都不会及时更新，所以本文采用手动安装方式，安装包可以直接在github上下载\n\n下载地址：[kubernetes v1.4.6](https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz)\n\n这里需要注意，kubernetes v1.5以后不再提供bin文件，所以你如果下载1.5以后的版本需要另外下载bin文件\n\n```shell\nwget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\ntar zxf kubernetes.tar.gz\ncd kubernetes/server\ntar zxf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes/server/bin\nmkdir -p ~/kubernetes-master/log\ncp kube-apiserver kube-controller-manager kubectl kube-dns kube-scheduler ~/kubernetes-master/\n\n# 将bin添加到path中\necho \"export PATH=~/kubernetes-master:\\$PATH\" >> ~/.bash_profile\n\nsource ~/.bash_profile\n```\n\n### init配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > init.sh\n#! /bin/sh\n\nKUBE_MASTER=10.10.0.1\n\n# Change to root auth\nsudo chown root.root kube-dns\nsudo chmod u+s kube-dns\nEOF\nchmod a+x init.sh\n./init.sh\n```\n\nkube-dns需要使用root权限\n\n### api启动配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_api.sh\n#! /bin/sh\n\n# start the main server of k8s master\nsudo ./kube-apiserver \\\n    --bind-address=0.0.0.0 \\\n    --insecure-bind-address=0.0.0.0 \\\n    --insecure-port=8080 \\\n    --etcd_servers=http://10.10.0.1:2379 \\\n    --allow-privileged=true \\\n    --service-cluster-ip-range=182.1.0.0/16 \\\n    --log-dir=\"log\" \\\n1> k8s-api.log 2>&1\nEOF\nchmod a+x start_k8s_api.sh\n```\n\n### controller-manager配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_cm.sh\n#! /bin/sh\n\n./kube-controller-manager \\\n    --master=\"http://127.0.0.1:8080\" \\\n    --log-dir=\"log\" \\\n1> k8s-cm.log 2>&1\nEOF\nchmod a+x start_k8s_cm.sh\n```\n\n### dns配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_dns.sh\n#! /bin/sh\n\nKUBE_DOMAIN=\"cluster.local\"\n\nsudo ./kube-dns \\\n    --domain=${KUBE_DOMAIN} \\\n    --kube-master-url=\"http://127.0.0.1:8080\" \\\n    --dns-port=53 \\\n    --log-dir=\"log\" \\\n1> k8s-dns.log 2>&1\nEOF\nchmod a+x start_k8s_dns.sh\n```\n\n### scheduler配置文件\n\n```\ncd ~/kubernetes-master\ncat <<EOF > start_k8s_scd.sh\n#! /bin/sh\n\n./kube-scheduler \\\n    --master=\"http://127.0.0.1:8080\" \\\n    --log-dir=\"log\" \\\n1> k8s-scd.log 2>&1\nEOF\nchmod a+x start_k8s_scd.sh\n```\n\n### start master\n\n```shell\ncd ~/etcd\n./start_etcd.sh\n\ncd ~/kubernetes-master\n./start_k8s_api.sh\n./start_k8s_dns.sh\n./start_k8s_cm.sh\n./start_k8s_scd.sh\n```\n\n# on minions\n\nminion需要部署所有机器上\nmaster上也需要部署minion\n\n## flannel\n\n### Download && Install flannel\n\n[flannel v0.6.1](https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz)\n\n```shell\ncd ~\nwget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz\ntar -zxf flannel-v0.6.1-linux-amd64.tar.gz\nmkdir flannel\nmv flannel-v0.6.1-linux-amd64.tar.gz flannel\ncd ~/flannel/\ntar -zxf flannel-v0.6.1-linux-amd64.tar.gz\n```\n\n### 启动配置脚本\n\n```\ncd ~/flannel\ncat <<EOF > flanneld-start.sh\n#! /bin/sh\n\nETCD_PREFIX=/flannel/network\n\n# delete docker0. it will recreate in docker starting\nrc=0\nip link show docker0 >/dev/null 2>&1 || rc=\"$?\"\nif [[ \"$rc\" -eq \"0\" ]]; then\n  sudo ip link set dev docker0 down\n  sudo ip link delete docker0\nfi\n\nsudo ./flanneld \\\n    --ip-masq \\\n    --subnet-file=\"run/subnet.env\" \\\n    --etcd-endpoints=http://10.10.0.1:2379 \\\n    --etcd-prefix=$ETCD_PREFIX > flanneld.log 2>&1\nEOF\nchmod a+x flanneld-start.sh\n```\n## kubernetes\n\n```shell\nwget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\ntar zxf kubernetes.tar.gz\ncd kubernetes/server\ntar zxf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes/server/bin\nmkdir -p ~/kubernetes/log\nmkdir -p ~/kubernetes/cert\nmv kubelet kube-proxy ~/kubernetes/\n```\n\n### kubeconfig配置文件\n\n```\ncat << EOF > /home/k8s/kubernetes/kubeconfig\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://10.10.0.1:8080\n  name: k8s-cluster\ncontexts:\n- context:\n    cluster: k8s-cluster\n    namespace: default\n  name: dft\n- context:\n    cluster: k8s-cluster\n    namespace: kube-system\n  name: sys\ncurrent-context: dft\nkind: Config\npreferences: {}\nEOF\n```\n\n### kubelet配置文件\n\nNODE_IP分别指定两台minion机器，分别是10.10.0.2和10.10.0.3\n\n```\ncd ~/kubernetes\ncat <<EOF > k8s-let-start.sh\n#! /bin/sh\n\nKUBE_MASTER=10.10.0.1\nNODE_IP=10.10.0.1\nCLUSTER_DOMAIN=cluster.local\n\nsudo ./kubelet \\\n    --address=0.0.0.0 \\\n    --port=10250 \\\n    --hostname_override=$NODE_IP \\\n    --require-kubeconfig=true \\\n    --kubeconfig=\"/home/k8s/kubernetes/kubeconfig\" \\\n    --cni-bin-dir=cni/bin \\\n    --cni-conf-dir=cni/net.d \\\n    --logtostderr=false \\\n    --allow-privileged=true \\\n    --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\\n    --cluster-dns=$KUBE_MASTER \\\n    --cluster-domain=$CLUSTER_DOMAIN \\\n    --pod-manifest-path=\"manifests\" \\\n    --log-dir=\"log\" \\\n1> k8s-let.log 2>&1\nEOF\nchmod a+x k8s-let-start.sh\n```\n\n### kube-proxy配置文件\n\n```\ncd ~/kubernetes\ncat <<EOF > k8s-proxy-start.sh\n#! /bin/sh\n\nNODE_IP=10.10.0.1\n\nsudo ./kube-proxy \\\n    --kubeconfig=\"/home/k8s/kubernetes/kubeconfig\" \\\n    --hostname_override=$NODE_IP \\\n    --proxy-mode=iptables \\\n    --log-dir=\"log\" \\\n1> k8s-proxy.log 2>&1\nEOF\nchmod a+x k8s-proxy-start.sh\n```\n\n## Init & Start minion\n\n### 配置flannel和docker0\n\n启动flannel\n\n```shell\ncd ~/flannel\n./start_flanneld.sh\n```\n\n如果是第一次启动需要进行docker的ip配置\n\n```\nsudo ./mk-docker-opts.sh -f ./run/subnet.env -d ./run/docker.env\ncat ./run/docker.env | grep -i DOCKER_OPTS\n```\n\n记录下上面命令输出的内容，内容大概如下：\n\n```\n--bip=172.17.95.1/24 --mtu=1472\n```\n\n编辑docker-network，并修改DOCKER_NETWORK_OPTIONS参数，填入上面输出的内容\n\n类似与下面的结果：\n\n```\nsudo vi /etc/sysconfig/docker-network\nDOCKER_NETWORK_OPTIONS=\" --bip=172.17.95.1/24 --mtu=1472\"\n```\n\n上述步骤是用于配置docker0中可分配的网段，如果使用yum install flannel在启动时会自动配置\n\n重新启动docker\n\n```\nsudo systemctl restart docker\n```\n\n通过ip a确定docker0和flannel0是在同一个网段\n\n```shell\nip a\n```\n\n### start kubelet\n\n```shell\ncd ~/kubernetes\n./start_k8s_let.sh\n./start_k8s_proxy.sh\n```\n\n相同的步骤配置10.10.0.2和10.10.0.3\n配置完成后三个节点都会加入到集群中\n\n# 验证\n\n登录到10.10.0.1上\n\n```\nkubectl get no\n```\n\n检查上个节点是否已配置完成\n\nDone\n","slug":"deploy-kubernetes","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u000032tsb0k880g86","content":"<h1 id=\"环境信息\"><a href=\"#环境信息\" class=\"headerlink\" title=\"环境信息\"></a>环境信息</h1><table>\n<thead>\n<tr>\n<th>ip</th>\n<th>role</th>\n<th>hostname</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10.10.0.1</td>\n<td>master&amp;minion</td>\n<td>k8s-master-1</td>\n</tr>\n<tr>\n<td>10.10.0.2</td>\n<td>minion</td>\n<td>k8s-node-1</td>\n</tr>\n<tr>\n<td>10.10.0.3</td>\n<td>minion</td>\n<td>k8s-node-2</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"on-all-machine\"><a href=\"#on-all-machine\" class=\"headerlink\" title=\"on all machine\"></a>on all machine</h1><p><em>Note: 部署的时候最好关闭selinux，否则像 nfs会挂载失败(nfs用于使用pv，不是必须的)</em></p>\n<h2 id=\"修改hostname\"><a href=\"#修改hostname\" class=\"headerlink\" title=\"修改hostname\"></a>修改hostname</h2><p>用户名建议使用中横杠，因为后面在进行kubernetes配置的时候可以直接使用hostname作为节点名称</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo \"k8s-node-xx\" &gt; /etc/hostname</span><br></pre></td></tr></table></figure>\n<h2 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h2><p>关闭防火墙<br>安装ntp</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl stop firewalld</span><br><span class=\"line\">sudo systemctl disable firewalld</span><br><span class=\"line\"></span><br><span class=\"line\">sudo yum -y install ntp</span><br><span class=\"line\">sudo systemctl start ntpd</span><br><span class=\"line\">sudo systemctl enable ntpd</span><br><span class=\"line\">sudo systemctl status ntpd</span><br></pre></td></tr></table></figure>\n<p>创建k8s帐号和sudo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo useradd -u 1008 k8s -m &amp;&amp; sudo passwd k8s</span><br><span class=\"line\"></span><br><span class=\"line\">sudo echo &quot;&quot; &gt;&gt; /etc/sudoers</span><br><span class=\"line\">sudo echo &quot;#add k8s user&quot; &gt;&gt; /etc/sudoers</span><br><span class=\"line\">sudo echo &quot;k8s ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoers</span><br></pre></td></tr></table></figure>\n<p>从下面开始所以操作都是在k8s这个用户下进行</p>\n<h1 id=\"on-master\"><a href=\"#on-master\" class=\"headerlink\" title=\"on master\"></a>on master</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><h3 id=\"Download-amp-amp-Install-etcd\"><a href=\"#Download-amp-amp-Install-etcd\" class=\"headerlink\" title=\"Download &amp;&amp; Install etcd\"></a>Download &amp;&amp; Install etcd</h3><p>etcd是类似与zookeeper的分布式配置存储解决方案，特意加了“配置”两个字是因为它的设计目标主要是用来存储配置文件</p>\n<p>下载地址：<a href=\"https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz\" target=\"_blank\" rel=\"noopener\">etcd v3.0.9</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz</span><br><span class=\"line\">tar -zxf etcd-v3.0.9-linux-amd64.tar.gz</span><br><span class=\"line\">ln -s etcd-v3.0.9-linux-amd64 etcd</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动配置脚本\"><a href=\"#启动配置脚本\" class=\"headerlink\" title=\"启动配置脚本\"></a>启动配置脚本</h3><p>因为这是实验环境，所以这里的etcd只使用一个节点，如果你是用于生产环境建议配置3个以上的etcd节点</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; etcd-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">THIS_IP=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\">./etcd \\</span><br><span class=\"line\">--name=infra1 \\</span><br><span class=\"line\">--advertise-client-urls http://$&#123;THIS_IP&#125;:2379 \\</span><br><span class=\"line\">--listen-client-urls http://$&#123;THIS_IP&#125;:2379,http://127.0.0.1:2379 \\</span><br><span class=\"line\">--data-dir=data  \\</span><br><span class=\"line\">1&gt; etcd.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x etcd-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用配置脚本启动etcd\"><a href=\"#使用配置脚本启动etcd\" class=\"headerlink\" title=\"使用配置脚本启动etcd\"></a>使用配置脚本启动etcd</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./etcd-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试etcd\"><a href=\"#测试etcd\" class=\"headerlink\" title=\"测试etcd\"></a>测试etcd</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./etcdctl ls</span><br></pre></td></tr></table></figure>\n<p>如果上述命令成功，那么etcd处于正常运行中</p>\n<h3 id=\"添加flannel的网络地址范围到etcd中\"><a href=\"#添加flannel的网络地址范围到etcd中\" class=\"headerlink\" title=\"添加flannel的网络地址范围到etcd中\"></a>添加flannel的网络地址范围到etcd中</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">./etcdctl mk /flannel/network/config &apos;&#123;&quot;Network&quot;:&quot;172.1.0.0/16&quot;&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>其中 /flannel/network/config这个地址是可以自己指定的，需要配置到flannel中</p>\n<h2 id=\"Kubernetes\"><a href=\"#Kubernetes\" class=\"headerlink\" title=\"Kubernetes\"></a>Kubernetes</h2><h3 id=\"下载并配置kubernetes\"><a href=\"#下载并配置kubernetes\" class=\"headerlink\" title=\"下载并配置kubernetes\"></a>下载并配置kubernetes</h3><p>kubernetes因为发布节奏很快，所以各种仓库都不会及时更新，所以本文采用手动安装方式，安装包可以直接在github上下载</p>\n<p>下载地址：<a href=\"https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\" target=\"_blank\" rel=\"noopener\">kubernetes v1.4.6</a></p>\n<p>这里需要注意，kubernetes v1.5以后不再提供bin文件，所以你如果下载1.5以后的版本需要另外下载bin文件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz</span><br><span class=\"line\">tar zxf kubernetes.tar.gz</span><br><span class=\"line\">cd kubernetes/server</span><br><span class=\"line\">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class=\"line\">cd kubernetes/server/bin</span><br><span class=\"line\">mkdir -p ~/kubernetes-master/log</span><br><span class=\"line\">cp kube-apiserver kube-controller-manager kubectl kube-dns kube-scheduler ~/kubernetes-master/</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span> 将bin添加到path中</span><br><span class=\"line\">echo \"export PATH=~/kubernetes-master:\\$PATH\" &gt;&gt; ~/.bash_profile</span><br><span class=\"line\"></span><br><span class=\"line\">source ~/.bash_profile</span><br></pre></td></tr></table></figure>\n<h3 id=\"init配置文件\"><a href=\"#init配置文件\" class=\"headerlink\" title=\"init配置文件\"></a>init配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; init.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_MASTER=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\"># Change to root auth</span><br><span class=\"line\">sudo chown root.root kube-dns</span><br><span class=\"line\">sudo chmod u+s kube-dns</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x init.sh</span><br><span class=\"line\">./init.sh</span><br></pre></td></tr></table></figure>\n<p>kube-dns需要使用root权限</p>\n<h3 id=\"api启动配置文件\"><a href=\"#api启动配置文件\" class=\"headerlink\" title=\"api启动配置文件\"></a>api启动配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_api.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\"># start the main server of k8s master</span><br><span class=\"line\">sudo ./kube-apiserver \\</span><br><span class=\"line\">    --bind-address=0.0.0.0 \\</span><br><span class=\"line\">    --insecure-bind-address=0.0.0.0 \\</span><br><span class=\"line\">    --insecure-port=8080 \\</span><br><span class=\"line\">    --etcd_servers=http://10.10.0.1:2379 \\</span><br><span class=\"line\">    --allow-privileged=true \\</span><br><span class=\"line\">    --service-cluster-ip-range=182.1.0.0/16 \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-api.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_api.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"controller-manager配置文件\"><a href=\"#controller-manager配置文件\" class=\"headerlink\" title=\"controller-manager配置文件\"></a>controller-manager配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_cm.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">./kube-controller-manager \\</span><br><span class=\"line\">    --master=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-cm.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_cm.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"dns配置文件\"><a href=\"#dns配置文件\" class=\"headerlink\" title=\"dns配置文件\"></a>dns配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_dns.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_DOMAIN=&quot;cluster.local&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kube-dns \\</span><br><span class=\"line\">    --domain=$&#123;KUBE_DOMAIN&#125; \\</span><br><span class=\"line\">    --kube-master-url=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --dns-port=53 \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-dns.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_dns.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"scheduler配置文件\"><a href=\"#scheduler配置文件\" class=\"headerlink\" title=\"scheduler配置文件\"></a>scheduler配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_scd.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">./kube-scheduler \\</span><br><span class=\"line\">    --master=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-scd.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_scd.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"start-master\"><a href=\"#start-master\" class=\"headerlink\" title=\"start master\"></a>start master</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">./start_etcd.sh</span><br><span class=\"line\"></span><br><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">./start_k8s_api.sh</span><br><span class=\"line\">./start_k8s_dns.sh</span><br><span class=\"line\">./start_k8s_cm.sh</span><br><span class=\"line\">./start_k8s_scd.sh</span><br></pre></td></tr></table></figure>\n<h1 id=\"on-minions\"><a href=\"#on-minions\" class=\"headerlink\" title=\"on minions\"></a>on minions</h1><p>minion需要部署所有机器上<br>master上也需要部署minion</p>\n<h2 id=\"flannel\"><a href=\"#flannel\" class=\"headerlink\" title=\"flannel\"></a>flannel</h2><h3 id=\"Download-amp-amp-Install-flannel\"><a href=\"#Download-amp-amp-Install-flannel\" class=\"headerlink\" title=\"Download &amp;&amp; Install flannel\"></a>Download &amp;&amp; Install flannel</h3><p><a href=\"https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz\" target=\"_blank\" rel=\"noopener\">flannel v0.6.1</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">wget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz</span><br><span class=\"line\">tar -zxf flannel-v0.6.1-linux-amd64.tar.gz</span><br><span class=\"line\">mkdir flannel</span><br><span class=\"line\">mv flannel-v0.6.1-linux-amd64.tar.gz flannel</span><br><span class=\"line\">cd ~/flannel/</span><br><span class=\"line\">tar -zxf flannel-v0.6.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动配置脚本-1\"><a href=\"#启动配置脚本-1\" class=\"headerlink\" title=\"启动配置脚本\"></a>启动配置脚本</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/flannel</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; flanneld-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_PREFIX=/flannel/network</span><br><span class=\"line\"></span><br><span class=\"line\"># delete docker0. it will recreate in docker starting</span><br><span class=\"line\">rc=0</span><br><span class=\"line\">ip link show docker0 &gt;/dev/null 2&gt;&amp;1 || rc=&quot;$?&quot;</span><br><span class=\"line\">if [[ &quot;$rc&quot; -eq &quot;0&quot; ]]; then</span><br><span class=\"line\">  sudo ip link set dev docker0 down</span><br><span class=\"line\">  sudo ip link delete docker0</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./flanneld \\</span><br><span class=\"line\">    --ip-masq \\</span><br><span class=\"line\">    --subnet-file=&quot;run/subnet.env&quot; \\</span><br><span class=\"line\">    --etcd-endpoints=http://10.10.0.1:2379 \\</span><br><span class=\"line\">    --etcd-prefix=$ETCD_PREFIX &gt; flanneld.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x flanneld-start.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"kubernetes\"><a href=\"#kubernetes\" class=\"headerlink\" title=\"kubernetes\"></a>kubernetes</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz</span><br><span class=\"line\">tar zxf kubernetes.tar.gz</span><br><span class=\"line\">cd kubernetes/server</span><br><span class=\"line\">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class=\"line\">cd kubernetes/server/bin</span><br><span class=\"line\">mkdir -p ~/kubernetes/log</span><br><span class=\"line\">mkdir -p ~/kubernetes/cert</span><br><span class=\"line\">mv kubelet kube-proxy ~/kubernetes/</span><br></pre></td></tr></table></figure>\n<h3 id=\"kubeconfig配置文件\"><a href=\"#kubeconfig配置文件\" class=\"headerlink\" title=\"kubeconfig配置文件\"></a>kubeconfig配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt; /home/k8s/kubernetes/kubeconfig</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    insecure-skip-tls-verify: true</span><br><span class=\"line\">    server: https://10.10.0.1:8080</span><br><span class=\"line\">  name: k8s-cluster</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: k8s-cluster</span><br><span class=\"line\">    namespace: default</span><br><span class=\"line\">  name: dft</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: k8s-cluster</span><br><span class=\"line\">    namespace: kube-system</span><br><span class=\"line\">  name: sys</span><br><span class=\"line\">current-context: dft</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h3 id=\"kubelet配置文件\"><a href=\"#kubelet配置文件\" class=\"headerlink\" title=\"kubelet配置文件\"></a>kubelet配置文件</h3><p>NODE_IP分别指定两台minion机器，分别是10.10.0.2和10.10.0.3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; k8s-let-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_MASTER=10.10.0.1</span><br><span class=\"line\">NODE_IP=10.10.0.1</span><br><span class=\"line\">CLUSTER_DOMAIN=cluster.local</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kubelet \\</span><br><span class=\"line\">    --address=0.0.0.0 \\</span><br><span class=\"line\">    --port=10250 \\</span><br><span class=\"line\">    --hostname_override=$NODE_IP \\</span><br><span class=\"line\">    --require-kubeconfig=true \\</span><br><span class=\"line\">    --kubeconfig=&quot;/home/k8s/kubernetes/kubeconfig&quot; \\</span><br><span class=\"line\">    --cni-bin-dir=cni/bin \\</span><br><span class=\"line\">    --cni-conf-dir=cni/net.d \\</span><br><span class=\"line\">    --logtostderr=false \\</span><br><span class=\"line\">    --allow-privileged=true \\</span><br><span class=\"line\">    --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\</span><br><span class=\"line\">    --cluster-dns=$KUBE_MASTER \\</span><br><span class=\"line\">    --cluster-domain=$CLUSTER_DOMAIN \\</span><br><span class=\"line\">    --pod-manifest-path=&quot;manifests&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-let.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x k8s-let-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"kube-proxy配置文件\"><a href=\"#kube-proxy配置文件\" class=\"headerlink\" title=\"kube-proxy配置文件\"></a>kube-proxy配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; k8s-proxy-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">NODE_IP=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kube-proxy \\</span><br><span class=\"line\">    --kubeconfig=&quot;/home/k8s/kubernetes/kubeconfig&quot; \\</span><br><span class=\"line\">    --hostname_override=$NODE_IP \\</span><br><span class=\"line\">    --proxy-mode=iptables \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-proxy.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x k8s-proxy-start.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"Init-amp-Start-minion\"><a href=\"#Init-amp-Start-minion\" class=\"headerlink\" title=\"Init &amp; Start minion\"></a>Init &amp; Start minion</h2><h3 id=\"配置flannel和docker0\"><a href=\"#配置flannel和docker0\" class=\"headerlink\" title=\"配置flannel和docker0\"></a>配置flannel和docker0</h3><p>启动flannel</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/flannel</span><br><span class=\"line\">./start_flanneld.sh</span><br></pre></td></tr></table></figure>\n<p>如果是第一次启动需要进行docker的ip配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./mk-docker-opts.sh -f ./run/subnet.env -d ./run/docker.env</span><br><span class=\"line\">cat ./run/docker.env | grep -i DOCKER_OPTS</span><br></pre></td></tr></table></figure>\n<p>记录下上面命令输出的内容，内容大概如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--bip=172.17.95.1/24 --mtu=1472</span><br></pre></td></tr></table></figure>\n<p>编辑docker-network，并修改DOCKER_NETWORK_OPTIONS参数，填入上面输出的内容</p>\n<p>类似与下面的结果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vi /etc/sysconfig/docker-network</span><br><span class=\"line\">DOCKER_NETWORK_OPTIONS=&quot; --bip=172.17.95.1/24 --mtu=1472&quot;</span><br></pre></td></tr></table></figure>\n<p>上述步骤是用于配置docker0中可分配的网段，如果使用yum install flannel在启动时会自动配置</p>\n<p>重新启动docker</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>通过ip a确定docker0和flannel0是在同一个网段</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ip a</span><br></pre></td></tr></table></figure>\n<h3 id=\"start-kubelet\"><a href=\"#start-kubelet\" class=\"headerlink\" title=\"start kubelet\"></a>start kubelet</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">./start_k8s_let.sh</span><br><span class=\"line\">./start_k8s_proxy.sh</span><br></pre></td></tr></table></figure>\n<p>相同的步骤配置10.10.0.2和10.10.0.3<br>配置完成后三个节点都会加入到集群中</p>\n<h1 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h1><p>登录到10.10.0.1上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl get no</span><br></pre></td></tr></table></figure>\n<p>检查上个节点是否已配置完成</p>\n<p>Done</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"环境信息\"><a href=\"#环境信息\" class=\"headerlink\" title=\"环境信息\"></a>环境信息</h1><table>\n<thead>\n<tr>\n<th>ip</th>\n<th>role</th>\n<th>hostname</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10.10.0.1</td>\n<td>master&amp;minion</td>\n<td>k8s-master-1</td>\n</tr>\n<tr>\n<td>10.10.0.2</td>\n<td>minion</td>\n<td>k8s-node-1</td>\n</tr>\n<tr>\n<td>10.10.0.3</td>\n<td>minion</td>\n<td>k8s-node-2</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"on-all-machine\"><a href=\"#on-all-machine\" class=\"headerlink\" title=\"on all machine\"></a>on all machine</h1><p><em>Note: 部署的时候最好关闭selinux，否则像 nfs会挂载失败(nfs用于使用pv，不是必须的)</em></p>\n<h2 id=\"修改hostname\"><a href=\"#修改hostname\" class=\"headerlink\" title=\"修改hostname\"></a>修改hostname</h2><p>用户名建议使用中横杠，因为后面在进行kubernetes配置的时候可以直接使用hostname作为节点名称</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo \"k8s-node-xx\" &gt; /etc/hostname</span><br></pre></td></tr></table></figure>\n<h2 id=\"基本配置\"><a href=\"#基本配置\" class=\"headerlink\" title=\"基本配置\"></a>基本配置</h2><p>关闭防火墙<br>安装ntp</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl stop firewalld</span><br><span class=\"line\">sudo systemctl disable firewalld</span><br><span class=\"line\"></span><br><span class=\"line\">sudo yum -y install ntp</span><br><span class=\"line\">sudo systemctl start ntpd</span><br><span class=\"line\">sudo systemctl enable ntpd</span><br><span class=\"line\">sudo systemctl status ntpd</span><br></pre></td></tr></table></figure>\n<p>创建k8s帐号和sudo</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo useradd -u 1008 k8s -m &amp;&amp; sudo passwd k8s</span><br><span class=\"line\"></span><br><span class=\"line\">sudo echo &quot;&quot; &gt;&gt; /etc/sudoers</span><br><span class=\"line\">sudo echo &quot;#add k8s user&quot; &gt;&gt; /etc/sudoers</span><br><span class=\"line\">sudo echo &quot;k8s ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoers</span><br></pre></td></tr></table></figure>\n<p>从下面开始所以操作都是在k8s这个用户下进行</p>\n<h1 id=\"on-master\"><a href=\"#on-master\" class=\"headerlink\" title=\"on master\"></a>on master</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><h3 id=\"Download-amp-amp-Install-etcd\"><a href=\"#Download-amp-amp-Install-etcd\" class=\"headerlink\" title=\"Download &amp;&amp; Install etcd\"></a>Download &amp;&amp; Install etcd</h3><p>etcd是类似与zookeeper的分布式配置存储解决方案，特意加了“配置”两个字是因为它的设计目标主要是用来存储配置文件</p>\n<p>下载地址：<a href=\"https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz\" target=\"_blank\" rel=\"noopener\">etcd v3.0.9</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/coreos/etcd/releases/download/v3.0.9/etcd-v3.0.9-linux-amd64.tar.gz</span><br><span class=\"line\">tar -zxf etcd-v3.0.9-linux-amd64.tar.gz</span><br><span class=\"line\">ln -s etcd-v3.0.9-linux-amd64 etcd</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动配置脚本\"><a href=\"#启动配置脚本\" class=\"headerlink\" title=\"启动配置脚本\"></a>启动配置脚本</h3><p>因为这是实验环境，所以这里的etcd只使用一个节点，如果你是用于生产环境建议配置3个以上的etcd节点</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; etcd-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">THIS_IP=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\">./etcd \\</span><br><span class=\"line\">--name=infra1 \\</span><br><span class=\"line\">--advertise-client-urls http://$&#123;THIS_IP&#125;:2379 \\</span><br><span class=\"line\">--listen-client-urls http://$&#123;THIS_IP&#125;:2379,http://127.0.0.1:2379 \\</span><br><span class=\"line\">--data-dir=data  \\</span><br><span class=\"line\">1&gt; etcd.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x etcd-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用配置脚本启动etcd\"><a href=\"#使用配置脚本启动etcd\" class=\"headerlink\" title=\"使用配置脚本启动etcd\"></a>使用配置脚本启动etcd</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./etcd-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"测试etcd\"><a href=\"#测试etcd\" class=\"headerlink\" title=\"测试etcd\"></a>测试etcd</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./etcdctl ls</span><br></pre></td></tr></table></figure>\n<p>如果上述命令成功，那么etcd处于正常运行中</p>\n<h3 id=\"添加flannel的网络地址范围到etcd中\"><a href=\"#添加flannel的网络地址范围到etcd中\" class=\"headerlink\" title=\"添加flannel的网络地址范围到etcd中\"></a>添加flannel的网络地址范围到etcd中</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">./etcdctl mk /flannel/network/config &apos;&#123;&quot;Network&quot;:&quot;172.1.0.0/16&quot;&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>其中 /flannel/network/config这个地址是可以自己指定的，需要配置到flannel中</p>\n<h2 id=\"Kubernetes\"><a href=\"#Kubernetes\" class=\"headerlink\" title=\"Kubernetes\"></a>Kubernetes</h2><h3 id=\"下载并配置kubernetes\"><a href=\"#下载并配置kubernetes\" class=\"headerlink\" title=\"下载并配置kubernetes\"></a>下载并配置kubernetes</h3><p>kubernetes因为发布节奏很快，所以各种仓库都不会及时更新，所以本文采用手动安装方式，安装包可以直接在github上下载</p>\n<p>下载地址：<a href=\"https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz\" target=\"_blank\" rel=\"noopener\">kubernetes v1.4.6</a></p>\n<p>这里需要注意，kubernetes v1.5以后不再提供bin文件，所以你如果下载1.5以后的版本需要另外下载bin文件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz</span><br><span class=\"line\">tar zxf kubernetes.tar.gz</span><br><span class=\"line\">cd kubernetes/server</span><br><span class=\"line\">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class=\"line\">cd kubernetes/server/bin</span><br><span class=\"line\">mkdir -p ~/kubernetes-master/log</span><br><span class=\"line\">cp kube-apiserver kube-controller-manager kubectl kube-dns kube-scheduler ~/kubernetes-master/</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#</span> 将bin添加到path中</span><br><span class=\"line\">echo \"export PATH=~/kubernetes-master:\\$PATH\" &gt;&gt; ~/.bash_profile</span><br><span class=\"line\"></span><br><span class=\"line\">source ~/.bash_profile</span><br></pre></td></tr></table></figure>\n<h3 id=\"init配置文件\"><a href=\"#init配置文件\" class=\"headerlink\" title=\"init配置文件\"></a>init配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; init.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_MASTER=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\"># Change to root auth</span><br><span class=\"line\">sudo chown root.root kube-dns</span><br><span class=\"line\">sudo chmod u+s kube-dns</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x init.sh</span><br><span class=\"line\">./init.sh</span><br></pre></td></tr></table></figure>\n<p>kube-dns需要使用root权限</p>\n<h3 id=\"api启动配置文件\"><a href=\"#api启动配置文件\" class=\"headerlink\" title=\"api启动配置文件\"></a>api启动配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_api.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\"># start the main server of k8s master</span><br><span class=\"line\">sudo ./kube-apiserver \\</span><br><span class=\"line\">    --bind-address=0.0.0.0 \\</span><br><span class=\"line\">    --insecure-bind-address=0.0.0.0 \\</span><br><span class=\"line\">    --insecure-port=8080 \\</span><br><span class=\"line\">    --etcd_servers=http://10.10.0.1:2379 \\</span><br><span class=\"line\">    --allow-privileged=true \\</span><br><span class=\"line\">    --service-cluster-ip-range=182.1.0.0/16 \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-api.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_api.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"controller-manager配置文件\"><a href=\"#controller-manager配置文件\" class=\"headerlink\" title=\"controller-manager配置文件\"></a>controller-manager配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_cm.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">./kube-controller-manager \\</span><br><span class=\"line\">    --master=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-cm.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_cm.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"dns配置文件\"><a href=\"#dns配置文件\" class=\"headerlink\" title=\"dns配置文件\"></a>dns配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_dns.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_DOMAIN=&quot;cluster.local&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kube-dns \\</span><br><span class=\"line\">    --domain=$&#123;KUBE_DOMAIN&#125; \\</span><br><span class=\"line\">    --kube-master-url=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --dns-port=53 \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-dns.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_dns.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"scheduler配置文件\"><a href=\"#scheduler配置文件\" class=\"headerlink\" title=\"scheduler配置文件\"></a>scheduler配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; start_k8s_scd.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">./kube-scheduler \\</span><br><span class=\"line\">    --master=&quot;http://127.0.0.1:8080&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-scd.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x start_k8s_scd.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"start-master\"><a href=\"#start-master\" class=\"headerlink\" title=\"start master\"></a>start master</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/etcd</span><br><span class=\"line\">./start_etcd.sh</span><br><span class=\"line\"></span><br><span class=\"line\">cd ~/kubernetes-master</span><br><span class=\"line\">./start_k8s_api.sh</span><br><span class=\"line\">./start_k8s_dns.sh</span><br><span class=\"line\">./start_k8s_cm.sh</span><br><span class=\"line\">./start_k8s_scd.sh</span><br></pre></td></tr></table></figure>\n<h1 id=\"on-minions\"><a href=\"#on-minions\" class=\"headerlink\" title=\"on minions\"></a>on minions</h1><p>minion需要部署所有机器上<br>master上也需要部署minion</p>\n<h2 id=\"flannel\"><a href=\"#flannel\" class=\"headerlink\" title=\"flannel\"></a>flannel</h2><h3 id=\"Download-amp-amp-Install-flannel\"><a href=\"#Download-amp-amp-Install-flannel\" class=\"headerlink\" title=\"Download &amp;&amp; Install flannel\"></a>Download &amp;&amp; Install flannel</h3><p><a href=\"https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz\" target=\"_blank\" rel=\"noopener\">flannel v0.6.1</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~</span><br><span class=\"line\">wget https://github.com/coreos/flannel/releases/download/v0.6.1/flannel-v0.6.1-linux-amd64.tar.gz</span><br><span class=\"line\">tar -zxf flannel-v0.6.1-linux-amd64.tar.gz</span><br><span class=\"line\">mkdir flannel</span><br><span class=\"line\">mv flannel-v0.6.1-linux-amd64.tar.gz flannel</span><br><span class=\"line\">cd ~/flannel/</span><br><span class=\"line\">tar -zxf flannel-v0.6.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动配置脚本-1\"><a href=\"#启动配置脚本-1\" class=\"headerlink\" title=\"启动配置脚本\"></a>启动配置脚本</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/flannel</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; flanneld-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_PREFIX=/flannel/network</span><br><span class=\"line\"></span><br><span class=\"line\"># delete docker0. it will recreate in docker starting</span><br><span class=\"line\">rc=0</span><br><span class=\"line\">ip link show docker0 &gt;/dev/null 2&gt;&amp;1 || rc=&quot;$?&quot;</span><br><span class=\"line\">if [[ &quot;$rc&quot; -eq &quot;0&quot; ]]; then</span><br><span class=\"line\">  sudo ip link set dev docker0 down</span><br><span class=\"line\">  sudo ip link delete docker0</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./flanneld \\</span><br><span class=\"line\">    --ip-masq \\</span><br><span class=\"line\">    --subnet-file=&quot;run/subnet.env&quot; \\</span><br><span class=\"line\">    --etcd-endpoints=http://10.10.0.1:2379 \\</span><br><span class=\"line\">    --etcd-prefix=$ETCD_PREFIX &gt; flanneld.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x flanneld-start.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"kubernetes\"><a href=\"#kubernetes\" class=\"headerlink\" title=\"kubernetes\"></a>kubernetes</h2><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/kubernetes/kubernetes/releases/download/v1.4.6/kubernetes.tar.gz</span><br><span class=\"line\">tar zxf kubernetes.tar.gz</span><br><span class=\"line\">cd kubernetes/server</span><br><span class=\"line\">tar zxf kubernetes-server-linux-amd64.tar.gz</span><br><span class=\"line\">cd kubernetes/server/bin</span><br><span class=\"line\">mkdir -p ~/kubernetes/log</span><br><span class=\"line\">mkdir -p ~/kubernetes/cert</span><br><span class=\"line\">mv kubelet kube-proxy ~/kubernetes/</span><br></pre></td></tr></table></figure>\n<h3 id=\"kubeconfig配置文件\"><a href=\"#kubeconfig配置文件\" class=\"headerlink\" title=\"kubeconfig配置文件\"></a>kubeconfig配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt; /home/k8s/kubernetes/kubeconfig</span><br><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    insecure-skip-tls-verify: true</span><br><span class=\"line\">    server: https://10.10.0.1:8080</span><br><span class=\"line\">  name: k8s-cluster</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: k8s-cluster</span><br><span class=\"line\">    namespace: default</span><br><span class=\"line\">  name: dft</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: k8s-cluster</span><br><span class=\"line\">    namespace: kube-system</span><br><span class=\"line\">  name: sys</span><br><span class=\"line\">current-context: dft</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h3 id=\"kubelet配置文件\"><a href=\"#kubelet配置文件\" class=\"headerlink\" title=\"kubelet配置文件\"></a>kubelet配置文件</h3><p>NODE_IP分别指定两台minion机器，分别是10.10.0.2和10.10.0.3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; k8s-let-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_MASTER=10.10.0.1</span><br><span class=\"line\">NODE_IP=10.10.0.1</span><br><span class=\"line\">CLUSTER_DOMAIN=cluster.local</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kubelet \\</span><br><span class=\"line\">    --address=0.0.0.0 \\</span><br><span class=\"line\">    --port=10250 \\</span><br><span class=\"line\">    --hostname_override=$NODE_IP \\</span><br><span class=\"line\">    --require-kubeconfig=true \\</span><br><span class=\"line\">    --kubeconfig=&quot;/home/k8s/kubernetes/kubeconfig&quot; \\</span><br><span class=\"line\">    --cni-bin-dir=cni/bin \\</span><br><span class=\"line\">    --cni-conf-dir=cni/net.d \\</span><br><span class=\"line\">    --logtostderr=false \\</span><br><span class=\"line\">    --allow-privileged=true \\</span><br><span class=\"line\">    --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\</span><br><span class=\"line\">    --cluster-dns=$KUBE_MASTER \\</span><br><span class=\"line\">    --cluster-domain=$CLUSTER_DOMAIN \\</span><br><span class=\"line\">    --pod-manifest-path=&quot;manifests&quot; \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-let.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x k8s-let-start.sh</span><br></pre></td></tr></table></figure>\n<h3 id=\"kube-proxy配置文件\"><a href=\"#kube-proxy配置文件\" class=\"headerlink\" title=\"kube-proxy配置文件\"></a>kube-proxy配置文件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">cat &lt;&lt;EOF &gt; k8s-proxy-start.sh</span><br><span class=\"line\">#! /bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">NODE_IP=10.10.0.1</span><br><span class=\"line\"></span><br><span class=\"line\">sudo ./kube-proxy \\</span><br><span class=\"line\">    --kubeconfig=&quot;/home/k8s/kubernetes/kubeconfig&quot; \\</span><br><span class=\"line\">    --hostname_override=$NODE_IP \\</span><br><span class=\"line\">    --proxy-mode=iptables \\</span><br><span class=\"line\">    --log-dir=&quot;log&quot; \\</span><br><span class=\"line\">1&gt; k8s-proxy.log 2&gt;&amp;1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">chmod a+x k8s-proxy-start.sh</span><br></pre></td></tr></table></figure>\n<h2 id=\"Init-amp-Start-minion\"><a href=\"#Init-amp-Start-minion\" class=\"headerlink\" title=\"Init &amp; Start minion\"></a>Init &amp; Start minion</h2><h3 id=\"配置flannel和docker0\"><a href=\"#配置flannel和docker0\" class=\"headerlink\" title=\"配置flannel和docker0\"></a>配置flannel和docker0</h3><p>启动flannel</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/flannel</span><br><span class=\"line\">./start_flanneld.sh</span><br></pre></td></tr></table></figure>\n<p>如果是第一次启动需要进行docker的ip配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./mk-docker-opts.sh -f ./run/subnet.env -d ./run/docker.env</span><br><span class=\"line\">cat ./run/docker.env | grep -i DOCKER_OPTS</span><br></pre></td></tr></table></figure>\n<p>记录下上面命令输出的内容，内容大概如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--bip=172.17.95.1/24 --mtu=1472</span><br></pre></td></tr></table></figure>\n<p>编辑docker-network，并修改DOCKER_NETWORK_OPTIONS参数，填入上面输出的内容</p>\n<p>类似与下面的结果：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vi /etc/sysconfig/docker-network</span><br><span class=\"line\">DOCKER_NETWORK_OPTIONS=&quot; --bip=172.17.95.1/24 --mtu=1472&quot;</span><br></pre></td></tr></table></figure>\n<p>上述步骤是用于配置docker0中可分配的网段，如果使用yum install flannel在启动时会自动配置</p>\n<p>重新启动docker</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>通过ip a确定docker0和flannel0是在同一个网段</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ip a</span><br></pre></td></tr></table></figure>\n<h3 id=\"start-kubelet\"><a href=\"#start-kubelet\" class=\"headerlink\" title=\"start kubelet\"></a>start kubelet</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/kubernetes</span><br><span class=\"line\">./start_k8s_let.sh</span><br><span class=\"line\">./start_k8s_proxy.sh</span><br></pre></td></tr></table></figure>\n<p>相同的步骤配置10.10.0.2和10.10.0.3<br>配置完成后三个节点都会加入到集群中</p>\n<h1 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h1><p>登录到10.10.0.1上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl get no</span><br></pre></td></tr></table></figure>\n<p>检查上个节点是否已配置完成</p>\n<p>Done</p>\n"},{"title":"Ingress Controller with Traefik on Kubernetes","date":"2017-03-02T16:00:00.000Z","_content":"\n# Kubernetes [Ingress](https://kubernetes.io/docs/user-guide/ingress/)\n\nkubernetes典型的访问是通过网络直接访问Service，由Service分发到pod上:\n\n```\n    internet\n        |\n  ------------\n  [ Services ]\n```\n\n如果增加Ingress:\n\n```\n    internet\n        |\n   [ Ingress ]\n   --|-----|--\n   [ Services ]\n```\n\nIngress部署在Service前面进行域名的解析和分发\n\n# Ingress Controller\n\n上文提到的Ingress对象实际上只是kubernetes中的一些配置文件，真正实现Ingress功能的实际是Ingress Controller。\n\nIngress Controller的功能主要有：\n- 访问kubernetes api，来感知Service的变化和Ingress的配置\n- 当感知到Service变化时，将配置应用的本身的配置文件中\n- Reload配置，实现分发\n\n目前Kubernetes官方提供gce和nginx的Controller，相见：[Ingress](https://github.com/kubernetes/ingress/tree/master/controllers)\n\n# [Traefik](https://github.com/containous/traefik)\n\ntraefik是一个为容器而生的反向代理和负载均衡服务，很轻量，采用go语言。可以支持很多容器相关架构，如：Consul, Etcd, Docker, Kubernetes, Messos等等\n\n另外，traefik支持Let\\'s Encrypt\n\n# Traefik deploy on Kubernetes\n\n## traefik deployment\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 80\n    nodePort: 80\n  selector:\n    k8s-app: traefik-ingress-controller\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress-controller\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-controller\n        name: traefik-ingress-controller\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: traefik:v1.1.2-alpine\n        name: traefik-ingress-controller\n        resources:\n          limits:\n            cpu: 200m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        ports:\n        - name: http\n          containerPort: 80\n        - name: admin\n          containerPort: 8081\n        args:\n        - -d\n        - --web\n        - --web.address=:8081\n        - --kubernetes\n        - --kubernetes.endpoint=http://139.119.3.113:6550/\n```\n\n`web.address=:8081`指示启动traefik的web端口\n`kubernetes.endpoint`指向kubernetes的api地址（apiserver）\n\n这里部署了2个traefik，是为了在集群中作为HA\n\nservice采用nodePort方式，在所有node上映射80端口（因为域名通常不需要端口，如果你指定了其他端口那么域名增加相应的端口即可）。为了指定80端口需要开发kubernetes api的限制，配置参数：`--service-node-port-range=1-65535`\n\n## deploy traefik ui\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  ports:\n  - name: web\n    port: 80\n    targetPort: 8081\n  selector:\n    k8s-app: traefik-ingress-controller\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  rules:\n  - host: traefik-ui.local.io\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: traefik-web-ui\n          servicePort: web\n```\n\n第一部分是traefik ui的service，ingress需要转发到service上\n第二部分创建ingress，如果访问的是指定域名traefik-ui.local.io，那么重定向到traefik-web-ui这个service的web端口上\n\n## 验证traefik ui\n\n如果是实际环境只需要将域名配置执行集群中的node的ip即可，默认是80端口\n\n使用curl验证地址\n\n```\ncurl -H \"Host: traefik-ui.local.io\" http://192.168.72.2\n```\n\n`-H` 用于指定访问域名\n\n或者可以直接在浏览器中打开\n\n## deploy testpage app\n\ntestpage是一个python的测试页面，通过http访问该页面会返回一个配置好的版本号\n\nDeployment:\n\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  ports:\n  - name: http\n    port: 8080\n  selector:\n    type: testpage\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: registry.local.com/testpage:0.5\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.5\"\n        ports:\n        - containerPort: 8080\n```\n\nIngress:\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: testpage\n  namespace: default\nspec:\n  rules:\n  - host: testpage.local.io\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: testpage\n          servicePort: http\n```\n\n验证\n\n```\n$ curl -H \"Host: testpage.local.io\" http://192.168.72.2\nI am1: 172.1.14.4, the version is: 0.5\n```\n\n# 参考\n[Kubernetes Ingress Controller](https://docs.traefik.io/user-guide/kubernetes/)\n[Kubernetes : Ingress Controller with Træfɪk and Let's Encrypt](https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html)\n[Traefik-kubernetes 初试](http://www.colabug.com/thread-1703745-1-1.html)\n","source":"_posts/deploy-traefik.md","raw":"title: Ingress Controller with Traefik on Kubernetes\ndate: 2017-03-03\ntags:\n- traefik\n- kubernetes\n---\n\n# Kubernetes [Ingress](https://kubernetes.io/docs/user-guide/ingress/)\n\nkubernetes典型的访问是通过网络直接访问Service，由Service分发到pod上:\n\n```\n    internet\n        |\n  ------------\n  [ Services ]\n```\n\n如果增加Ingress:\n\n```\n    internet\n        |\n   [ Ingress ]\n   --|-----|--\n   [ Services ]\n```\n\nIngress部署在Service前面进行域名的解析和分发\n\n# Ingress Controller\n\n上文提到的Ingress对象实际上只是kubernetes中的一些配置文件，真正实现Ingress功能的实际是Ingress Controller。\n\nIngress Controller的功能主要有：\n- 访问kubernetes api，来感知Service的变化和Ingress的配置\n- 当感知到Service变化时，将配置应用的本身的配置文件中\n- Reload配置，实现分发\n\n目前Kubernetes官方提供gce和nginx的Controller，相见：[Ingress](https://github.com/kubernetes/ingress/tree/master/controllers)\n\n# [Traefik](https://github.com/containous/traefik)\n\ntraefik是一个为容器而生的反向代理和负载均衡服务，很轻量，采用go语言。可以支持很多容器相关架构，如：Consul, Etcd, Docker, Kubernetes, Messos等等\n\n另外，traefik支持Let\\'s Encrypt\n\n# Traefik deploy on Kubernetes\n\n## traefik deployment\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - name: http\n    port: 80\n    nodePort: 80\n  selector:\n    k8s-app: traefik-ingress-controller\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: traefik-ingress-controller\n  namespace: kube-system\n  labels:\n    k8s-app: traefik-ingress-controller\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        k8s-app: traefik-ingress-controller\n        name: traefik-ingress-controller\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - image: traefik:v1.1.2-alpine\n        name: traefik-ingress-controller\n        resources:\n          limits:\n            cpu: 200m\n            memory: 30Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        ports:\n        - name: http\n          containerPort: 80\n        - name: admin\n          containerPort: 8081\n        args:\n        - -d\n        - --web\n        - --web.address=:8081\n        - --kubernetes\n        - --kubernetes.endpoint=http://139.119.3.113:6550/\n```\n\n`web.address=:8081`指示启动traefik的web端口\n`kubernetes.endpoint`指向kubernetes的api地址（apiserver）\n\n这里部署了2个traefik，是为了在集群中作为HA\n\nservice采用nodePort方式，在所有node上映射80端口（因为域名通常不需要端口，如果你指定了其他端口那么域名增加相应的端口即可）。为了指定80端口需要开发kubernetes api的限制，配置参数：`--service-node-port-range=1-65535`\n\n## deploy traefik ui\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  ports:\n  - name: web\n    port: 80\n    targetPort: 8081\n  selector:\n    k8s-app: traefik-ingress-controller\n---\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: traefik-web-ui\n  namespace: kube-system\nspec:\n  rules:\n  - host: traefik-ui.local.io\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: traefik-web-ui\n          servicePort: web\n```\n\n第一部分是traefik ui的service，ingress需要转发到service上\n第二部分创建ingress，如果访问的是指定域名traefik-ui.local.io，那么重定向到traefik-web-ui这个service的web端口上\n\n## 验证traefik ui\n\n如果是实际环境只需要将域名配置执行集群中的node的ip即可，默认是80端口\n\n使用curl验证地址\n\n```\ncurl -H \"Host: traefik-ui.local.io\" http://192.168.72.2\n```\n\n`-H` 用于指定访问域名\n\n或者可以直接在浏览器中打开\n\n## deploy testpage app\n\ntestpage是一个python的测试页面，通过http访问该页面会返回一个配置好的版本号\n\nDeployment:\n\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  ports:\n  - name: http\n    port: 8080\n  selector:\n    type: testpage\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: registry.local.com/testpage:0.5\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.5\"\n        ports:\n        - containerPort: 8080\n```\n\nIngress:\n```yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: testpage\n  namespace: default\nspec:\n  rules:\n  - host: testpage.local.io\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: testpage\n          servicePort: http\n```\n\n验证\n\n```\n$ curl -H \"Host: testpage.local.io\" http://192.168.72.2\nI am1: 172.1.14.4, the version is: 0.5\n```\n\n# 参考\n[Kubernetes Ingress Controller](https://docs.traefik.io/user-guide/kubernetes/)\n[Kubernetes : Ingress Controller with Træfɪk and Let's Encrypt](https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html)\n[Traefik-kubernetes 初试](http://www.colabug.com/thread-1703745-1-1.html)\n","slug":"deploy-traefik","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u200042tsb8bnmcq5u","content":"<h1 id=\"Kubernetes-Ingress\"><a href=\"#Kubernetes-Ingress\" class=\"headerlink\" title=\"Kubernetes Ingress\"></a>Kubernetes <a href=\"https://kubernetes.io/docs/user-guide/ingress/\" target=\"_blank\" rel=\"noopener\">Ingress</a></h1><p>kubernetes典型的访问是通过网络直接访问Service，由Service分发到pod上:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  internet</span><br><span class=\"line\">      |</span><br><span class=\"line\">------------</span><br><span class=\"line\">[ Services ]</span><br></pre></td></tr></table></figure>\n<p>如果增加Ingress:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> internet</span><br><span class=\"line\">     |</span><br><span class=\"line\">[ Ingress ]</span><br><span class=\"line\">--|-----|--</span><br><span class=\"line\">[ Services ]</span><br></pre></td></tr></table></figure>\n<p>Ingress部署在Service前面进行域名的解析和分发</p>\n<h1 id=\"Ingress-Controller\"><a href=\"#Ingress-Controller\" class=\"headerlink\" title=\"Ingress Controller\"></a>Ingress Controller</h1><p>上文提到的Ingress对象实际上只是kubernetes中的一些配置文件，真正实现Ingress功能的实际是Ingress Controller。</p>\n<p>Ingress Controller的功能主要有：</p>\n<ul>\n<li>访问kubernetes api，来感知Service的变化和Ingress的配置</li>\n<li>当感知到Service变化时，将配置应用的本身的配置文件中</li>\n<li>Reload配置，实现分发</li>\n</ul>\n<p>目前Kubernetes官方提供gce和nginx的Controller，相见：<a href=\"https://github.com/kubernetes/ingress/tree/master/controllers\" target=\"_blank\" rel=\"noopener\">Ingress</a></p>\n<h1 id=\"Traefik\"><a href=\"#Traefik\" class=\"headerlink\" title=\"Traefik\"></a><a href=\"https://github.com/containous/traefik\" target=\"_blank\" rel=\"noopener\">Traefik</a></h1><p>traefik是一个为容器而生的反向代理和负载均衡服务，很轻量，采用go语言。可以支持很多容器相关架构，如：Consul, Etcd, Docker, Kubernetes, Messos等等</p>\n<p>另外，traefik支持Let\\’s Encrypt</p>\n<h1 id=\"Traefik-deploy-on-Kubernetes\"><a href=\"#Traefik-deploy-on-Kubernetes\" class=\"headerlink\" title=\"Traefik deploy on Kubernetes\"></a>Traefik deploy on Kubernetes</h1><h2 id=\"traefik-deployment\"><a href=\"#traefik-deployment\" class=\"headerlink\" title=\"traefik deployment\"></a>traefik deployment</h2><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">NodePort</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">    nodePort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">        name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      terminationGracePeriodSeconds:</span> <span class=\"number\">60</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - image:</span> <span class=\"attr\">traefik:v1.1.2-alpine</span></span><br><span class=\"line\"><span class=\"attr\">        name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">200</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">30</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">          requests:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">100</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">          containerPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">admin</span></span><br><span class=\"line\"><span class=\"attr\">          containerPort:</span> <span class=\"number\">8081</span></span><br><span class=\"line\"><span class=\"attr\">        args:</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">-d</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--web</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--web.address=:8081</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--kubernetes</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--kubernetes.endpoint=http://139.119.3.113:6550/</span></span><br></pre></td></tr></table></figure>\n<p><code>web.address=:8081</code>指示启动traefik的web端口<br><code>kubernetes.endpoint</code>指向kubernetes的api地址（apiserver）</p>\n<p>这里部署了2个traefik，是为了在集群中作为HA</p>\n<p>service采用nodePort方式，在所有node上映射80端口（因为域名通常不需要端口，如果你指定了其他端口那么域名增加相应的端口即可）。为了指定80端口需要开发kubernetes api的限制，配置参数：<code>--service-node-port-range=1-65535</code></p>\n<h2 id=\"deploy-traefik-ui\"><a href=\"#deploy-traefik-ui\" class=\"headerlink\" title=\"deploy traefik ui\"></a>deploy traefik ui</h2><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">web</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">    targetPort:</span> <span class=\"number\">8081</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Ingress</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  rules:</span></span><br><span class=\"line\"><span class=\"attr\">  - host:</span> <span class=\"string\">traefik-ui.local.io</span></span><br><span class=\"line\"><span class=\"attr\">    http:</span></span><br><span class=\"line\"><span class=\"attr\">      paths:</span></span><br><span class=\"line\"><span class=\"attr\">      - path:</span> <span class=\"string\">/</span></span><br><span class=\"line\"><span class=\"attr\">        backend:</span></span><br><span class=\"line\"><span class=\"attr\">          serviceName:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">          servicePort:</span> <span class=\"string\">web</span></span><br></pre></td></tr></table></figure>\n<p>第一部分是traefik ui的service，ingress需要转发到service上<br>第二部分创建ingress，如果访问的是指定域名traefik-ui.local.io，那么重定向到traefik-web-ui这个service的web端口上</p>\n<h2 id=\"验证traefik-ui\"><a href=\"#验证traefik-ui\" class=\"headerlink\" title=\"验证traefik ui\"></a>验证traefik ui</h2><p>如果是实际环境只需要将域名配置执行集群中的node的ip即可，默认是80端口</p>\n<p>使用curl验证地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -H &quot;Host: traefik-ui.local.io&quot; http://192.168.72.2</span><br></pre></td></tr></table></figure>\n<p><code>-H</code> 用于指定访问域名</p>\n<p>或者可以直接在浏览器中打开</p>\n<h2 id=\"deploy-testpage-app\"><a href=\"#deploy-testpage-app\" class=\"headerlink\" title=\"deploy testpage app\"></a>deploy testpage app</h2><p>testpage是一个python的测试页面，通过http访问该页面会返回一个配置好的版本号</p>\n<p>Deployment:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">        image:</span> <span class=\"string\">registry.local.com/testpage:0.5</span></span><br><span class=\"line\"><span class=\"attr\">        imagePullPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">10</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        env:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">VERSION</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"string\">\"0.5\"</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - containerPort:</span> <span class=\"number\">8080</span></span><br></pre></td></tr></table></figure>\n<p>Ingress:<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Ingress</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  rules:</span></span><br><span class=\"line\"><span class=\"attr\">  - host:</span> <span class=\"string\">testpage.local.io</span></span><br><span class=\"line\"><span class=\"attr\">    http:</span></span><br><span class=\"line\"><span class=\"attr\">      paths:</span></span><br><span class=\"line\"><span class=\"attr\">      - path:</span> <span class=\"string\">/</span></span><br><span class=\"line\"><span class=\"attr\">        backend:</span></span><br><span class=\"line\"><span class=\"attr\">          serviceName:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">          servicePort:</span> <span class=\"string\">http</span></span><br></pre></td></tr></table></figure></p>\n<p>验证</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H &quot;Host: testpage.local.io&quot; http://192.168.72.2</span><br><span class=\"line\">I am1: 172.1.14.4, the version is: 0.5</span><br></pre></td></tr></table></figure>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://docs.traefik.io/user-guide/kubernetes/\" target=\"_blank\" rel=\"noopener\">Kubernetes Ingress Controller</a><br><a href=\"https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html\" target=\"_blank\" rel=\"noopener\">Kubernetes : Ingress Controller with Træfɪk and Let’s Encrypt</a><br><a href=\"http://www.colabug.com/thread-1703745-1-1.html\" target=\"_blank\" rel=\"noopener\">Traefik-kubernetes 初试</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kubernetes-Ingress\"><a href=\"#Kubernetes-Ingress\" class=\"headerlink\" title=\"Kubernetes Ingress\"></a>Kubernetes <a href=\"https://kubernetes.io/docs/user-guide/ingress/\" target=\"_blank\" rel=\"noopener\">Ingress</a></h1><p>kubernetes典型的访问是通过网络直接访问Service，由Service分发到pod上:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  internet</span><br><span class=\"line\">      |</span><br><span class=\"line\">------------</span><br><span class=\"line\">[ Services ]</span><br></pre></td></tr></table></figure>\n<p>如果增加Ingress:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> internet</span><br><span class=\"line\">     |</span><br><span class=\"line\">[ Ingress ]</span><br><span class=\"line\">--|-----|--</span><br><span class=\"line\">[ Services ]</span><br></pre></td></tr></table></figure>\n<p>Ingress部署在Service前面进行域名的解析和分发</p>\n<h1 id=\"Ingress-Controller\"><a href=\"#Ingress-Controller\" class=\"headerlink\" title=\"Ingress Controller\"></a>Ingress Controller</h1><p>上文提到的Ingress对象实际上只是kubernetes中的一些配置文件，真正实现Ingress功能的实际是Ingress Controller。</p>\n<p>Ingress Controller的功能主要有：</p>\n<ul>\n<li>访问kubernetes api，来感知Service的变化和Ingress的配置</li>\n<li>当感知到Service变化时，将配置应用的本身的配置文件中</li>\n<li>Reload配置，实现分发</li>\n</ul>\n<p>目前Kubernetes官方提供gce和nginx的Controller，相见：<a href=\"https://github.com/kubernetes/ingress/tree/master/controllers\" target=\"_blank\" rel=\"noopener\">Ingress</a></p>\n<h1 id=\"Traefik\"><a href=\"#Traefik\" class=\"headerlink\" title=\"Traefik\"></a><a href=\"https://github.com/containous/traefik\" target=\"_blank\" rel=\"noopener\">Traefik</a></h1><p>traefik是一个为容器而生的反向代理和负载均衡服务，很轻量，采用go语言。可以支持很多容器相关架构，如：Consul, Etcd, Docker, Kubernetes, Messos等等</p>\n<p>另外，traefik支持Let\\’s Encrypt</p>\n<h1 id=\"Traefik-deploy-on-Kubernetes\"><a href=\"#Traefik-deploy-on-Kubernetes\" class=\"headerlink\" title=\"Traefik deploy on Kubernetes\"></a>Traefik deploy on Kubernetes</h1><h2 id=\"traefik-deployment\"><a href=\"#traefik-deployment\" class=\"headerlink\" title=\"traefik deployment\"></a>traefik deployment</h2><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">NodePort</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">    nodePort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">        name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      terminationGracePeriodSeconds:</span> <span class=\"number\">60</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - image:</span> <span class=\"attr\">traefik:v1.1.2-alpine</span></span><br><span class=\"line\"><span class=\"attr\">        name:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">200</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">30</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">          requests:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">100</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">          containerPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">admin</span></span><br><span class=\"line\"><span class=\"attr\">          containerPort:</span> <span class=\"number\">8081</span></span><br><span class=\"line\"><span class=\"attr\">        args:</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">-d</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--web</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--web.address=:8081</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--kubernetes</span></span><br><span class=\"line\"><span class=\"bullet\">        -</span> <span class=\"bullet\">--kubernetes.endpoint=http://139.119.3.113:6550/</span></span><br></pre></td></tr></table></figure>\n<p><code>web.address=:8081</code>指示启动traefik的web端口<br><code>kubernetes.endpoint</code>指向kubernetes的api地址（apiserver）</p>\n<p>这里部署了2个traefik，是为了在集群中作为HA</p>\n<p>service采用nodePort方式，在所有node上映射80端口（因为域名通常不需要端口，如果你指定了其他端口那么域名增加相应的端口即可）。为了指定80端口需要开发kubernetes api的限制，配置参数：<code>--service-node-port-range=1-65535</code></p>\n<h2 id=\"deploy-traefik-ui\"><a href=\"#deploy-traefik-ui\" class=\"headerlink\" title=\"deploy traefik ui\"></a>deploy traefik ui</h2><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">web</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"attr\">    targetPort:</span> <span class=\"number\">8081</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    k8s-app:</span> <span class=\"string\">traefik-ingress-controller</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Ingress</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">kube-system</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  rules:</span></span><br><span class=\"line\"><span class=\"attr\">  - host:</span> <span class=\"string\">traefik-ui.local.io</span></span><br><span class=\"line\"><span class=\"attr\">    http:</span></span><br><span class=\"line\"><span class=\"attr\">      paths:</span></span><br><span class=\"line\"><span class=\"attr\">      - path:</span> <span class=\"string\">/</span></span><br><span class=\"line\"><span class=\"attr\">        backend:</span></span><br><span class=\"line\"><span class=\"attr\">          serviceName:</span> <span class=\"string\">traefik-web-ui</span></span><br><span class=\"line\"><span class=\"attr\">          servicePort:</span> <span class=\"string\">web</span></span><br></pre></td></tr></table></figure>\n<p>第一部分是traefik ui的service，ingress需要转发到service上<br>第二部分创建ingress，如果访问的是指定域名traefik-ui.local.io，那么重定向到traefik-web-ui这个service的web端口上</p>\n<h2 id=\"验证traefik-ui\"><a href=\"#验证traefik-ui\" class=\"headerlink\" title=\"验证traefik ui\"></a>验证traefik ui</h2><p>如果是实际环境只需要将域名配置执行集群中的node的ip即可，默认是80端口</p>\n<p>使用curl验证地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -H &quot;Host: traefik-ui.local.io&quot; http://192.168.72.2</span><br></pre></td></tr></table></figure>\n<p><code>-H</code> 用于指定访问域名</p>\n<p>或者可以直接在浏览器中打开</p>\n<h2 id=\"deploy-testpage-app\"><a href=\"#deploy-testpage-app\" class=\"headerlink\" title=\"deploy testpage app\"></a>deploy testpage app</h2><p>testpage是一个python的测试页面，通过http访问该页面会返回一个配置好的版本号</p>\n<p>Deployment:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - name:</span> <span class=\"string\">http</span></span><br><span class=\"line\"><span class=\"attr\">    port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">        image:</span> <span class=\"string\">registry.local.com/testpage:0.5</span></span><br><span class=\"line\"><span class=\"attr\">        imagePullPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">10</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        env:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">VERSION</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"string\">\"0.5\"</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - containerPort:</span> <span class=\"number\">8080</span></span><br></pre></td></tr></table></figure>\n<p>Ingress:<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Ingress</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  rules:</span></span><br><span class=\"line\"><span class=\"attr\">  - host:</span> <span class=\"string\">testpage.local.io</span></span><br><span class=\"line\"><span class=\"attr\">    http:</span></span><br><span class=\"line\"><span class=\"attr\">      paths:</span></span><br><span class=\"line\"><span class=\"attr\">      - path:</span> <span class=\"string\">/</span></span><br><span class=\"line\"><span class=\"attr\">        backend:</span></span><br><span class=\"line\"><span class=\"attr\">          serviceName:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">          servicePort:</span> <span class=\"string\">http</span></span><br></pre></td></tr></table></figure></p>\n<p>验证</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl -H &quot;Host: testpage.local.io&quot; http://192.168.72.2</span><br><span class=\"line\">I am1: 172.1.14.4, the version is: 0.5</span><br></pre></td></tr></table></figure>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://docs.traefik.io/user-guide/kubernetes/\" target=\"_blank\" rel=\"noopener\">Kubernetes Ingress Controller</a><br><a href=\"https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html\" target=\"_blank\" rel=\"noopener\">Kubernetes : Ingress Controller with Træfɪk and Let’s Encrypt</a><br><a href=\"http://www.colabug.com/thread-1703745-1-1.html\" target=\"_blank\" rel=\"noopener\">Traefik-kubernetes 初试</a></p>\n"},{"title":"Docker Thin pool","date":"2017-01-17T16:00:00.000Z","_content":"\nDocker默认使用文件系统来存储image和container信息，container在运行的时候经常会访问文件系统，这会导致文件系统的性能影响到container的运行。Docker官方推荐一个更适合用于生产环境的thin pool方案。\n\n详细介绍可以查看docker.com中的[device-mapper-driver](https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/)\n\n下面是thin pool的配置\n\n## 停止docker\n\n```\nsystemctl stop docker\n```\n\n## 删除原有docker配置文件\n\n```\nrm -rf /var/lib/docker\n```\n\n## 创建用于docker的vg\n\n```\npvcreate /dev/vdb\nvgcreate docker /dev/vdb\n```\n\n## 修改docker-storage-setup\n\n```\ncat << EOF > /etc/sysconfig/docker-storage-setup\nSTORAGE_DRIVER=devicemapper\nVG=docker\nEXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"\nEOF\n```\n\n## 更新docker-storage配置\n\n```\ndocker-storage-setup\n```\n\n## 重启docker\n\n```\nsystemctl daemon-reload\nsystemctl start docker\n```\n\n配置完成后，使用lvs你可以看到有一个docker-pool的lv\n\nDone\n","source":"_posts/docker-thin-pool.md","raw":"title: Docker Thin pool\ndate: 2017-01-18\ntags:\n- docker\n---\n\nDocker默认使用文件系统来存储image和container信息，container在运行的时候经常会访问文件系统，这会导致文件系统的性能影响到container的运行。Docker官方推荐一个更适合用于生产环境的thin pool方案。\n\n详细介绍可以查看docker.com中的[device-mapper-driver](https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/)\n\n下面是thin pool的配置\n\n## 停止docker\n\n```\nsystemctl stop docker\n```\n\n## 删除原有docker配置文件\n\n```\nrm -rf /var/lib/docker\n```\n\n## 创建用于docker的vg\n\n```\npvcreate /dev/vdb\nvgcreate docker /dev/vdb\n```\n\n## 修改docker-storage-setup\n\n```\ncat << EOF > /etc/sysconfig/docker-storage-setup\nSTORAGE_DRIVER=devicemapper\nVG=docker\nEXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"\nEOF\n```\n\n## 更新docker-storage配置\n\n```\ndocker-storage-setup\n```\n\n## 重启docker\n\n```\nsystemctl daemon-reload\nsystemctl start docker\n```\n\n配置完成后，使用lvs你可以看到有一个docker-pool的lv\n\nDone\n","slug":"docker-thin-pool","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u300052tsb73xcaosx","content":"<p>Docker默认使用文件系统来存储image和container信息，container在运行的时候经常会访问文件系统，这会导致文件系统的性能影响到container的运行。Docker官方推荐一个更适合用于生产环境的thin pool方案。</p>\n<p>详细介绍可以查看docker.com中的<a href=\"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/\" target=\"_blank\" rel=\"noopener\">device-mapper-driver</a></p>\n<p>下面是thin pool的配置</p>\n<h2 id=\"停止docker\"><a href=\"#停止docker\" class=\"headerlink\" title=\"停止docker\"></a>停止docker</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"删除原有docker配置文件\"><a href=\"#删除原有docker配置文件\" class=\"headerlink\" title=\"删除原有docker配置文件\"></a>删除原有docker配置文件</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建用于docker的vg\"><a href=\"#创建用于docker的vg\" class=\"headerlink\" title=\"创建用于docker的vg\"></a>创建用于docker的vg</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pvcreate /dev/vdb</span><br><span class=\"line\">vgcreate docker /dev/vdb</span><br></pre></td></tr></table></figure>\n<h2 id=\"修改docker-storage-setup\"><a href=\"#修改docker-storage-setup\" class=\"headerlink\" title=\"修改docker-storage-setup\"></a>修改docker-storage-setup</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt; /etc/sysconfig/docker-storage-setup</span><br><span class=\"line\">STORAGE_DRIVER=devicemapper</span><br><span class=\"line\">VG=docker</span><br><span class=\"line\">EXTRA_DOCKER_STORAGE_OPTIONS=&quot; --storage-opt dm.use_deferred_removal=true&quot;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h2 id=\"更新docker-storage配置\"><a href=\"#更新docker-storage配置\" class=\"headerlink\" title=\"更新docker-storage配置\"></a>更新docker-storage配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker-storage-setup</span><br></pre></td></tr></table></figure>\n<h2 id=\"重启docker\"><a href=\"#重启docker\" class=\"headerlink\" title=\"重启docker\"></a>重启docker</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<p>配置完成后，使用lvs你可以看到有一个docker-pool的lv</p>\n<p>Done</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Docker默认使用文件系统来存储image和container信息，container在运行的时候经常会访问文件系统，这会导致文件系统的性能影响到container的运行。Docker官方推荐一个更适合用于生产环境的thin pool方案。</p>\n<p>详细介绍可以查看docker.com中的<a href=\"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/\" target=\"_blank\" rel=\"noopener\">device-mapper-driver</a></p>\n<p>下面是thin pool的配置</p>\n<h2 id=\"停止docker\"><a href=\"#停止docker\" class=\"headerlink\" title=\"停止docker\"></a>停止docker</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"删除原有docker配置文件\"><a href=\"#删除原有docker配置文件\" class=\"headerlink\" title=\"删除原有docker配置文件\"></a>删除原有docker配置文件</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure>\n<h2 id=\"创建用于docker的vg\"><a href=\"#创建用于docker的vg\" class=\"headerlink\" title=\"创建用于docker的vg\"></a>创建用于docker的vg</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pvcreate /dev/vdb</span><br><span class=\"line\">vgcreate docker /dev/vdb</span><br></pre></td></tr></table></figure>\n<h2 id=\"修改docker-storage-setup\"><a href=\"#修改docker-storage-setup\" class=\"headerlink\" title=\"修改docker-storage-setup\"></a>修改docker-storage-setup</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt; EOF &gt; /etc/sysconfig/docker-storage-setup</span><br><span class=\"line\">STORAGE_DRIVER=devicemapper</span><br><span class=\"line\">VG=docker</span><br><span class=\"line\">EXTRA_DOCKER_STORAGE_OPTIONS=&quot; --storage-opt dm.use_deferred_removal=true&quot;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<h2 id=\"更新docker-storage配置\"><a href=\"#更新docker-storage配置\" class=\"headerlink\" title=\"更新docker-storage配置\"></a>更新docker-storage配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker-storage-setup</span><br></pre></td></tr></table></figure>\n<h2 id=\"重启docker\"><a href=\"#重启docker\" class=\"headerlink\" title=\"重启docker\"></a>重启docker</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<p>配置完成后，使用lvs你可以看到有一个docker-pool的lv</p>\n<p>Done</p>\n"},{"title":"Docker, move the /var/lib/docker directory","date":"2017-02-21T16:00:00.000Z","_content":"\ndocker 默认的数据存储目录为：/var/lib/docker，主要的文件是/var/lib/docker/devicemapper/devicemapper/data,该文件存储docker的镜像文件，如果镜像过多经常会导致/var使用率100%，当然如果是生产环境你可能不会使用文件系统而是使用thinpool来存储镜像文件，而且一般生产上一台机器上的docker镜像和容器一般也会不过多。\n\n下面我们分别介绍下修改thinpool和移动/var/lib/docker的基本操作：\n\n# Docker config thin pool\n\nabout: [device-mapper-driver](https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/)\n\n```shell\nsystemctl stop docker\nrm -rf /var/lib/docker\n\npvcreate /dev/vdb\nvgcreate docker /dev/vdb\n\ncat << EOF > /etc/sysconfig/docker-storage-setup\nSTORAGE_DRIVER=devicemapper\nVG=docker\nEXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"\nEOF\n\ndocker-storage-setup\n\nsystemctl daemon-reload\nsystemctl start docker\n```\n\n# Move /var/lib/docker ( centos 7 )\n\n```shell\nsystemctl stop docker\n```\n\nset in the `/etc/sysconfig/docker` file, add `-g`\n\n```shell\nOPTIONS='--selinux-enabled --log-driver=journald'\n```\nto \n\n```shell\nOPTIONS='--selinux-enabled --log-driver=journald -g /data/docker/'\n```\n\nthe `/data/docker/` is new docker directory\n\nif `/var/lib/docker` have your images and want to save it. Move the data to `/data/docker`\n\n```\nmv /var/lib/docker /data/\n```\n\n```shell\nsystemctl start docker\n```\n\nlink: \n[#3127](https://github.com/docker/docker/issues/3127)\n[How do I change the Docker image installation directory](https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169/2)\n","source":"_posts/docker-var-lib-docker-directory-move.md","raw":"title: Docker, move the /var/lib/docker directory\ndate: 2017-02-22\ntags:\n- docker\n---\n\ndocker 默认的数据存储目录为：/var/lib/docker，主要的文件是/var/lib/docker/devicemapper/devicemapper/data,该文件存储docker的镜像文件，如果镜像过多经常会导致/var使用率100%，当然如果是生产环境你可能不会使用文件系统而是使用thinpool来存储镜像文件，而且一般生产上一台机器上的docker镜像和容器一般也会不过多。\n\n下面我们分别介绍下修改thinpool和移动/var/lib/docker的基本操作：\n\n# Docker config thin pool\n\nabout: [device-mapper-driver](https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/)\n\n```shell\nsystemctl stop docker\nrm -rf /var/lib/docker\n\npvcreate /dev/vdb\nvgcreate docker /dev/vdb\n\ncat << EOF > /etc/sysconfig/docker-storage-setup\nSTORAGE_DRIVER=devicemapper\nVG=docker\nEXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"\nEOF\n\ndocker-storage-setup\n\nsystemctl daemon-reload\nsystemctl start docker\n```\n\n# Move /var/lib/docker ( centos 7 )\n\n```shell\nsystemctl stop docker\n```\n\nset in the `/etc/sysconfig/docker` file, add `-g`\n\n```shell\nOPTIONS='--selinux-enabled --log-driver=journald'\n```\nto \n\n```shell\nOPTIONS='--selinux-enabled --log-driver=journald -g /data/docker/'\n```\n\nthe `/data/docker/` is new docker directory\n\nif `/var/lib/docker` have your images and want to save it. Move the data to `/data/docker`\n\n```\nmv /var/lib/docker /data/\n```\n\n```shell\nsystemctl start docker\n```\n\nlink: \n[#3127](https://github.com/docker/docker/issues/3127)\n[How do I change the Docker image installation directory](https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169/2)\n","slug":"docker-var-lib-docker-directory-move","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u500072tsbu7ag8s9a","content":"<p>docker 默认的数据存储目录为：/var/lib/docker，主要的文件是/var/lib/docker/devicemapper/devicemapper/data,该文件存储docker的镜像文件，如果镜像过多经常会导致/var使用率100%，当然如果是生产环境你可能不会使用文件系统而是使用thinpool来存储镜像文件，而且一般生产上一台机器上的docker镜像和容器一般也会不过多。</p>\n<p>下面我们分别介绍下修改thinpool和移动/var/lib/docker的基本操作：</p>\n<h1 id=\"Docker-config-thin-pool\"><a href=\"#Docker-config-thin-pool\" class=\"headerlink\" title=\"Docker config thin pool\"></a>Docker config thin pool</h1><p>about: <a href=\"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/\" target=\"_blank\" rel=\"noopener\">device-mapper-driver</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br><span class=\"line\">rm -rf /var/lib/docker</span><br><span class=\"line\"></span><br><span class=\"line\">pvcreate /dev/vdb</span><br><span class=\"line\">vgcreate docker /dev/vdb</span><br><span class=\"line\"></span><br><span class=\"line\">cat &lt;&lt; EOF &gt; /etc/sysconfig/docker-storage-setup</span><br><span class=\"line\">STORAGE_DRIVER=devicemapper</span><br><span class=\"line\">VG=docker</span><br><span class=\"line\">EXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">docker-storage-setup</span><br><span class=\"line\"></span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<h1 id=\"Move-var-lib-docker-centos-7\"><a href=\"#Move-var-lib-docker-centos-7\" class=\"headerlink\" title=\"Move /var/lib/docker ( centos 7 )\"></a>Move /var/lib/docker ( centos 7 )</h1><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br></pre></td></tr></table></figure>\n<p>set in the <code>/etc/sysconfig/docker</code> file, add <code>-g</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OPTIONS='--selinux-enabled --log-driver=journald'</span><br></pre></td></tr></table></figure>\n<p>to </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OPTIONS='--selinux-enabled --log-driver=journald -g /data/docker/'</span><br></pre></td></tr></table></figure>\n<p>the <code>/data/docker/</code> is new docker directory</p>\n<p>if <code>/var/lib/docker</code> have your images and want to save it. Move the data to <code>/data/docker</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv /var/lib/docker /data/</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<p>link:<br><a href=\"https://github.com/docker/docker/issues/3127\" target=\"_blank\" rel=\"noopener\">#3127</a><br><a href=\"https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169/2\" target=\"_blank\" rel=\"noopener\">How do I change the Docker image installation directory</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>docker 默认的数据存储目录为：/var/lib/docker，主要的文件是/var/lib/docker/devicemapper/devicemapper/data,该文件存储docker的镜像文件，如果镜像过多经常会导致/var使用率100%，当然如果是生产环境你可能不会使用文件系统而是使用thinpool来存储镜像文件，而且一般生产上一台机器上的docker镜像和容器一般也会不过多。</p>\n<p>下面我们分别介绍下修改thinpool和移动/var/lib/docker的基本操作：</p>\n<h1 id=\"Docker-config-thin-pool\"><a href=\"#Docker-config-thin-pool\" class=\"headerlink\" title=\"Docker config thin pool\"></a>Docker config thin pool</h1><p>about: <a href=\"https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/\" target=\"_blank\" rel=\"noopener\">device-mapper-driver</a></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br><span class=\"line\">rm -rf /var/lib/docker</span><br><span class=\"line\"></span><br><span class=\"line\">pvcreate /dev/vdb</span><br><span class=\"line\">vgcreate docker /dev/vdb</span><br><span class=\"line\"></span><br><span class=\"line\">cat &lt;&lt; EOF &gt; /etc/sysconfig/docker-storage-setup</span><br><span class=\"line\">STORAGE_DRIVER=devicemapper</span><br><span class=\"line\">VG=docker</span><br><span class=\"line\">EXTRA_DOCKER_STORAGE_OPTIONS=\" --storage-opt dm.use_deferred_removal=true\"</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">docker-storage-setup</span><br><span class=\"line\"></span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<h1 id=\"Move-var-lib-docker-centos-7\"><a href=\"#Move-var-lib-docker-centos-7\" class=\"headerlink\" title=\"Move /var/lib/docker ( centos 7 )\"></a>Move /var/lib/docker ( centos 7 )</h1><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl stop docker</span><br></pre></td></tr></table></figure>\n<p>set in the <code>/etc/sysconfig/docker</code> file, add <code>-g</code></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OPTIONS='--selinux-enabled --log-driver=journald'</span><br></pre></td></tr></table></figure>\n<p>to </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OPTIONS='--selinux-enabled --log-driver=journald -g /data/docker/'</span><br></pre></td></tr></table></figure>\n<p>the <code>/data/docker/</code> is new docker directory</p>\n<p>if <code>/var/lib/docker</code> have your images and want to save it. Move the data to <code>/data/docker</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv /var/lib/docker /data/</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure>\n<p>link:<br><a href=\"https://github.com/docker/docker/issues/3127\" target=\"_blank\" rel=\"noopener\">#3127</a><br><a href=\"https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169/2\" target=\"_blank\" rel=\"noopener\">How do I change the Docker image installation directory</a></p>\n"},{"title":"使用docker命令时需要sudo问题","date":"2017-01-17T16:00:00.000Z","_content":"\n默认安装完docker后，在使用如docker ps等命令的时候要么使用root帐号，要么需要sudo docker ps。这是因为docker在设计的时候并未考虑安全问题，所以docker理论上需要在root环境下才能运行。\n\n这里介绍一个使用非root用户执行docker命令不需要sudo的方法，该方法是利用docker这个用户组来进行配置：\n\n创建docker用户组\n```\nsudo groupadd docker\n```\n\n把当前用户添加到docker组中\n```\nsudo usermod -aG docker $(whoami)\n```\n\n重启docker\n```\nsudo systemctl restart docker\n```\n\n重新登录即可\n```\ndocker ps\n```\n\nDone\n","source":"_posts/execute-docker-without-sudo.md","raw":"title: 使用docker命令时需要sudo问题\ndate: 2017-01-18\ntags:\n- docker\n---\n\n默认安装完docker后，在使用如docker ps等命令的时候要么使用root帐号，要么需要sudo docker ps。这是因为docker在设计的时候并未考虑安全问题，所以docker理论上需要在root环境下才能运行。\n\n这里介绍一个使用非root用户执行docker命令不需要sudo的方法，该方法是利用docker这个用户组来进行配置：\n\n创建docker用户组\n```\nsudo groupadd docker\n```\n\n把当前用户添加到docker组中\n```\nsudo usermod -aG docker $(whoami)\n```\n\n重启docker\n```\nsudo systemctl restart docker\n```\n\n重新登录即可\n```\ndocker ps\n```\n\nDone\n","slug":"execute-docker-without-sudo","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u600082tsb6yofe24b","content":"<p>默认安装完docker后，在使用如docker ps等命令的时候要么使用root帐号，要么需要sudo docker ps。这是因为docker在设计的时候并未考虑安全问题，所以docker理论上需要在root环境下才能运行。</p>\n<p>这里介绍一个使用非root用户执行docker命令不需要sudo的方法，该方法是利用docker这个用户组来进行配置：</p>\n<p>创建docker用户组<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo groupadd docker</span><br></pre></td></tr></table></figure></p>\n<p>把当前用户添加到docker组中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo usermod -aG docker $(whoami)</span><br></pre></td></tr></table></figure></p>\n<p>重启docker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure></p>\n<p>重新登录即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker ps</span><br></pre></td></tr></table></figure></p>\n<p>Done</p>\n","site":{"data":{}},"excerpt":"","more":"<p>默认安装完docker后，在使用如docker ps等命令的时候要么使用root帐号，要么需要sudo docker ps。这是因为docker在设计的时候并未考虑安全问题，所以docker理论上需要在root环境下才能运行。</p>\n<p>这里介绍一个使用非root用户执行docker命令不需要sudo的方法，该方法是利用docker这个用户组来进行配置：</p>\n<p>创建docker用户组<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo groupadd docker</span><br></pre></td></tr></table></figure></p>\n<p>把当前用户添加到docker组中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo usermod -aG docker $(whoami)</span><br></pre></td></tr></table></figure></p>\n<p>重启docker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo systemctl restart docker</span><br></pre></td></tr></table></figure></p>\n<p>重新登录即可<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker ps</span><br></pre></td></tr></table></figure></p>\n<p>Done</p>\n"},{"title":"Harbor Deploy","date":"2017-04-18T16:00:00.000Z","_content":"\nHarbor 是企业级的docker registry版本，实际上它主要对docker registry封装了两个功能：\n1.权限管理\n2.同步功能\n\n这篇文章主要介绍下harbor的部署，另外也会结合Kubernetes介绍如何在访问需要帐号的docker registry\n\n# 环境\n\nCentos 7\nHarbor v1.1.0\nKubernetes 1.5\n\n# 安装docker和docker-compose\n\nharbor 官方文档使用docker-compose进行部署，当然现在也增加了部署在kubernetes上，这里选择使用docker-compose进行部署，以后有机会再研究下Kuberentes部署方式\n\n\n安装docker\n```\nyum install docker -y\n\nsystemctl enable docker\nsystemctl start docker\n```\n\n安装docker-compose\n```\nyum install python-pip -y\npip install docker-compose\n```\n\n# 配置harbor\n\n直接在Harbor的github上下载online安装文件：[https://github.com/vmware/harbor/releases](https://github.com/vmware/harbor/releases)\n\n```\nwget https://github.com/vmware/harbor/releases/download/v1.1.0/harbor-online-installer-v1.1.0.tgz\ntar -zxf harbor-online-installer-v1.1.0.tgz\n```\n\n## 配置ssl\n\n`/data`用于harbor的数据存储，最好挂载在存储上\n\n```\nmkdir -p /data/cert/\n```\n\n生成ssl keys\n这里需要注意的是生成key过程中的Common Name填入你要使用的域名，如:reg.local.com\n\n```\ncd /data/cert/\n$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt\nGenerating a 4096 bit RSA private key\n...........................................................................................................................................................................................................................................................................................................................................................................................++\n..++\nwriting new private key to 'ca.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:reg.local.com\nEmail Address []:\n\n$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout server.key -out server.csr\nGenerating a 4096 bit RSA private key\n..........................................................................................................................................................................++\n...................................................................................++\nwriting new private key to 'server.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:reg.local.com\nEmail Address []:\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:\nAn optional company name []:\n \n$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt\nSignature ok\nsubject=/C=XX/L=Default City/O=Default Company Ltd/CN=reg.local.com\nGetting CA Private Key\n```\n\n这里不详细介绍这几个key的原理了，大家可以自行baidu，ssl key还是建议大家去研究下，毕竟很多产品部署都会用到\n\n## 修改配置文件：harbor.cfg\n\n- hostname 如果是域名访问则设置成域名，否则使用IP，本文使用：reg.local.com\n- db_password MySQL的数据库密码\n- harbor_admin_password Harbor默认的管理员admin的密码\n\n其他参数可以不修改，但需要注意默认的ssl key配置的路径和文件名和我们生成的是一样的，如果不一样需要修改对应的配置\n\n## Install\n\n```\n./install.sh\n```\n\n这里会执行一堆检查，然后启动docker-compose来拉去harbor使用的几个docker，执行时间主要依赖你的网络拉去镜像的速度。\n\n如果这一步没有报错harbor就部署完成了。\n\n查看harbor启动情况\n```\n# docker ps\nCONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                                                              NAMES\n15452d1c314e        vmware/nginx:1.11.5-patched        \"nginx -g 'daemon off\"   20 hours ago        Up 20 hours         0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp, 0.0.0.0:4443->4443/tcp   nginx\n252065f73780        vmware/harbor-jobservice:v1.1.0    \"/harbor/harbor_jobse\"   20 hours ago        Up 20 hours                                                                            harbor-jobservice\n281a82e4373c        vmware/harbor-ui:v1.1.0            \"/harbor/harbor_ui\"      20 hours ago        Up 20 hours                                                                            harbor-ui\n644245ba9fbf        vmware/harbor-adminserver:v1.1.0   \"/harbor/harbor_admin\"   21 hours ago        Up 20 hours                                                                            harbor-adminserver\n02900a2759b6        vmware/harbor-db:v1.1.0            \"docker-entrypoint.sh\"   21 hours ago        Up 20 hours         3306/tcp                                                           harbor-db\n814b2c0df893        vmware/registry:photon-2.6.0       \"/entrypoint.sh serve\"   21 hours ago        Up 20 hours         5000/tcp                                                           registry\nee32489ccee9        vmware/harbor-log:v1.1.0           \"/bin/sh -c 'crond &&\"   21 hours ago        Up 20 hours         127.0.0.1:1514->514/tcp                                            harbor-log\n```\n\n接下来将域名reg.local.com配置指向Harbor部署的机器，就可以通过浏览器访问：https://reg.local.com\n帐号为admin, 密码在harbor.cfg的harbor_admin_password配置的：Harbor12345\n\n# harbor 基本维护操作\n\n```\ncd ~/harbor\ndocker-compose down\ndocker-compose start\n```\n\n# docker 使用\n\ndocker 要连接harbor需要两个条件\n1. ca证书\n2. 帐号密码\n\nca证书就是我们刚才生成的文件ca.crt证书，该证书需要放置在要访问harbor的机器的`/etc/docker/certs.d`目录\n\n创建证书目录，证书目录使用域名作为目录名称\n```\nmkdir -p /etc/docker/certs.d/reg.local.com\n```\n\n将ca.crt放置在上述目录\n```\nmv ca.crt /etc/docker/certs.d/reg.local.com\n```\n\n登录harbor\n```\n$ docker login reg.local.com\nUsername (test): test\nPassword: \nWARNING: login credentials saved in /home/docker/.docker/config.json\nLogin Succeeded\n```\n\n登录的帐号可以是admin，也可以是其他你通过web上创建的帐号\n\n注意上面的WARNING，它告诉我们auth key放置在文件`/home/docker/.docker/config.json`中（我发现在archlinux如果你用非root帐号执行，这个key也是会放到`/home/root/.docker/config.json`中）\n\n这个auth key在结合其他产品的时候会用到，如kubernetes、drone等\n\npush镜像\n```\n$ docker tag busybox reg.local.com/test/busybox\n$ docker push reg.local.com/test/busybox\nThe push refers to a repository [reg.local.com/test/busybox]\nc0de73ac9968: Layer already exists \nlatest: digest: sha256:92b7c19467bf868e52949d7e5404e309486ba9ee7eb4b88b882747ee1522d3cd size: 505\n```\n\n# kubernetes连接harbor\n\n需要几个东西\n1. docker 的 auth key\n2. ca.crt证书\n3. 帐号的secret\n\n获取k8s所有node的ip\n```\n$ nodes=$(kubectl get nodes -o jsonpath='{range.items[*].status.addresses[?(@.type==\"InternalIP\")]}{.address} {end}')\n$ echo $nodes\n192.168.1.1 192.168.1.2 192.168.1.3\n```\n\n使用`docker login reg.local.com`生成auth key\n\n这里默认为`~/.docker/config.json`\n\n```\n$ cat ~/.docker/config.json\n{\n        \"auths\": {\n                \"reg.local.com\": {\n                        \"auth\": \"dGVzdDpEWFEefMTIzNDU=\",\n                        \"email\": \"test@local.com\"\n                }\n        }\n}\n```\n\n复制auth key到所有节点\n```\n$ for n in $nodes; do ssh root@$n mkdir -p /root/.docker; scp ~/.docker/config.json root@$n:/root/.docker/config.json; done\n```\n\n复制ca.crt到所有节点\n```\n$ cert_path=\"/etc/docker/certs.d/reg.local.com\"\nfor n in $nodes; do ssh root@$n mkdir -p $cert_path; scp $cert_path/ca.crt root@$n:$cert_path/ca.crt; done\n```\n\n创建帐号secret\n```\nkubectl create secret docker-registry myregistrykey --docker-server=reg.local.com --docker-username=test --docker-password=Test12345 --docker-email=test@local.com\n```\n\npod配置中增加`imagePullSecrets`\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - image: reg.local.com/test/busybox\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    command:\n      - sleep\n      - \"3600\"\n  restartPolicy: Always\n  imagePullSecrets:\n    - name: myregistrykey\n```\n\n# 总结\n\n到这里docker和kubernetes都可以访问harbor\n\n下次有机会介绍下几个内容：\n- harbor的ha方案，基于它自己提供的复制功能\n- harbor部署在kubernetes上\n","source":"_posts/harbor-deploy.md","raw":"title: Harbor Deploy\ndate: 2017-04-19\ntags:\n- kubernetes\n- harbor\n- registry\n---\n\nHarbor 是企业级的docker registry版本，实际上它主要对docker registry封装了两个功能：\n1.权限管理\n2.同步功能\n\n这篇文章主要介绍下harbor的部署，另外也会结合Kubernetes介绍如何在访问需要帐号的docker registry\n\n# 环境\n\nCentos 7\nHarbor v1.1.0\nKubernetes 1.5\n\n# 安装docker和docker-compose\n\nharbor 官方文档使用docker-compose进行部署，当然现在也增加了部署在kubernetes上，这里选择使用docker-compose进行部署，以后有机会再研究下Kuberentes部署方式\n\n\n安装docker\n```\nyum install docker -y\n\nsystemctl enable docker\nsystemctl start docker\n```\n\n安装docker-compose\n```\nyum install python-pip -y\npip install docker-compose\n```\n\n# 配置harbor\n\n直接在Harbor的github上下载online安装文件：[https://github.com/vmware/harbor/releases](https://github.com/vmware/harbor/releases)\n\n```\nwget https://github.com/vmware/harbor/releases/download/v1.1.0/harbor-online-installer-v1.1.0.tgz\ntar -zxf harbor-online-installer-v1.1.0.tgz\n```\n\n## 配置ssl\n\n`/data`用于harbor的数据存储，最好挂载在存储上\n\n```\nmkdir -p /data/cert/\n```\n\n生成ssl keys\n这里需要注意的是生成key过程中的Common Name填入你要使用的域名，如:reg.local.com\n\n```\ncd /data/cert/\n$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt\nGenerating a 4096 bit RSA private key\n...........................................................................................................................................................................................................................................................................................................................................................................................++\n..++\nwriting new private key to 'ca.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:reg.local.com\nEmail Address []:\n\n$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout server.key -out server.csr\nGenerating a 4096 bit RSA private key\n..........................................................................................................................................................................++\n...................................................................................++\nwriting new private key to 'server.key'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [XX]:\nState or Province Name (full name) []:\nLocality Name (eg, city) [Default City]:\nOrganization Name (eg, company) [Default Company Ltd]:\nOrganizational Unit Name (eg, section) []:\nCommon Name (eg, your name or your server's hostname) []:reg.local.com\nEmail Address []:\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:\nAn optional company name []:\n \n$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt\nSignature ok\nsubject=/C=XX/L=Default City/O=Default Company Ltd/CN=reg.local.com\nGetting CA Private Key\n```\n\n这里不详细介绍这几个key的原理了，大家可以自行baidu，ssl key还是建议大家去研究下，毕竟很多产品部署都会用到\n\n## 修改配置文件：harbor.cfg\n\n- hostname 如果是域名访问则设置成域名，否则使用IP，本文使用：reg.local.com\n- db_password MySQL的数据库密码\n- harbor_admin_password Harbor默认的管理员admin的密码\n\n其他参数可以不修改，但需要注意默认的ssl key配置的路径和文件名和我们生成的是一样的，如果不一样需要修改对应的配置\n\n## Install\n\n```\n./install.sh\n```\n\n这里会执行一堆检查，然后启动docker-compose来拉去harbor使用的几个docker，执行时间主要依赖你的网络拉去镜像的速度。\n\n如果这一步没有报错harbor就部署完成了。\n\n查看harbor启动情况\n```\n# docker ps\nCONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                                                              NAMES\n15452d1c314e        vmware/nginx:1.11.5-patched        \"nginx -g 'daemon off\"   20 hours ago        Up 20 hours         0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp, 0.0.0.0:4443->4443/tcp   nginx\n252065f73780        vmware/harbor-jobservice:v1.1.0    \"/harbor/harbor_jobse\"   20 hours ago        Up 20 hours                                                                            harbor-jobservice\n281a82e4373c        vmware/harbor-ui:v1.1.0            \"/harbor/harbor_ui\"      20 hours ago        Up 20 hours                                                                            harbor-ui\n644245ba9fbf        vmware/harbor-adminserver:v1.1.0   \"/harbor/harbor_admin\"   21 hours ago        Up 20 hours                                                                            harbor-adminserver\n02900a2759b6        vmware/harbor-db:v1.1.0            \"docker-entrypoint.sh\"   21 hours ago        Up 20 hours         3306/tcp                                                           harbor-db\n814b2c0df893        vmware/registry:photon-2.6.0       \"/entrypoint.sh serve\"   21 hours ago        Up 20 hours         5000/tcp                                                           registry\nee32489ccee9        vmware/harbor-log:v1.1.0           \"/bin/sh -c 'crond &&\"   21 hours ago        Up 20 hours         127.0.0.1:1514->514/tcp                                            harbor-log\n```\n\n接下来将域名reg.local.com配置指向Harbor部署的机器，就可以通过浏览器访问：https://reg.local.com\n帐号为admin, 密码在harbor.cfg的harbor_admin_password配置的：Harbor12345\n\n# harbor 基本维护操作\n\n```\ncd ~/harbor\ndocker-compose down\ndocker-compose start\n```\n\n# docker 使用\n\ndocker 要连接harbor需要两个条件\n1. ca证书\n2. 帐号密码\n\nca证书就是我们刚才生成的文件ca.crt证书，该证书需要放置在要访问harbor的机器的`/etc/docker/certs.d`目录\n\n创建证书目录，证书目录使用域名作为目录名称\n```\nmkdir -p /etc/docker/certs.d/reg.local.com\n```\n\n将ca.crt放置在上述目录\n```\nmv ca.crt /etc/docker/certs.d/reg.local.com\n```\n\n登录harbor\n```\n$ docker login reg.local.com\nUsername (test): test\nPassword: \nWARNING: login credentials saved in /home/docker/.docker/config.json\nLogin Succeeded\n```\n\n登录的帐号可以是admin，也可以是其他你通过web上创建的帐号\n\n注意上面的WARNING，它告诉我们auth key放置在文件`/home/docker/.docker/config.json`中（我发现在archlinux如果你用非root帐号执行，这个key也是会放到`/home/root/.docker/config.json`中）\n\n这个auth key在结合其他产品的时候会用到，如kubernetes、drone等\n\npush镜像\n```\n$ docker tag busybox reg.local.com/test/busybox\n$ docker push reg.local.com/test/busybox\nThe push refers to a repository [reg.local.com/test/busybox]\nc0de73ac9968: Layer already exists \nlatest: digest: sha256:92b7c19467bf868e52949d7e5404e309486ba9ee7eb4b88b882747ee1522d3cd size: 505\n```\n\n# kubernetes连接harbor\n\n需要几个东西\n1. docker 的 auth key\n2. ca.crt证书\n3. 帐号的secret\n\n获取k8s所有node的ip\n```\n$ nodes=$(kubectl get nodes -o jsonpath='{range.items[*].status.addresses[?(@.type==\"InternalIP\")]}{.address} {end}')\n$ echo $nodes\n192.168.1.1 192.168.1.2 192.168.1.3\n```\n\n使用`docker login reg.local.com`生成auth key\n\n这里默认为`~/.docker/config.json`\n\n```\n$ cat ~/.docker/config.json\n{\n        \"auths\": {\n                \"reg.local.com\": {\n                        \"auth\": \"dGVzdDpEWFEefMTIzNDU=\",\n                        \"email\": \"test@local.com\"\n                }\n        }\n}\n```\n\n复制auth key到所有节点\n```\n$ for n in $nodes; do ssh root@$n mkdir -p /root/.docker; scp ~/.docker/config.json root@$n:/root/.docker/config.json; done\n```\n\n复制ca.crt到所有节点\n```\n$ cert_path=\"/etc/docker/certs.d/reg.local.com\"\nfor n in $nodes; do ssh root@$n mkdir -p $cert_path; scp $cert_path/ca.crt root@$n:$cert_path/ca.crt; done\n```\n\n创建帐号secret\n```\nkubectl create secret docker-registry myregistrykey --docker-server=reg.local.com --docker-username=test --docker-password=Test12345 --docker-email=test@local.com\n```\n\npod配置中增加`imagePullSecrets`\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - image: reg.local.com/test/busybox\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    command:\n      - sleep\n      - \"3600\"\n  restartPolicy: Always\n  imagePullSecrets:\n    - name: myregistrykey\n```\n\n# 总结\n\n到这里docker和kubernetes都可以访问harbor\n\n下次有机会介绍下几个内容：\n- harbor的ha方案，基于它自己提供的复制功能\n- harbor部署在kubernetes上\n","slug":"harbor-deploy","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u8000a2tsby4nxiect","content":"<p>Harbor 是企业级的docker registry版本，实际上它主要对docker registry封装了两个功能：<br>1.权限管理<br>2.同步功能</p>\n<p>这篇文章主要介绍下harbor的部署，另外也会结合Kubernetes介绍如何在访问需要帐号的docker registry</p>\n<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>Centos 7<br>Harbor v1.1.0<br>Kubernetes 1.5</p>\n<h1 id=\"安装docker和docker-compose\"><a href=\"#安装docker和docker-compose\" class=\"headerlink\" title=\"安装docker和docker-compose\"></a>安装docker和docker-compose</h1><p>harbor 官方文档使用docker-compose进行部署，当然现在也增加了部署在kubernetes上，这里选择使用docker-compose进行部署，以后有机会再研究下Kuberentes部署方式</p>\n<p>安装docker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install docker -y</span><br><span class=\"line\"></span><br><span class=\"line\">systemctl enable docker</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure></p>\n<p>安装docker-compose<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install python-pip -y</span><br><span class=\"line\">pip install docker-compose</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"配置harbor\"><a href=\"#配置harbor\" class=\"headerlink\" title=\"配置harbor\"></a>配置harbor</h1><p>直接在Harbor的github上下载online安装文件：<a href=\"https://github.com/vmware/harbor/releases\" target=\"_blank\" rel=\"noopener\">https://github.com/vmware/harbor/releases</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/vmware/harbor/releases/download/v1.1.0/harbor-online-installer-v1.1.0.tgz</span><br><span class=\"line\">tar -zxf harbor-online-installer-v1.1.0.tgz</span><br></pre></td></tr></table></figure>\n<h2 id=\"配置ssl\"><a href=\"#配置ssl\" class=\"headerlink\" title=\"配置ssl\"></a>配置ssl</h2><p><code>/data</code>用于harbor的数据存储，最好挂载在存储上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /data/cert/</span><br></pre></td></tr></table></figure>\n<p>生成ssl keys<br>这里需要注意的是生成key过程中的Common Name填入你要使用的域名，如:reg.local.com</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd /data/cert/</span><br><span class=\"line\">$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt</span><br><span class=\"line\">Generating a 4096 bit RSA private key</span><br><span class=\"line\">...........................................................................................................................................................................................................................................................................................................................................................................................++</span><br><span class=\"line\">..++</span><br><span class=\"line\">writing new private key to &apos;ca.key&apos;</span><br><span class=\"line\">-----</span><br><span class=\"line\">You are about to be asked to enter information that will be incorporated</span><br><span class=\"line\">into your certificate request.</span><br><span class=\"line\">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class=\"line\">There are quite a few fields but you can leave some blank</span><br><span class=\"line\">For some fields there will be a default value,</span><br><span class=\"line\">If you enter &apos;.&apos;, the field will be left blank.</span><br><span class=\"line\">-----</span><br><span class=\"line\">Country Name (2 letter code) [XX]:</span><br><span class=\"line\">State or Province Name (full name) []:</span><br><span class=\"line\">Locality Name (eg, city) [Default City]:</span><br><span class=\"line\">Organization Name (eg, company) [Default Company Ltd]:</span><br><span class=\"line\">Organizational Unit Name (eg, section) []:</span><br><span class=\"line\">Common Name (eg, your name or your server&apos;s hostname) []:reg.local.com</span><br><span class=\"line\">Email Address []:</span><br><span class=\"line\"></span><br><span class=\"line\">$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout server.key -out server.csr</span><br><span class=\"line\">Generating a 4096 bit RSA private key</span><br><span class=\"line\">..........................................................................................................................................................................++</span><br><span class=\"line\">...................................................................................++</span><br><span class=\"line\">writing new private key to &apos;server.key&apos;</span><br><span class=\"line\">-----</span><br><span class=\"line\">You are about to be asked to enter information that will be incorporated</span><br><span class=\"line\">into your certificate request.</span><br><span class=\"line\">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class=\"line\">There are quite a few fields but you can leave some blank</span><br><span class=\"line\">For some fields there will be a default value,</span><br><span class=\"line\">If you enter &apos;.&apos;, the field will be left blank.</span><br><span class=\"line\">-----</span><br><span class=\"line\">Country Name (2 letter code) [XX]:</span><br><span class=\"line\">State or Province Name (full name) []:</span><br><span class=\"line\">Locality Name (eg, city) [Default City]:</span><br><span class=\"line\">Organization Name (eg, company) [Default Company Ltd]:</span><br><span class=\"line\">Organizational Unit Name (eg, section) []:</span><br><span class=\"line\">Common Name (eg, your name or your server&apos;s hostname) []:reg.local.com</span><br><span class=\"line\">Email Address []:</span><br><span class=\"line\"></span><br><span class=\"line\">Please enter the following &apos;extra&apos; attributes</span><br><span class=\"line\">to be sent with your certificate request</span><br><span class=\"line\">A challenge password []:</span><br><span class=\"line\">An optional company name []:</span><br><span class=\"line\"> </span><br><span class=\"line\">$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt</span><br><span class=\"line\">Signature ok</span><br><span class=\"line\">subject=/C=XX/L=Default City/O=Default Company Ltd/CN=reg.local.com</span><br><span class=\"line\">Getting CA Private Key</span><br></pre></td></tr></table></figure>\n<p>这里不详细介绍这几个key的原理了，大家可以自行baidu，ssl key还是建议大家去研究下，毕竟很多产品部署都会用到</p>\n<h2 id=\"修改配置文件：harbor-cfg\"><a href=\"#修改配置文件：harbor-cfg\" class=\"headerlink\" title=\"修改配置文件：harbor.cfg\"></a>修改配置文件：harbor.cfg</h2><ul>\n<li>hostname 如果是域名访问则设置成域名，否则使用IP，本文使用：reg.local.com</li>\n<li>db_password MySQL的数据库密码</li>\n<li>harbor_admin_password Harbor默认的管理员admin的密码</li>\n</ul>\n<p>其他参数可以不修改，但需要注意默认的ssl key配置的路径和文件名和我们生成的是一样的，如果不一样需要修改对应的配置</p>\n<h2 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./install.sh</span><br></pre></td></tr></table></figure>\n<p>这里会执行一堆检查，然后启动docker-compose来拉去harbor使用的几个docker，执行时间主要依赖你的网络拉去镜像的速度。</p>\n<p>如果这一步没有报错harbor就部署完成了。</p>\n<p>查看harbor启动情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker ps</span><br><span class=\"line\">CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                                                              NAMES</span><br><span class=\"line\">15452d1c314e        vmware/nginx:1.11.5-patched        &quot;nginx -g &apos;daemon off&quot;   20 hours ago        Up 20 hours         0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp   nginx</span><br><span class=\"line\">252065f73780        vmware/harbor-jobservice:v1.1.0    &quot;/harbor/harbor_jobse&quot;   20 hours ago        Up 20 hours                                                                            harbor-jobservice</span><br><span class=\"line\">281a82e4373c        vmware/harbor-ui:v1.1.0            &quot;/harbor/harbor_ui&quot;      20 hours ago        Up 20 hours                                                                            harbor-ui</span><br><span class=\"line\">644245ba9fbf        vmware/harbor-adminserver:v1.1.0   &quot;/harbor/harbor_admin&quot;   21 hours ago        Up 20 hours                                                                            harbor-adminserver</span><br><span class=\"line\">02900a2759b6        vmware/harbor-db:v1.1.0            &quot;docker-entrypoint.sh&quot;   21 hours ago        Up 20 hours         3306/tcp                                                           harbor-db</span><br><span class=\"line\">814b2c0df893        vmware/registry:photon-2.6.0       &quot;/entrypoint.sh serve&quot;   21 hours ago        Up 20 hours         5000/tcp                                                           registry</span><br><span class=\"line\">ee32489ccee9        vmware/harbor-log:v1.1.0           &quot;/bin/sh -c &apos;crond &amp;&amp;&quot;   21 hours ago        Up 20 hours         127.0.0.1:1514-&gt;514/tcp                                            harbor-log</span><br></pre></td></tr></table></figure></p>\n<p>接下来将域名reg.local.com配置指向Harbor部署的机器，就可以通过浏览器访问：<a href=\"https://reg.local.com\" target=\"_blank\" rel=\"noopener\">https://reg.local.com</a><br>帐号为admin, 密码在harbor.cfg的harbor_admin_password配置的：Harbor12345</p>\n<h1 id=\"harbor-基本维护操作\"><a href=\"#harbor-基本维护操作\" class=\"headerlink\" title=\"harbor 基本维护操作\"></a>harbor 基本维护操作</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/harbor</span><br><span class=\"line\">docker-compose down</span><br><span class=\"line\">docker-compose start</span><br></pre></td></tr></table></figure>\n<h1 id=\"docker-使用\"><a href=\"#docker-使用\" class=\"headerlink\" title=\"docker 使用\"></a>docker 使用</h1><p>docker 要连接harbor需要两个条件</p>\n<ol>\n<li>ca证书</li>\n<li>帐号密码</li>\n</ol>\n<p>ca证书就是我们刚才生成的文件ca.crt证书，该证书需要放置在要访问harbor的机器的<code>/etc/docker/certs.d</code>目录</p>\n<p>创建证书目录，证书目录使用域名作为目录名称<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /etc/docker/certs.d/reg.local.com</span><br></pre></td></tr></table></figure></p>\n<p>将ca.crt放置在上述目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv ca.crt /etc/docker/certs.d/reg.local.com</span><br></pre></td></tr></table></figure></p>\n<p>登录harbor<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker login reg.local.com</span><br><span class=\"line\">Username (test): test</span><br><span class=\"line\">Password: </span><br><span class=\"line\">WARNING: login credentials saved in /home/docker/.docker/config.json</span><br><span class=\"line\">Login Succeeded</span><br></pre></td></tr></table></figure></p>\n<p>登录的帐号可以是admin，也可以是其他你通过web上创建的帐号</p>\n<p>注意上面的WARNING，它告诉我们auth key放置在文件<code>/home/docker/.docker/config.json</code>中（我发现在archlinux如果你用非root帐号执行，这个key也是会放到<code>/home/root/.docker/config.json</code>中）</p>\n<p>这个auth key在结合其他产品的时候会用到，如kubernetes、drone等</p>\n<p>push镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag busybox reg.local.com/test/busybox</span><br><span class=\"line\">$ docker push reg.local.com/test/busybox</span><br><span class=\"line\">The push refers to a repository [reg.local.com/test/busybox]</span><br><span class=\"line\">c0de73ac9968: Layer already exists </span><br><span class=\"line\">latest: digest: sha256:92b7c19467bf868e52949d7e5404e309486ba9ee7eb4b88b882747ee1522d3cd size: 505</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"kubernetes连接harbor\"><a href=\"#kubernetes连接harbor\" class=\"headerlink\" title=\"kubernetes连接harbor\"></a>kubernetes连接harbor</h1><p>需要几个东西</p>\n<ol>\n<li>docker 的 auth key</li>\n<li>ca.crt证书</li>\n<li>帐号的secret</li>\n</ol>\n<p>获取k8s所有node的ip<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ nodes=$(kubectl get nodes -o jsonpath=&apos;&#123;range.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)]&#125;&#123;.address&#125; &#123;end&#125;&apos;)</span><br><span class=\"line\">$ echo $nodes</span><br><span class=\"line\">192.168.1.1 192.168.1.2 192.168.1.3</span><br></pre></td></tr></table></figure></p>\n<p>使用<code>docker login reg.local.com</code>生成auth key</p>\n<p>这里默认为<code>~/.docker/config.json</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat ~/.docker/config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        &quot;auths&quot;: &#123;</span><br><span class=\"line\">                &quot;reg.local.com&quot;: &#123;</span><br><span class=\"line\">                        &quot;auth&quot;: &quot;dGVzdDpEWFEefMTIzNDU=&quot;,</span><br><span class=\"line\">                        &quot;email&quot;: &quot;test@local.com&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>复制auth key到所有节点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ for n in $nodes; do ssh root@$n mkdir -p /root/.docker; scp ~/.docker/config.json root@$n:/root/.docker/config.json; done</span><br></pre></td></tr></table></figure></p>\n<p>复制ca.crt到所有节点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cert_path=&quot;/etc/docker/certs.d/reg.local.com&quot;</span><br><span class=\"line\">for n in $nodes; do ssh root@$n mkdir -p $cert_path; scp $cert_path/ca.crt root@$n:$cert_path/ca.crt; done</span><br></pre></td></tr></table></figure></p>\n<p>创建帐号secret<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl create secret docker-registry myregistrykey --docker-server=reg.local.com --docker-username=test --docker-password=Test12345 --docker-email=test@local.com</span><br></pre></td></tr></table></figure></p>\n<p>pod配置中增加<code>imagePullSecrets</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: busybox</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - image: reg.local.com/test/busybox</span><br><span class=\"line\">    imagePullPolicy: IfNotPresent</span><br><span class=\"line\">    name: busybox</span><br><span class=\"line\">    command:</span><br><span class=\"line\">      - sleep</span><br><span class=\"line\">      - &quot;3600&quot;</span><br><span class=\"line\">  restartPolicy: Always</span><br><span class=\"line\">  imagePullSecrets:</span><br><span class=\"line\">    - name: myregistrykey</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>到这里docker和kubernetes都可以访问harbor</p>\n<p>下次有机会介绍下几个内容：</p>\n<ul>\n<li>harbor的ha方案，基于它自己提供的复制功能</li>\n<li>harbor部署在kubernetes上</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Harbor 是企业级的docker registry版本，实际上它主要对docker registry封装了两个功能：<br>1.权限管理<br>2.同步功能</p>\n<p>这篇文章主要介绍下harbor的部署，另外也会结合Kubernetes介绍如何在访问需要帐号的docker registry</p>\n<h1 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h1><p>Centos 7<br>Harbor v1.1.0<br>Kubernetes 1.5</p>\n<h1 id=\"安装docker和docker-compose\"><a href=\"#安装docker和docker-compose\" class=\"headerlink\" title=\"安装docker和docker-compose\"></a>安装docker和docker-compose</h1><p>harbor 官方文档使用docker-compose进行部署，当然现在也增加了部署在kubernetes上，这里选择使用docker-compose进行部署，以后有机会再研究下Kuberentes部署方式</p>\n<p>安装docker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install docker -y</span><br><span class=\"line\"></span><br><span class=\"line\">systemctl enable docker</span><br><span class=\"line\">systemctl start docker</span><br></pre></td></tr></table></figure></p>\n<p>安装docker-compose<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install python-pip -y</span><br><span class=\"line\">pip install docker-compose</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"配置harbor\"><a href=\"#配置harbor\" class=\"headerlink\" title=\"配置harbor\"></a>配置harbor</h1><p>直接在Harbor的github上下载online安装文件：<a href=\"https://github.com/vmware/harbor/releases\" target=\"_blank\" rel=\"noopener\">https://github.com/vmware/harbor/releases</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/vmware/harbor/releases/download/v1.1.0/harbor-online-installer-v1.1.0.tgz</span><br><span class=\"line\">tar -zxf harbor-online-installer-v1.1.0.tgz</span><br></pre></td></tr></table></figure>\n<h2 id=\"配置ssl\"><a href=\"#配置ssl\" class=\"headerlink\" title=\"配置ssl\"></a>配置ssl</h2><p><code>/data</code>用于harbor的数据存储，最好挂载在存储上</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /data/cert/</span><br></pre></td></tr></table></figure>\n<p>生成ssl keys<br>这里需要注意的是生成key过程中的Common Name填入你要使用的域名，如:reg.local.com</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd /data/cert/</span><br><span class=\"line\">$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt</span><br><span class=\"line\">Generating a 4096 bit RSA private key</span><br><span class=\"line\">...........................................................................................................................................................................................................................................................................................................................................................................................++</span><br><span class=\"line\">..++</span><br><span class=\"line\">writing new private key to &apos;ca.key&apos;</span><br><span class=\"line\">-----</span><br><span class=\"line\">You are about to be asked to enter information that will be incorporated</span><br><span class=\"line\">into your certificate request.</span><br><span class=\"line\">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class=\"line\">There are quite a few fields but you can leave some blank</span><br><span class=\"line\">For some fields there will be a default value,</span><br><span class=\"line\">If you enter &apos;.&apos;, the field will be left blank.</span><br><span class=\"line\">-----</span><br><span class=\"line\">Country Name (2 letter code) [XX]:</span><br><span class=\"line\">State or Province Name (full name) []:</span><br><span class=\"line\">Locality Name (eg, city) [Default City]:</span><br><span class=\"line\">Organization Name (eg, company) [Default Company Ltd]:</span><br><span class=\"line\">Organizational Unit Name (eg, section) []:</span><br><span class=\"line\">Common Name (eg, your name or your server&apos;s hostname) []:reg.local.com</span><br><span class=\"line\">Email Address []:</span><br><span class=\"line\"></span><br><span class=\"line\">$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout server.key -out server.csr</span><br><span class=\"line\">Generating a 4096 bit RSA private key</span><br><span class=\"line\">..........................................................................................................................................................................++</span><br><span class=\"line\">...................................................................................++</span><br><span class=\"line\">writing new private key to &apos;server.key&apos;</span><br><span class=\"line\">-----</span><br><span class=\"line\">You are about to be asked to enter information that will be incorporated</span><br><span class=\"line\">into your certificate request.</span><br><span class=\"line\">What you are about to enter is what is called a Distinguished Name or a DN.</span><br><span class=\"line\">There are quite a few fields but you can leave some blank</span><br><span class=\"line\">For some fields there will be a default value,</span><br><span class=\"line\">If you enter &apos;.&apos;, the field will be left blank.</span><br><span class=\"line\">-----</span><br><span class=\"line\">Country Name (2 letter code) [XX]:</span><br><span class=\"line\">State or Province Name (full name) []:</span><br><span class=\"line\">Locality Name (eg, city) [Default City]:</span><br><span class=\"line\">Organization Name (eg, company) [Default Company Ltd]:</span><br><span class=\"line\">Organizational Unit Name (eg, section) []:</span><br><span class=\"line\">Common Name (eg, your name or your server&apos;s hostname) []:reg.local.com</span><br><span class=\"line\">Email Address []:</span><br><span class=\"line\"></span><br><span class=\"line\">Please enter the following &apos;extra&apos; attributes</span><br><span class=\"line\">to be sent with your certificate request</span><br><span class=\"line\">A challenge password []:</span><br><span class=\"line\">An optional company name []:</span><br><span class=\"line\"> </span><br><span class=\"line\">$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt</span><br><span class=\"line\">Signature ok</span><br><span class=\"line\">subject=/C=XX/L=Default City/O=Default Company Ltd/CN=reg.local.com</span><br><span class=\"line\">Getting CA Private Key</span><br></pre></td></tr></table></figure>\n<p>这里不详细介绍这几个key的原理了，大家可以自行baidu，ssl key还是建议大家去研究下，毕竟很多产品部署都会用到</p>\n<h2 id=\"修改配置文件：harbor-cfg\"><a href=\"#修改配置文件：harbor-cfg\" class=\"headerlink\" title=\"修改配置文件：harbor.cfg\"></a>修改配置文件：harbor.cfg</h2><ul>\n<li>hostname 如果是域名访问则设置成域名，否则使用IP，本文使用：reg.local.com</li>\n<li>db_password MySQL的数据库密码</li>\n<li>harbor_admin_password Harbor默认的管理员admin的密码</li>\n</ul>\n<p>其他参数可以不修改，但需要注意默认的ssl key配置的路径和文件名和我们生成的是一样的，如果不一样需要修改对应的配置</p>\n<h2 id=\"Install\"><a href=\"#Install\" class=\"headerlink\" title=\"Install\"></a>Install</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./install.sh</span><br></pre></td></tr></table></figure>\n<p>这里会执行一堆检查，然后启动docker-compose来拉去harbor使用的几个docker，执行时间主要依赖你的网络拉去镜像的速度。</p>\n<p>如果这一步没有报错harbor就部署完成了。</p>\n<p>查看harbor启动情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># docker ps</span><br><span class=\"line\">CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                                                              NAMES</span><br><span class=\"line\">15452d1c314e        vmware/nginx:1.11.5-patched        &quot;nginx -g &apos;daemon off&quot;   20 hours ago        Up 20 hours         0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp   nginx</span><br><span class=\"line\">252065f73780        vmware/harbor-jobservice:v1.1.0    &quot;/harbor/harbor_jobse&quot;   20 hours ago        Up 20 hours                                                                            harbor-jobservice</span><br><span class=\"line\">281a82e4373c        vmware/harbor-ui:v1.1.0            &quot;/harbor/harbor_ui&quot;      20 hours ago        Up 20 hours                                                                            harbor-ui</span><br><span class=\"line\">644245ba9fbf        vmware/harbor-adminserver:v1.1.0   &quot;/harbor/harbor_admin&quot;   21 hours ago        Up 20 hours                                                                            harbor-adminserver</span><br><span class=\"line\">02900a2759b6        vmware/harbor-db:v1.1.0            &quot;docker-entrypoint.sh&quot;   21 hours ago        Up 20 hours         3306/tcp                                                           harbor-db</span><br><span class=\"line\">814b2c0df893        vmware/registry:photon-2.6.0       &quot;/entrypoint.sh serve&quot;   21 hours ago        Up 20 hours         5000/tcp                                                           registry</span><br><span class=\"line\">ee32489ccee9        vmware/harbor-log:v1.1.0           &quot;/bin/sh -c &apos;crond &amp;&amp;&quot;   21 hours ago        Up 20 hours         127.0.0.1:1514-&gt;514/tcp                                            harbor-log</span><br></pre></td></tr></table></figure></p>\n<p>接下来将域名reg.local.com配置指向Harbor部署的机器，就可以通过浏览器访问：<a href=\"https://reg.local.com\" target=\"_blank\" rel=\"noopener\">https://reg.local.com</a><br>帐号为admin, 密码在harbor.cfg的harbor_admin_password配置的：Harbor12345</p>\n<h1 id=\"harbor-基本维护操作\"><a href=\"#harbor-基本维护操作\" class=\"headerlink\" title=\"harbor 基本维护操作\"></a>harbor 基本维护操作</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/harbor</span><br><span class=\"line\">docker-compose down</span><br><span class=\"line\">docker-compose start</span><br></pre></td></tr></table></figure>\n<h1 id=\"docker-使用\"><a href=\"#docker-使用\" class=\"headerlink\" title=\"docker 使用\"></a>docker 使用</h1><p>docker 要连接harbor需要两个条件</p>\n<ol>\n<li>ca证书</li>\n<li>帐号密码</li>\n</ol>\n<p>ca证书就是我们刚才生成的文件ca.crt证书，该证书需要放置在要访问harbor的机器的<code>/etc/docker/certs.d</code>目录</p>\n<p>创建证书目录，证书目录使用域名作为目录名称<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p /etc/docker/certs.d/reg.local.com</span><br></pre></td></tr></table></figure></p>\n<p>将ca.crt放置在上述目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv ca.crt /etc/docker/certs.d/reg.local.com</span><br></pre></td></tr></table></figure></p>\n<p>登录harbor<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker login reg.local.com</span><br><span class=\"line\">Username (test): test</span><br><span class=\"line\">Password: </span><br><span class=\"line\">WARNING: login credentials saved in /home/docker/.docker/config.json</span><br><span class=\"line\">Login Succeeded</span><br></pre></td></tr></table></figure></p>\n<p>登录的帐号可以是admin，也可以是其他你通过web上创建的帐号</p>\n<p>注意上面的WARNING，它告诉我们auth key放置在文件<code>/home/docker/.docker/config.json</code>中（我发现在archlinux如果你用非root帐号执行，这个key也是会放到<code>/home/root/.docker/config.json</code>中）</p>\n<p>这个auth key在结合其他产品的时候会用到，如kubernetes、drone等</p>\n<p>push镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag busybox reg.local.com/test/busybox</span><br><span class=\"line\">$ docker push reg.local.com/test/busybox</span><br><span class=\"line\">The push refers to a repository [reg.local.com/test/busybox]</span><br><span class=\"line\">c0de73ac9968: Layer already exists </span><br><span class=\"line\">latest: digest: sha256:92b7c19467bf868e52949d7e5404e309486ba9ee7eb4b88b882747ee1522d3cd size: 505</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"kubernetes连接harbor\"><a href=\"#kubernetes连接harbor\" class=\"headerlink\" title=\"kubernetes连接harbor\"></a>kubernetes连接harbor</h1><p>需要几个东西</p>\n<ol>\n<li>docker 的 auth key</li>\n<li>ca.crt证书</li>\n<li>帐号的secret</li>\n</ol>\n<p>获取k8s所有node的ip<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ nodes=$(kubectl get nodes -o jsonpath=&apos;&#123;range.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)]&#125;&#123;.address&#125; &#123;end&#125;&apos;)</span><br><span class=\"line\">$ echo $nodes</span><br><span class=\"line\">192.168.1.1 192.168.1.2 192.168.1.3</span><br></pre></td></tr></table></figure></p>\n<p>使用<code>docker login reg.local.com</code>生成auth key</p>\n<p>这里默认为<code>~/.docker/config.json</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat ~/.docker/config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        &quot;auths&quot;: &#123;</span><br><span class=\"line\">                &quot;reg.local.com&quot;: &#123;</span><br><span class=\"line\">                        &quot;auth&quot;: &quot;dGVzdDpEWFEefMTIzNDU=&quot;,</span><br><span class=\"line\">                        &quot;email&quot;: &quot;test@local.com&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>复制auth key到所有节点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ for n in $nodes; do ssh root@$n mkdir -p /root/.docker; scp ~/.docker/config.json root@$n:/root/.docker/config.json; done</span><br></pre></td></tr></table></figure></p>\n<p>复制ca.crt到所有节点<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cert_path=&quot;/etc/docker/certs.d/reg.local.com&quot;</span><br><span class=\"line\">for n in $nodes; do ssh root@$n mkdir -p $cert_path; scp $cert_path/ca.crt root@$n:$cert_path/ca.crt; done</span><br></pre></td></tr></table></figure></p>\n<p>创建帐号secret<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl create secret docker-registry myregistrykey --docker-server=reg.local.com --docker-username=test --docker-password=Test12345 --docker-email=test@local.com</span><br></pre></td></tr></table></figure></p>\n<p>pod配置中增加<code>imagePullSecrets</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: busybox</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - image: reg.local.com/test/busybox</span><br><span class=\"line\">    imagePullPolicy: IfNotPresent</span><br><span class=\"line\">    name: busybox</span><br><span class=\"line\">    command:</span><br><span class=\"line\">      - sleep</span><br><span class=\"line\">      - &quot;3600&quot;</span><br><span class=\"line\">  restartPolicy: Always</span><br><span class=\"line\">  imagePullSecrets:</span><br><span class=\"line\">    - name: myregistrykey</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>到这里docker和kubernetes都可以访问harbor</p>\n<p>下次有机会介绍下几个内容：</p>\n<ul>\n<li>harbor的ha方案，基于它自己提供的复制功能</li>\n<li>harbor部署在kubernetes上</li>\n</ul>\n"},{"title":"Markdown sample","date":"2017-03-08T16:00:00.000Z","_content":"\n# 简介\n\n测试下使用markdown\n\n## 无序列表\n\n- 第一\n- 第二\n\n## 有序列表\n\n1. 我们\n2. 他们\n3. 你们\n\n## 链接\n[简书](http://www.jianshu.com)\n\n## 插入图片\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n## 引用\n\n> 引用他人的文字\n\n# 测试2\n\n## 斜体和粗体\n\n*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n## 代码引用\n\n`hello word`\n\n```\n第一行\n第二行\n```\n\n## 表格\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\ndog | bird | cat\n----|------|----\nfoo | foo  | foo\nbar | bar  | bar\nbaz | baz  | baz\n\n## 显示链接中带括号的图片\n\n![][1]\n[1]: http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1\n","source":"_posts/markdown-sample.md","raw":"title: Markdown sample\ndate: 2017-03-09\ntags:\n- markdown\n---\n\n# 简介\n\n测试下使用markdown\n\n## 无序列表\n\n- 第一\n- 第二\n\n## 有序列表\n\n1. 我们\n2. 他们\n3. 你们\n\n## 链接\n[简书](http://www.jianshu.com)\n\n## 插入图片\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n## 引用\n\n> 引用他人的文字\n\n# 测试2\n\n## 斜体和粗体\n\n*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n## 代码引用\n\n`hello word`\n\n```\n第一行\n第二行\n```\n\n## 表格\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\ndog | bird | cat\n----|------|----\nfoo | foo  | foo\nbar | bar  | bar\nbaz | baz  | baz\n\n## 显示链接中带括号的图片\n\n![][1]\n[1]: http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1\n","slug":"markdown-sample","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48u9000b2tsbz1afm1z0","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>测试下使用markdown</p>\n<h2 id=\"无序列表\"><a href=\"#无序列表\" class=\"headerlink\" title=\"无序列表\"></a>无序列表</h2><ul>\n<li>第一</li>\n<li>第二</li>\n</ul>\n<h2 id=\"有序列表\"><a href=\"#有序列表\" class=\"headerlink\" title=\"有序列表\"></a>有序列表</h2><ol>\n<li>我们</li>\n<li>他们</li>\n<li>你们</li>\n</ol>\n<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a>链接</h2><p><a href=\"http://www.jianshu.com\" target=\"_blank\" rel=\"noopener\">简书</a></p>\n<h2 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h2><p><img src=\"http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg\" alt=\"\"></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><blockquote>\n<p>引用他人的文字</p>\n</blockquote>\n<h1 id=\"测试2\"><a href=\"#测试2\" class=\"headerlink\" title=\"测试2\"></a>测试2</h1><h2 id=\"斜体和粗体\"><a href=\"#斜体和粗体\" class=\"headerlink\" title=\"斜体和粗体\"></a>斜体和粗体</h2><p><em>一盏灯</em>， 一片昏黄；<strong>一简书</strong>， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。</p>\n<h2 id=\"代码引用\"><a href=\"#代码引用\" class=\"headerlink\" title=\"代码引用\"></a>代码引用</h2><p><code>hello word</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">第一行</span><br><span class=\"line\">第二行</span><br></pre></td></tr></table></figure>\n<h2 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h2><table>\n<thead>\n<tr>\n<th>Tables</th>\n<th style=\"text-align:center\">Are</th>\n<th style=\"text-align:right\">Cool</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>col 3 is</td>\n<td style=\"text-align:center\">right-aligned</td>\n<td style=\"text-align:right\">$1600</td>\n</tr>\n<tr>\n<td>col 2 is</td>\n<td style=\"text-align:center\">centered</td>\n<td style=\"text-align:right\">$12</td>\n</tr>\n<tr>\n<td>zebra stripes</td>\n<td style=\"text-align:center\">are neat</td>\n<td style=\"text-align:right\">$1</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>dog</th>\n<th>bird</th>\n<th>cat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>foo</td>\n<td>foo</td>\n<td>foo</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>bar</td>\n<td>bar</td>\n</tr>\n<tr>\n<td>baz</td>\n<td>baz</td>\n<td>baz</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"显示链接中带括号的图片\"><a href=\"#显示链接中带括号的图片\" class=\"headerlink\" title=\"显示链接中带括号的图片\"></a>显示链接中带括号的图片</h2><p>![][1]<br>[1]: <a href=\"http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1\" target=\"_blank\" rel=\"noopener\">http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>测试下使用markdown</p>\n<h2 id=\"无序列表\"><a href=\"#无序列表\" class=\"headerlink\" title=\"无序列表\"></a>无序列表</h2><ul>\n<li>第一</li>\n<li>第二</li>\n</ul>\n<h2 id=\"有序列表\"><a href=\"#有序列表\" class=\"headerlink\" title=\"有序列表\"></a>有序列表</h2><ol>\n<li>我们</li>\n<li>他们</li>\n<li>你们</li>\n</ol>\n<h2 id=\"链接\"><a href=\"#链接\" class=\"headerlink\" title=\"链接\"></a>链接</h2><p><a href=\"http://www.jianshu.com\" target=\"_blank\" rel=\"noopener\">简书</a></p>\n<h2 id=\"插入图片\"><a href=\"#插入图片\" class=\"headerlink\" title=\"插入图片\"></a>插入图片</h2><p><img src=\"http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg\" alt=\"\"></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><blockquote>\n<p>引用他人的文字</p>\n</blockquote>\n<h1 id=\"测试2\"><a href=\"#测试2\" class=\"headerlink\" title=\"测试2\"></a>测试2</h1><h2 id=\"斜体和粗体\"><a href=\"#斜体和粗体\" class=\"headerlink\" title=\"斜体和粗体\"></a>斜体和粗体</h2><p><em>一盏灯</em>， 一片昏黄；<strong>一简书</strong>， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。</p>\n<h2 id=\"代码引用\"><a href=\"#代码引用\" class=\"headerlink\" title=\"代码引用\"></a>代码引用</h2><p><code>hello word</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">第一行</span><br><span class=\"line\">第二行</span><br></pre></td></tr></table></figure>\n<h2 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h2><table>\n<thead>\n<tr>\n<th>Tables</th>\n<th style=\"text-align:center\">Are</th>\n<th style=\"text-align:right\">Cool</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>col 3 is</td>\n<td style=\"text-align:center\">right-aligned</td>\n<td style=\"text-align:right\">$1600</td>\n</tr>\n<tr>\n<td>col 2 is</td>\n<td style=\"text-align:center\">centered</td>\n<td style=\"text-align:right\">$12</td>\n</tr>\n<tr>\n<td>zebra stripes</td>\n<td style=\"text-align:center\">are neat</td>\n<td style=\"text-align:right\">$1</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>dog</th>\n<th>bird</th>\n<th>cat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>foo</td>\n<td>foo</td>\n<td>foo</td>\n</tr>\n<tr>\n<td>bar</td>\n<td>bar</td>\n<td>bar</td>\n</tr>\n<tr>\n<td>baz</td>\n<td>baz</td>\n<td>baz</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"显示链接中带括号的图片\"><a href=\"#显示链接中带括号的图片\" class=\"headerlink\" title=\"显示链接中带括号的图片\"></a>显示链接中带括号的图片</h2><p>![][1]<br>[1]: <a href=\"http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1\" target=\"_blank\" rel=\"noopener\">http://latex.codecogs.com/gif.latex?\\prod%20\\(n_{i}\\)+1</a></p>\n"},{"title":"kuberntes rollout update","date":"2017-03-07T16:00:00.000Z","_content":"\n# “灰度”发布\n\n这次聊聊“灰度”发布（也叫“滚动”发布）\n\n因为互联网或者说网络的发展速度太快导致竞争力很大，各家公司为了保证为用户提供更好的体验就慢慢出现了“灰度”发布这种业务零中断的发布方式。\n\n所谓的“灰度”发布是指为了让老版本更新成新版本，以及为了更充分测试老版本会让部署在生产环境的应用处于老版本和新版本共存，并逐渐过度到新版本的一种发布方式。\n\n其实应用很容易支持“灰度”发布，因为它是无状态的。但是如果应用依赖了数据库那么它就可能变成“有状态”。这里的可能是指当需要变更数据库的时候它是有状态，如果不需要变更数据库它依然是无状态。\n\n所以为了让应用可以支持“灰度”发布，就应该考虑好数据库的依赖问题，主要有两个方面：\n1. 在对事务要求不高的应用上使用非关系数据库，如，mongo，hbase等这类弱模式的数据库。这样在进行应用变更时可以动态变更数据库的表结构（或者说无需变更表结构）\n2. 在设计关系数据库时需要考虑好扩容问题，应避免因表结构变动影响到新老应用的兼容问题\n\n在容器之前一般有两种办法进行“灰度发布”：\n1. 最早采用手工一台一台应用进行发布\n2. 采用部署工具，比如salt或者ansible等进行自动发布\n\n不管采用哪种方式，其原理都是一样的：\n1. 部署一台新的应用\n2. 验证新应用是否正常\n3. 正常后停掉一台老应用\n4. 继续1-3步骤，直到所有应用升级完成\n\n我们目前大部分应用采用的是kubernetes进行的容器化部署，kubernetes本身有一个“滚动”发布的功能用于进行“灰度”发布。下面我们来测试下kuberentes的“滚动”发布是如果玩的。\n\n# 基础测试程序\n\n这里准备了一个叫做testpage的简单应用，这个应用提供一个rest的访问接口返回自身ip和版本，如：I am: 172.1.62.11, the version is: 0.5\n\nDockerfile:\n```shell\nFROM python:2.7-alpine\n\nADD testpage.py /testpage.py\n\nCMD [\"python\", \"/testpage.py\"]\n\nEXPOSE 8080\n```\n\ntestpage.py:\n```python\n#-*- coding:utf-8 -*-\n\n#----------------------------------------------------------------------\nimport socket\nimport fcntl\nimport struct\n\ndef get_ip_address(ifname):\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    return socket.inet_ntoa(fcntl.ioctl(\n        s.fileno(),\n        0x8915,  # SIOCGIFADDR\n        struct.pack('256s', ifname[:15])\n    )[20:24])\n\n#----------------------------------------------------------------------\nimport BaseHTTPServer\nimport os\n\n\nclass RequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n\n    Page = 'I am: %s, the version is: %s\\n' % (get_ip_address('eth0'), os.environ.get('VERSION'))\n\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header(\"Content-Type\", \"text/html\")\n        self.send_header(\"Content-Length\", str(len(self.Page)))\n        self.end_headers()\n        self.wfile.write(self.Page)\n\n#----------------------------------------------------------------------\n\nif __name__ == '__main__':\n    serverAddress = ('', 8080)\n    server = BaseHTTPServer.HTTPServer(serverAddress, RequestHandler)\n    server.serve_forever()\n```\n\npython脚本启动一个web服务，接受一个VERSION的操作系统环境变量，然后将自身的ip和version返回\n\nbuild好上述镜像，取名testpage:[version]\n\n# kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）\n\nservice是为了让集群外的测试脚本可以访问到容器，不是必须\n\nservice.yaml\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  type: NodePort\n  ports:\n  - port: 8080\n    nodePort: 21000\n  selector:\n    type: testpage\n```\n\ndeployment.yaml\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: dean/testpage:0.5\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.5\"\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n```\n\ndeployment.yaml几个需要注意：\n1. revisionHistoryLimit用于限制保留多少份历史版本，因为“滚动”发布每次发布都会最为一个版本来进行管理，默认所有历史版本都会保留，也提供你进行回退等操作\n2. strategy这里制定了“滚动”发布的策略，maxSurge是最大的变动pod数量（这里主要是只新增pod个数）maxUnavailable指定允许最大的无效pod数量，这个值等于0的意义是我们在“滚动”发布的过程中必须保证有replicas个的pod在提供服务\n3. livenessProbe和readinessProbe，这两个参数用于指定如何验证pod的“存活”和“可使用”。“滚动”发布主要使用readinessProbe进行验证。当“滚动”发布过程中一个新pod创建，那么kubernetes需要检查readinessProbe为ok（这里是验证端口8080是否可访问）才会删除老的pod\n\n# 测试\n\n## 部署testpage\n\n部署testpage\n```\n$ kubectl create -f service.yaml\n$ kubectl create -f deployment.yaml\n$ kubectl get po -o wide -l type=testpage\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running   0          4m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running   0          4m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running   0          4m        172.1.14.7    192.168.72.128\n```\n\n## 测试脚本\n\n这个脚本每一秒访问一次testpage服务，返回访问到的pod的ip和version\n```\nwhile true; do date; curl http://192.168.72.2:21000; sleep 1; done\n```\n\n监控testpage的pod变化情况\n```\nwhile true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done\n```\n\n监控testpage的deployment“滚动”发布状态\n```\nwhile true; do date; kubectl rollout status deployment testpage; sleep 1; done\n```\n\n## 开始测试\n\n我们模拟的testpage当前版本是0.8，这时我们需要将其升级到0.9\n\n1. 根据上述dockerfile大家可以直接build出0.9的镜像: dean/testpage:0.9\n2. 修改deployment.yaml（将镜像版本和VERSION变量修改成0.9）\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: dean/testpage:0.9\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.9\"\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n```\n3. 执行测试脚本：\n```\nwhile true; do date; curl http://192.168.72.2:21000; sleep 1; done\n```\n该脚本持续返回：\n```\nThu Mar  9 10:39:09 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:10 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:11 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:12 CST 2017\nI am: 172.1.60.5, the version is: 0.8\nThu Mar  9 10:39:13 CST 2017\nI am: 172.1.62.10, the version is: 0.8\n```\n\n从上面的输出我们可以看到总共有三个pod对应的ip地址\n\n这里会返回三个ip，对应的三个pod，版本是0.5\n\n4. 执行两个监控脚本\n```\nwhile true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done\nwhile true; do date; kubectl rollout status deployment testpage; sleep 1; done\n```\n5. 开始“滚动”发布\n```\nkubectl apply -f deployment.yaml\n```\n6. 观察测试脚本和监控脚本的变化（因为都是动态的就不录制视频了）：\n\nrollout status结果如下\n```\nThu Mar  9 10:42:24 CST 2017\ndeployment \"testpage\" successfully rolled out\nThu Mar  9 10:42:25 CST 2017\nWaiting for deployment spec update to be observed...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 old replicas are pending termination...\nWaiting for rollout to finish: 1 old replicas are pending termination...\ndeployment \"testpage\" successfully rolled out\n```\n上述过程可以看到kuberentes的pod是一个一个更新的。直到最后老pod被清理掉。升级完成\n\n\nkubectl get po输出, 这里可以直观看出整个过程，下列的输出我是截取了关键的变化点。\n\n我在几个关键点后面写上（标注N）用于进行过程解释\n\n标注1，因为配置文件中strategy的配置，不运行kuberentes进行同位替换，所有kuberentes创建了一个新pod\n标注2，新pod创建成功，但是因为readinessProbe的配置kubernetes需要检测到8080端口的可访问才算是READY状态，所以状态READY为0/1（这里数字的原因是一个pod中可以有多个容器）\n标注3，新pod创建成功，并且READY处于1/1（说明一个pod中的容器全部ready）这是该新pod会被添加到service中接受访问，我们结合testpage的访问数据可以看到在43:09时新pod被访问到\n```\nThu Mar  9 10:43:09 CST 2017\nI am: 172.1.65.4, the version is: 0.9\n```\n标注4，包括新pod因为有3个pod可以提供服务，所有这是kubernetes下令中止一个老pod\n标注5，因老pod已被中止（这里是异步的），kuberentes下令开始再创建一个新pod用于进行下一个老pod的替换\n循环这个过程直到所有老pod替换完成，升级结束\n\n```\nThu Mar  9 10:42:39 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running             0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   0/1       ContainerCreating   0          14s       <none>        192.168.72.190   (标注1)\nThu Mar  9 10:42:40 CST 2017\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running   0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running   0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running   0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   0/1       Running   0          15s       172.1.65.4    192.168.72.190    (标注2)\n...\nThu Mar  9 10:43:06 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Terminating         0          7m        172.1.62.10   192.168.72.233   (标注4)\ntestpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running             0          41s       172.1.65.4    192.168.72.190   (标注3)\ntestpage-515849216-1sf6b   0/1       ContainerCreating   0          1s        <none>        192.168.72.233   (标注5)\n...\nThu Mar  9 10:43:18 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Terminating   0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running       0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running       0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          53s       172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   0/1       Running       0          13s       172.1.62.11   192.168.72.233\n...\nThu Mar  9 10:43:36 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-glvbz   1/1       Terminating         0          8m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running             0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running             0          31s       172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   0/1       ContainerCreating   0          0s        <none>        192.168.72.2\n...\nThu Mar  9 10:43:49 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-glvbz   1/1       Terminating   0          8m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running       0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running       0          44s       172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   0/1       Running       0          13s       172.1.60.11   192.168.72.2\n...\nThu Mar  9 10:44:07 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-hpx9s   1/1       Terminating   0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running       0          1m        172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   1/1       Running       0          31s       172.1.60.11   192.168.72.2\n...\nThu Mar  9 10:47:46 CST 2017\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-515849216-059b0   1/1       Running   0          5m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running   0          4m        172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   1/1       Running   0          4m        172.1.60.11   192.168.72.2\n```\n\n# 总结\n\n1. 策略配置让kubernetes必须先创建一个新pod去替换老pod（而不是删除老pod再创建新pod，这个策略可自行配置）\n2. 每次仅替换一个pod，并进行业务的无缝切换\n3. 整个过程readinessProbe非常重要，确定着业务是否会被中断（必须确保readinessProbe的验证是对业务的准确验证，因为测试这里仅进行端口验证）\n\n\n# kubernetes支持其他rollout操作\n\nRead the deployment history\n\n```\nkubectl rollout history deployment testpage\nkubectl rollout history deployment testpage --revision 42\n```\n\nRollback to the previous deployed version\n\n```\nkubectl rollout undo deployment testpage\nkubectl rollout undo deployment testpage --to-revision 21\n```\n\nPause and Resume\n\n```\nkubectl rollout pause deployment testpage\nkubectl rollout resume deployment testpage\n```\n\n","source":"_posts/kuberntes-rollout-update.md","raw":"title: kuberntes rollout update\ndate: 2017-03-08\ntags:\n- kubernetes\n- rollout\n---\n\n# “灰度”发布\n\n这次聊聊“灰度”发布（也叫“滚动”发布）\n\n因为互联网或者说网络的发展速度太快导致竞争力很大，各家公司为了保证为用户提供更好的体验就慢慢出现了“灰度”发布这种业务零中断的发布方式。\n\n所谓的“灰度”发布是指为了让老版本更新成新版本，以及为了更充分测试老版本会让部署在生产环境的应用处于老版本和新版本共存，并逐渐过度到新版本的一种发布方式。\n\n其实应用很容易支持“灰度”发布，因为它是无状态的。但是如果应用依赖了数据库那么它就可能变成“有状态”。这里的可能是指当需要变更数据库的时候它是有状态，如果不需要变更数据库它依然是无状态。\n\n所以为了让应用可以支持“灰度”发布，就应该考虑好数据库的依赖问题，主要有两个方面：\n1. 在对事务要求不高的应用上使用非关系数据库，如，mongo，hbase等这类弱模式的数据库。这样在进行应用变更时可以动态变更数据库的表结构（或者说无需变更表结构）\n2. 在设计关系数据库时需要考虑好扩容问题，应避免因表结构变动影响到新老应用的兼容问题\n\n在容器之前一般有两种办法进行“灰度发布”：\n1. 最早采用手工一台一台应用进行发布\n2. 采用部署工具，比如salt或者ansible等进行自动发布\n\n不管采用哪种方式，其原理都是一样的：\n1. 部署一台新的应用\n2. 验证新应用是否正常\n3. 正常后停掉一台老应用\n4. 继续1-3步骤，直到所有应用升级完成\n\n我们目前大部分应用采用的是kubernetes进行的容器化部署，kubernetes本身有一个“滚动”发布的功能用于进行“灰度”发布。下面我们来测试下kuberentes的“滚动”发布是如果玩的。\n\n# 基础测试程序\n\n这里准备了一个叫做testpage的简单应用，这个应用提供一个rest的访问接口返回自身ip和版本，如：I am: 172.1.62.11, the version is: 0.5\n\nDockerfile:\n```shell\nFROM python:2.7-alpine\n\nADD testpage.py /testpage.py\n\nCMD [\"python\", \"/testpage.py\"]\n\nEXPOSE 8080\n```\n\ntestpage.py:\n```python\n#-*- coding:utf-8 -*-\n\n#----------------------------------------------------------------------\nimport socket\nimport fcntl\nimport struct\n\ndef get_ip_address(ifname):\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    return socket.inet_ntoa(fcntl.ioctl(\n        s.fileno(),\n        0x8915,  # SIOCGIFADDR\n        struct.pack('256s', ifname[:15])\n    )[20:24])\n\n#----------------------------------------------------------------------\nimport BaseHTTPServer\nimport os\n\n\nclass RequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\n\n    Page = 'I am: %s, the version is: %s\\n' % (get_ip_address('eth0'), os.environ.get('VERSION'))\n\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header(\"Content-Type\", \"text/html\")\n        self.send_header(\"Content-Length\", str(len(self.Page)))\n        self.end_headers()\n        self.wfile.write(self.Page)\n\n#----------------------------------------------------------------------\n\nif __name__ == '__main__':\n    serverAddress = ('', 8080)\n    server = BaseHTTPServer.HTTPServer(serverAddress, RequestHandler)\n    server.serve_forever()\n```\n\npython脚本启动一个web服务，接受一个VERSION的操作系统环境变量，然后将自身的ip和version返回\n\nbuild好上述镜像，取名testpage:[version]\n\n# kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）\n\nservice是为了让集群外的测试脚本可以访问到容器，不是必须\n\nservice.yaml\n```yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  type: NodePort\n  ports:\n  - port: 8080\n    nodePort: 21000\n  selector:\n    type: testpage\n```\n\ndeployment.yaml\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: dean/testpage:0.5\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.5\"\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n```\n\ndeployment.yaml几个需要注意：\n1. revisionHistoryLimit用于限制保留多少份历史版本，因为“滚动”发布每次发布都会最为一个版本来进行管理，默认所有历史版本都会保留，也提供你进行回退等操作\n2. strategy这里制定了“滚动”发布的策略，maxSurge是最大的变动pod数量（这里主要是只新增pod个数）maxUnavailable指定允许最大的无效pod数量，这个值等于0的意义是我们在“滚动”发布的过程中必须保证有replicas个的pod在提供服务\n3. livenessProbe和readinessProbe，这两个参数用于指定如何验证pod的“存活”和“可使用”。“滚动”发布主要使用readinessProbe进行验证。当“滚动”发布过程中一个新pod创建，那么kubernetes需要检查readinessProbe为ok（这里是验证端口8080是否可访问）才会删除老的pod\n\n# 测试\n\n## 部署testpage\n\n部署testpage\n```\n$ kubectl create -f service.yaml\n$ kubectl create -f deployment.yaml\n$ kubectl get po -o wide -l type=testpage\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running   0          4m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running   0          4m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running   0          4m        172.1.14.7    192.168.72.128\n```\n\n## 测试脚本\n\n这个脚本每一秒访问一次testpage服务，返回访问到的pod的ip和version\n```\nwhile true; do date; curl http://192.168.72.2:21000; sleep 1; done\n```\n\n监控testpage的pod变化情况\n```\nwhile true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done\n```\n\n监控testpage的deployment“滚动”发布状态\n```\nwhile true; do date; kubectl rollout status deployment testpage; sleep 1; done\n```\n\n## 开始测试\n\n我们模拟的testpage当前版本是0.8，这时我们需要将其升级到0.9\n\n1. 根据上述dockerfile大家可以直接build出0.9的镜像: dean/testpage:0.9\n2. 修改deployment.yaml（将镜像版本和VERSION变量修改成0.9）\n```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: testpage\n  namespace: default\n  labels:\n    type: testpage\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        type: testpage\n    spec:\n      containers:\n      - name: testpage\n        image: dean/testpage:0.9\n        imagePullPolicy: Always\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n        env:\n        - name: VERSION\n          value: \"0.9\"\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          tcpSocket:\n            port: 8080\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n```\n3. 执行测试脚本：\n```\nwhile true; do date; curl http://192.168.72.2:21000; sleep 1; done\n```\n该脚本持续返回：\n```\nThu Mar  9 10:39:09 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:10 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:11 CST 2017\nI am: 172.1.14.7, the version is: 0.8\nThu Mar  9 10:39:12 CST 2017\nI am: 172.1.60.5, the version is: 0.8\nThu Mar  9 10:39:13 CST 2017\nI am: 172.1.62.10, the version is: 0.8\n```\n\n从上面的输出我们可以看到总共有三个pod对应的ip地址\n\n这里会返回三个ip，对应的三个pod，版本是0.5\n\n4. 执行两个监控脚本\n```\nwhile true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done\nwhile true; do date; kubectl rollout status deployment testpage; sleep 1; done\n```\n5. 开始“滚动”发布\n```\nkubectl apply -f deployment.yaml\n```\n6. 观察测试脚本和监控脚本的变化（因为都是动态的就不录制视频了）：\n\nrollout status结果如下\n```\nThu Mar  9 10:42:24 CST 2017\ndeployment \"testpage\" successfully rolled out\nThu Mar  9 10:42:25 CST 2017\nWaiting for deployment spec update to be observed...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nWaiting for rollout to finish: 1 old replicas are pending termination...\nWaiting for rollout to finish: 1 old replicas are pending termination...\ndeployment \"testpage\" successfully rolled out\n```\n上述过程可以看到kuberentes的pod是一个一个更新的。直到最后老pod被清理掉。升级完成\n\n\nkubectl get po输出, 这里可以直观看出整个过程，下列的输出我是截取了关键的变化点。\n\n我在几个关键点后面写上（标注N）用于进行过程解释\n\n标注1，因为配置文件中strategy的配置，不运行kuberentes进行同位替换，所有kuberentes创建了一个新pod\n标注2，新pod创建成功，但是因为readinessProbe的配置kubernetes需要检测到8080端口的可访问才算是READY状态，所以状态READY为0/1（这里数字的原因是一个pod中可以有多个容器）\n标注3，新pod创建成功，并且READY处于1/1（说明一个pod中的容器全部ready）这是该新pod会被添加到service中接受访问，我们结合testpage的访问数据可以看到在43:09时新pod被访问到\n```\nThu Mar  9 10:43:09 CST 2017\nI am: 172.1.65.4, the version is: 0.9\n```\n标注4，包括新pod因为有3个pod可以提供服务，所有这是kubernetes下令中止一个老pod\n标注5，因老pod已被中止（这里是异步的），kuberentes下令开始再创建一个新pod用于进行下一个老pod的替换\n循环这个过程直到所有老pod替换完成，升级结束\n\n```\nThu Mar  9 10:42:39 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running             0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   0/1       ContainerCreating   0          14s       <none>        192.168.72.190   (标注1)\nThu Mar  9 10:42:40 CST 2017\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Running   0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running   0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running   0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   0/1       Running   0          15s       172.1.65.4    192.168.72.190    (标注2)\n...\nThu Mar  9 10:43:06 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Terminating         0          7m        172.1.62.10   192.168.72.233   (标注4)\ntestpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running             0          41s       172.1.65.4    192.168.72.190   (标注3)\ntestpage-515849216-1sf6b   0/1       ContainerCreating   0          1s        <none>        192.168.72.233   (标注5)\n...\nThu Mar  9 10:43:18 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-fm0m1   1/1       Terminating   0          7m        172.1.62.10   192.168.72.233\ntestpage-230767614-glvbz   1/1       Running       0          7m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running       0          7m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          53s       172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   0/1       Running       0          13s       172.1.62.11   192.168.72.233\n...\nThu Mar  9 10:43:36 CST 2017\nNAME                       READY     STATUS              RESTARTS   AGE       IP            NODE\ntestpage-230767614-glvbz   1/1       Terminating         0          8m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running             0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running             0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running             0          31s       172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   0/1       ContainerCreating   0          0s        <none>        192.168.72.2\n...\nThu Mar  9 10:43:49 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-glvbz   1/1       Terminating   0          8m        172.1.60.5    192.168.72.2\ntestpage-230767614-hpx9s   1/1       Running       0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running       0          44s       172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   0/1       Running       0          13s       172.1.60.11   192.168.72.2\n...\nThu Mar  9 10:44:07 CST 2017\nNAME                       READY     STATUS        RESTARTS   AGE       IP            NODE\ntestpage-230767614-hpx9s   1/1       Terminating   0          8m        172.1.14.7    192.168.72.128\ntestpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running       0          1m        172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   1/1       Running       0          31s       172.1.60.11   192.168.72.2\n...\nThu Mar  9 10:47:46 CST 2017\nNAME                       READY     STATUS    RESTARTS   AGE       IP            NODE\ntestpage-515849216-059b0   1/1       Running   0          5m        172.1.65.4    192.168.72.190\ntestpage-515849216-1sf6b   1/1       Running   0          4m        172.1.62.11   192.168.72.233\ntestpage-515849216-5jfb5   1/1       Running   0          4m        172.1.60.11   192.168.72.2\n```\n\n# 总结\n\n1. 策略配置让kubernetes必须先创建一个新pod去替换老pod（而不是删除老pod再创建新pod，这个策略可自行配置）\n2. 每次仅替换一个pod，并进行业务的无缝切换\n3. 整个过程readinessProbe非常重要，确定着业务是否会被中断（必须确保readinessProbe的验证是对业务的准确验证，因为测试这里仅进行端口验证）\n\n\n# kubernetes支持其他rollout操作\n\nRead the deployment history\n\n```\nkubectl rollout history deployment testpage\nkubectl rollout history deployment testpage --revision 42\n```\n\nRollback to the previous deployed version\n\n```\nkubectl rollout undo deployment testpage\nkubectl rollout undo deployment testpage --to-revision 21\n```\n\nPause and Resume\n\n```\nkubectl rollout pause deployment testpage\nkubectl rollout resume deployment testpage\n```\n\n","slug":"kuberntes-rollout-update","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48ub000d2tsbo2cd9kx2","content":"<h1 id=\"“灰度”发布\"><a href=\"#“灰度”发布\" class=\"headerlink\" title=\"“灰度”发布\"></a>“灰度”发布</h1><p>这次聊聊“灰度”发布（也叫“滚动”发布）</p>\n<p>因为互联网或者说网络的发展速度太快导致竞争力很大，各家公司为了保证为用户提供更好的体验就慢慢出现了“灰度”发布这种业务零中断的发布方式。</p>\n<p>所谓的“灰度”发布是指为了让老版本更新成新版本，以及为了更充分测试老版本会让部署在生产环境的应用处于老版本和新版本共存，并逐渐过度到新版本的一种发布方式。</p>\n<p>其实应用很容易支持“灰度”发布，因为它是无状态的。但是如果应用依赖了数据库那么它就可能变成“有状态”。这里的可能是指当需要变更数据库的时候它是有状态，如果不需要变更数据库它依然是无状态。</p>\n<p>所以为了让应用可以支持“灰度”发布，就应该考虑好数据库的依赖问题，主要有两个方面：</p>\n<ol>\n<li>在对事务要求不高的应用上使用非关系数据库，如，mongo，hbase等这类弱模式的数据库。这样在进行应用变更时可以动态变更数据库的表结构（或者说无需变更表结构）</li>\n<li>在设计关系数据库时需要考虑好扩容问题，应避免因表结构变动影响到新老应用的兼容问题</li>\n</ol>\n<p>在容器之前一般有两种办法进行“灰度发布”：</p>\n<ol>\n<li>最早采用手工一台一台应用进行发布</li>\n<li>采用部署工具，比如salt或者ansible等进行自动发布</li>\n</ol>\n<p>不管采用哪种方式，其原理都是一样的：</p>\n<ol>\n<li>部署一台新的应用</li>\n<li>验证新应用是否正常</li>\n<li>正常后停掉一台老应用</li>\n<li>继续1-3步骤，直到所有应用升级完成</li>\n</ol>\n<p>我们目前大部分应用采用的是kubernetes进行的容器化部署，kubernetes本身有一个“滚动”发布的功能用于进行“灰度”发布。下面我们来测试下kuberentes的“滚动”发布是如果玩的。</p>\n<h1 id=\"基础测试程序\"><a href=\"#基础测试程序\" class=\"headerlink\" title=\"基础测试程序\"></a>基础测试程序</h1><p>这里准备了一个叫做testpage的简单应用，这个应用提供一个rest的访问接口返回自身ip和版本，如：I am: 172.1.62.11, the version is: 0.5</p>\n<p>Dockerfile:<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM python:2.7-alpine</span><br><span class=\"line\"></span><br><span class=\"line\">ADD testpage.py /testpage.py</span><br><span class=\"line\"></span><br><span class=\"line\">CMD [\"python\", \"/testpage.py\"]</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 8080</span><br></pre></td></tr></table></figure></p>\n<p>testpage.py:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding:utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> socket</span><br><span class=\"line\"><span class=\"keyword\">import</span> fcntl</span><br><span class=\"line\"><span class=\"keyword\">import</span> struct</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_ip_address</span><span class=\"params\">(ifname)</span>:</span></span><br><span class=\"line\">    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> socket.inet_ntoa(fcntl.ioctl(</span><br><span class=\"line\">        s.fileno(),</span><br><span class=\"line\">        <span class=\"number\">0x8915</span>,  <span class=\"comment\"># SIOCGIFADDR</span></span><br><span class=\"line\">        struct.pack(<span class=\"string\">'256s'</span>, ifname[:<span class=\"number\">15</span>])</span><br><span class=\"line\">    )[<span class=\"number\">20</span>:<span class=\"number\">24</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> BaseHTTPServer</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RequestHandler</span><span class=\"params\">(BaseHTTPServer.BaseHTTPRequestHandler)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    Page = <span class=\"string\">'I am: %s, the version is: %s\\n'</span> % (get_ip_address(<span class=\"string\">'eth0'</span>), os.environ.get(<span class=\"string\">'VERSION'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">do_GET</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.send_response(<span class=\"number\">200</span>)</span><br><span class=\"line\">        self.send_header(<span class=\"string\">\"Content-Type\"</span>, <span class=\"string\">\"text/html\"</span>)</span><br><span class=\"line\">        self.send_header(<span class=\"string\">\"Content-Length\"</span>, str(len(self.Page)))</span><br><span class=\"line\">        self.end_headers()</span><br><span class=\"line\">        self.wfile.write(self.Page)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    serverAddress = (<span class=\"string\">''</span>, <span class=\"number\">8080</span>)</span><br><span class=\"line\">    server = BaseHTTPServer.HTTPServer(serverAddress, RequestHandler)</span><br><span class=\"line\">    server.serve_forever()</span><br></pre></td></tr></table></figure></p>\n<p>python脚本启动一个web服务，接受一个VERSION的操作系统环境变量，然后将自身的ip和version返回</p>\n<p>build好上述镜像，取名testpage:[version]</p>\n<h1 id=\"kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes-io脑补下）\"><a href=\"#kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes-io脑补下）\" class=\"headerlink\" title=\"kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）\"></a>kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）</h1><p>service是为了让集群外的测试脚本可以访问到容器，不是必须</p>\n<p>service.yaml<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">NodePort</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">    nodePort:</span> <span class=\"number\">21000</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br></pre></td></tr></table></figure></p>\n<p>deployment.yaml<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"attr\">  revisionHistoryLimit:</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"attr\">  strategy:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">RollingUpdate</span></span><br><span class=\"line\"><span class=\"attr\">    rollingUpdate:</span></span><br><span class=\"line\"><span class=\"attr\">      maxSurge:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">      maxUnavailable:</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">        image:</span> <span class=\"string\">dean/testpage:0.5</span></span><br><span class=\"line\"><span class=\"attr\">        imagePullPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">10</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        env:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">VERSION</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"string\">\"0.5\"</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - containerPort:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">        readinessProbe:</span></span><br><span class=\"line\"><span class=\"attr\">          tcpSocket:</span></span><br><span class=\"line\"><span class=\"attr\">            port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">        livenessProbe:</span></span><br><span class=\"line\"><span class=\"attr\">          tcpSocket:</span></span><br><span class=\"line\"><span class=\"attr\">            port:</span> <span class=\"number\">8080</span></span><br></pre></td></tr></table></figure></p>\n<p>deployment.yaml几个需要注意：</p>\n<ol>\n<li>revisionHistoryLimit用于限制保留多少份历史版本，因为“滚动”发布每次发布都会最为一个版本来进行管理，默认所有历史版本都会保留，也提供你进行回退等操作</li>\n<li>strategy这里制定了“滚动”发布的策略，maxSurge是最大的变动pod数量（这里主要是只新增pod个数）maxUnavailable指定允许最大的无效pod数量，这个值等于0的意义是我们在“滚动”发布的过程中必须保证有replicas个的pod在提供服务</li>\n<li>livenessProbe和readinessProbe，这两个参数用于指定如何验证pod的“存活”和“可使用”。“滚动”发布主要使用readinessProbe进行验证。当“滚动”发布过程中一个新pod创建，那么kubernetes需要检查readinessProbe为ok（这里是验证端口8080是否可访问）才会删除老的pod</li>\n</ol>\n<h1 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h1><h2 id=\"部署testpage\"><a href=\"#部署testpage\" class=\"headerlink\" title=\"部署testpage\"></a>部署testpage</h2><p>部署testpage<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create -f service.yaml</span><br><span class=\"line\">$ kubectl create -f deployment.yaml</span><br><span class=\"line\">$ kubectl get po -o wide -l type=testpage</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running   0          4m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running   0          4m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running   0          4m        172.1.14.7    192.168.72.128</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"测试脚本\"><a href=\"#测试脚本\" class=\"headerlink\" title=\"测试脚本\"></a>测试脚本</h2><p>这个脚本每一秒访问一次testpage服务，返回访问到的pod的ip和version<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; curl http://192.168.72.2:21000; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<p>监控testpage的pod变化情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<p>监控testpage的deployment“滚动”发布状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl rollout status deployment testpage; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"开始测试\"><a href=\"#开始测试\" class=\"headerlink\" title=\"开始测试\"></a>开始测试</h2><p>我们模拟的testpage当前版本是0.8，这时我们需要将其升级到0.9</p>\n<ol>\n<li>根据上述dockerfile大家可以直接build出0.9的镜像: dean/testpage:0.9</li>\n<li><p>修改deployment.yaml（将镜像版本和VERSION变量修改成0.9）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: testpage</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    type: testpage</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 3</span><br><span class=\"line\">  revisionHistoryLimit: 3</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxSurge: 1</span><br><span class=\"line\">      maxUnavailable: 0</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        type: testpage</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: testpage</span><br><span class=\"line\">        image: dean/testpage:0.9</span><br><span class=\"line\">        imagePullPolicy: Always</span><br><span class=\"line\">        resources:</span><br><span class=\"line\">          limits:</span><br><span class=\"line\">            cpu: 10m</span><br><span class=\"line\">            memory: 20Mi</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: VERSION</span><br><span class=\"line\">          value: &quot;0.9&quot;</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8080</span><br><span class=\"line\">        readinessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 8080</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 8080</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行测试脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; curl http://192.168.72.2:21000; sleep 1; done</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>该脚本持续返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:39:09 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:10 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:11 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:12 CST 2017</span><br><span class=\"line\">I am: 172.1.60.5, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:13 CST 2017</span><br><span class=\"line\">I am: 172.1.62.10, the version is: 0.8</span><br></pre></td></tr></table></figure></p>\n<p>从上面的输出我们可以看到总共有三个pod对应的ip地址</p>\n<p>这里会返回三个ip，对应的三个pod，版本是0.5</p>\n<ol start=\"4\">\n<li><p>执行两个监控脚本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done</span><br><span class=\"line\">while true; do date; kubectl rollout status deployment testpage; sleep 1; done</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开始“滚动”发布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f deployment.yaml</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>观察测试脚本和监控脚本的变化（因为都是动态的就不录制视频了）：</p>\n</li>\n</ol>\n<p>rollout status结果如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:42:24 CST 2017</span><br><span class=\"line\">deployment &quot;testpage&quot; successfully rolled out</span><br><span class=\"line\">Thu Mar  9 10:42:25 CST 2017</span><br><span class=\"line\">Waiting for deployment spec update to be observed...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 old replicas are pending termination...</span><br><span class=\"line\">Waiting for rollout to finish: 1 old replicas are pending termination...</span><br><span class=\"line\">deployment &quot;testpage&quot; successfully rolled out</span><br></pre></td></tr></table></figure></p>\n<p>上述过程可以看到kuberentes的pod是一个一个更新的。直到最后老pod被清理掉。升级完成</p>\n<p>kubectl get po输出, 这里可以直观看出整个过程，下列的输出我是截取了关键的变化点。</p>\n<p>我在几个关键点后面写上（标注N）用于进行过程解释</p>\n<p>标注1，因为配置文件中strategy的配置，不运行kuberentes进行同位替换，所有kuberentes创建了一个新pod<br>标注2，新pod创建成功，但是因为readinessProbe的配置kubernetes需要检测到8080端口的可访问才算是READY状态，所以状态READY为0/1（这里数字的原因是一个pod中可以有多个容器）<br>标注3，新pod创建成功，并且READY处于1/1（说明一个pod中的容器全部ready）这是该新pod会被添加到service中接受访问，我们结合testpage的访问数据可以看到在43:09时新pod被访问到<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:43:09 CST 2017</span><br><span class=\"line\">I am: 172.1.65.4, the version is: 0.9</span><br></pre></td></tr></table></figure></p>\n<p>标注4，包括新pod因为有3个pod可以提供服务，所有这是kubernetes下令中止一个老pod<br>标注5，因老pod已被中止（这里是异步的），kuberentes下令开始再创建一个新pod用于进行下一个老pod的替换<br>循环这个过程直到所有老pod替换完成，升级结束</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:42:39 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running             0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   0/1       ContainerCreating   0          14s       &lt;none&gt;        192.168.72.190   (标注1)</span><br><span class=\"line\">Thu Mar  9 10:42:40 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running   0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running   0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running   0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   0/1       Running   0          15s       172.1.65.4    192.168.72.190    (标注2)</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:06 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Terminating         0          7m        172.1.62.10   192.168.72.233   (标注4)</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running             0          41s       172.1.65.4    192.168.72.190   (标注3)</span><br><span class=\"line\">testpage-515849216-1sf6b   0/1       ContainerCreating   0          1s        &lt;none&gt;        192.168.72.233   (标注5)</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:18 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Terminating   0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running       0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running       0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          53s       172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   0/1       Running       0          13s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:36 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Terminating         0          8m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running             0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running             0          31s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   0/1       ContainerCreating   0          0s        &lt;none&gt;        192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:49 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Terminating   0          8m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running       0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running       0          44s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   0/1       Running       0          13s       172.1.60.11   192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:44:07 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Terminating   0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running       0          1m        172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   1/1       Running       0          31s       172.1.60.11   192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:47:46 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running   0          5m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running   0          4m        172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   1/1       Running   0          4m        172.1.60.11   192.168.72.2</span><br></pre></td></tr></table></figure>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><ol>\n<li>策略配置让kubernetes必须先创建一个新pod去替换老pod（而不是删除老pod再创建新pod，这个策略可自行配置）</li>\n<li>每次仅替换一个pod，并进行业务的无缝切换</li>\n<li>整个过程readinessProbe非常重要，确定着业务是否会被中断（必须确保readinessProbe的验证是对业务的准确验证，因为测试这里仅进行端口验证）</li>\n</ol>\n<h1 id=\"kubernetes支持其他rollout操作\"><a href=\"#kubernetes支持其他rollout操作\" class=\"headerlink\" title=\"kubernetes支持其他rollout操作\"></a>kubernetes支持其他rollout操作</h1><p>Read the deployment history</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout history deployment testpage</span><br><span class=\"line\">kubectl rollout history deployment testpage --revision 42</span><br></pre></td></tr></table></figure>\n<p>Rollback to the previous deployed version</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout undo deployment testpage</span><br><span class=\"line\">kubectl rollout undo deployment testpage --to-revision 21</span><br></pre></td></tr></table></figure>\n<p>Pause and Resume</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout pause deployment testpage</span><br><span class=\"line\">kubectl rollout resume deployment testpage</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"“灰度”发布\"><a href=\"#“灰度”发布\" class=\"headerlink\" title=\"“灰度”发布\"></a>“灰度”发布</h1><p>这次聊聊“灰度”发布（也叫“滚动”发布）</p>\n<p>因为互联网或者说网络的发展速度太快导致竞争力很大，各家公司为了保证为用户提供更好的体验就慢慢出现了“灰度”发布这种业务零中断的发布方式。</p>\n<p>所谓的“灰度”发布是指为了让老版本更新成新版本，以及为了更充分测试老版本会让部署在生产环境的应用处于老版本和新版本共存，并逐渐过度到新版本的一种发布方式。</p>\n<p>其实应用很容易支持“灰度”发布，因为它是无状态的。但是如果应用依赖了数据库那么它就可能变成“有状态”。这里的可能是指当需要变更数据库的时候它是有状态，如果不需要变更数据库它依然是无状态。</p>\n<p>所以为了让应用可以支持“灰度”发布，就应该考虑好数据库的依赖问题，主要有两个方面：</p>\n<ol>\n<li>在对事务要求不高的应用上使用非关系数据库，如，mongo，hbase等这类弱模式的数据库。这样在进行应用变更时可以动态变更数据库的表结构（或者说无需变更表结构）</li>\n<li>在设计关系数据库时需要考虑好扩容问题，应避免因表结构变动影响到新老应用的兼容问题</li>\n</ol>\n<p>在容器之前一般有两种办法进行“灰度发布”：</p>\n<ol>\n<li>最早采用手工一台一台应用进行发布</li>\n<li>采用部署工具，比如salt或者ansible等进行自动发布</li>\n</ol>\n<p>不管采用哪种方式，其原理都是一样的：</p>\n<ol>\n<li>部署一台新的应用</li>\n<li>验证新应用是否正常</li>\n<li>正常后停掉一台老应用</li>\n<li>继续1-3步骤，直到所有应用升级完成</li>\n</ol>\n<p>我们目前大部分应用采用的是kubernetes进行的容器化部署，kubernetes本身有一个“滚动”发布的功能用于进行“灰度”发布。下面我们来测试下kuberentes的“滚动”发布是如果玩的。</p>\n<h1 id=\"基础测试程序\"><a href=\"#基础测试程序\" class=\"headerlink\" title=\"基础测试程序\"></a>基础测试程序</h1><p>这里准备了一个叫做testpage的简单应用，这个应用提供一个rest的访问接口返回自身ip和版本，如：I am: 172.1.62.11, the version is: 0.5</p>\n<p>Dockerfile:<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM python:2.7-alpine</span><br><span class=\"line\"></span><br><span class=\"line\">ADD testpage.py /testpage.py</span><br><span class=\"line\"></span><br><span class=\"line\">CMD [\"python\", \"/testpage.py\"]</span><br><span class=\"line\"></span><br><span class=\"line\">EXPOSE 8080</span><br></pre></td></tr></table></figure></p>\n<p>testpage.py:<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#-*- coding:utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> socket</span><br><span class=\"line\"><span class=\"keyword\">import</span> fcntl</span><br><span class=\"line\"><span class=\"keyword\">import</span> struct</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_ip_address</span><span class=\"params\">(ifname)</span>:</span></span><br><span class=\"line\">    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> socket.inet_ntoa(fcntl.ioctl(</span><br><span class=\"line\">        s.fileno(),</span><br><span class=\"line\">        <span class=\"number\">0x8915</span>,  <span class=\"comment\"># SIOCGIFADDR</span></span><br><span class=\"line\">        struct.pack(<span class=\"string\">'256s'</span>, ifname[:<span class=\"number\">15</span>])</span><br><span class=\"line\">    )[<span class=\"number\">20</span>:<span class=\"number\">24</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> BaseHTTPServer</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RequestHandler</span><span class=\"params\">(BaseHTTPServer.BaseHTTPRequestHandler)</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    Page = <span class=\"string\">'I am: %s, the version is: %s\\n'</span> % (get_ip_address(<span class=\"string\">'eth0'</span>), os.environ.get(<span class=\"string\">'VERSION'</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">do_GET</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        self.send_response(<span class=\"number\">200</span>)</span><br><span class=\"line\">        self.send_header(<span class=\"string\">\"Content-Type\"</span>, <span class=\"string\">\"text/html\"</span>)</span><br><span class=\"line\">        self.send_header(<span class=\"string\">\"Content-Length\"</span>, str(len(self.Page)))</span><br><span class=\"line\">        self.end_headers()</span><br><span class=\"line\">        self.wfile.write(self.Page)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#----------------------------------------------------------------------</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    serverAddress = (<span class=\"string\">''</span>, <span class=\"number\">8080</span>)</span><br><span class=\"line\">    server = BaseHTTPServer.HTTPServer(serverAddress, RequestHandler)</span><br><span class=\"line\">    server.serve_forever()</span><br></pre></td></tr></table></figure></p>\n<p>python脚本启动一个web服务，接受一个VERSION的操作系统环境变量，然后将自身的ip和version返回</p>\n<p>build好上述镜像，取名testpage:[version]</p>\n<h1 id=\"kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes-io脑补下）\"><a href=\"#kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes-io脑补下）\" class=\"headerlink\" title=\"kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）\"></a>kubernetes的部署脚本（如果你不会写k8s的yaml需要自己去kubernetes.io脑补下）</h1><p>service是为了让集群外的测试脚本可以访问到容器，不是必须</p>\n<p>service.yaml<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">NodePort</span></span><br><span class=\"line\"><span class=\"attr\">  ports:</span></span><br><span class=\"line\"><span class=\"attr\">  - port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">    nodePort:</span> <span class=\"number\">21000</span></span><br><span class=\"line\"><span class=\"attr\">  selector:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br></pre></td></tr></table></figure></p>\n<p>deployment.yaml<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">extensions/v1beta1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">  namespace:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">  labels:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  replicas:</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"attr\">  revisionHistoryLimit:</span> <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"attr\">  strategy:</span></span><br><span class=\"line\"><span class=\"attr\">    type:</span> <span class=\"string\">RollingUpdate</span></span><br><span class=\"line\"><span class=\"attr\">    rollingUpdate:</span></span><br><span class=\"line\"><span class=\"attr\">      maxSurge:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">      maxUnavailable:</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"attr\">  template:</span></span><br><span class=\"line\"><span class=\"attr\">    metadata:</span></span><br><span class=\"line\"><span class=\"attr\">      labels:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">    spec:</span></span><br><span class=\"line\"><span class=\"attr\">      containers:</span></span><br><span class=\"line\"><span class=\"attr\">      - name:</span> <span class=\"string\">testpage</span></span><br><span class=\"line\"><span class=\"attr\">        image:</span> <span class=\"string\">dean/testpage:0.5</span></span><br><span class=\"line\"><span class=\"attr\">        imagePullPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">        resources:</span></span><br><span class=\"line\"><span class=\"attr\">          limits:</span></span><br><span class=\"line\"><span class=\"attr\">            cpu:</span> <span class=\"number\">10</span><span class=\"string\">m</span></span><br><span class=\"line\"><span class=\"attr\">            memory:</span> <span class=\"number\">20</span><span class=\"string\">Mi</span></span><br><span class=\"line\"><span class=\"attr\">        env:</span></span><br><span class=\"line\"><span class=\"attr\">        - name:</span> <span class=\"string\">VERSION</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"string\">\"0.5\"</span></span><br><span class=\"line\"><span class=\"attr\">        ports:</span></span><br><span class=\"line\"><span class=\"attr\">        - containerPort:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">        readinessProbe:</span></span><br><span class=\"line\"><span class=\"attr\">          tcpSocket:</span></span><br><span class=\"line\"><span class=\"attr\">            port:</span> <span class=\"number\">8080</span></span><br><span class=\"line\"><span class=\"attr\">        livenessProbe:</span></span><br><span class=\"line\"><span class=\"attr\">          tcpSocket:</span></span><br><span class=\"line\"><span class=\"attr\">            port:</span> <span class=\"number\">8080</span></span><br></pre></td></tr></table></figure></p>\n<p>deployment.yaml几个需要注意：</p>\n<ol>\n<li>revisionHistoryLimit用于限制保留多少份历史版本，因为“滚动”发布每次发布都会最为一个版本来进行管理，默认所有历史版本都会保留，也提供你进行回退等操作</li>\n<li>strategy这里制定了“滚动”发布的策略，maxSurge是最大的变动pod数量（这里主要是只新增pod个数）maxUnavailable指定允许最大的无效pod数量，这个值等于0的意义是我们在“滚动”发布的过程中必须保证有replicas个的pod在提供服务</li>\n<li>livenessProbe和readinessProbe，这两个参数用于指定如何验证pod的“存活”和“可使用”。“滚动”发布主要使用readinessProbe进行验证。当“滚动”发布过程中一个新pod创建，那么kubernetes需要检查readinessProbe为ok（这里是验证端口8080是否可访问）才会删除老的pod</li>\n</ol>\n<h1 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h1><h2 id=\"部署testpage\"><a href=\"#部署testpage\" class=\"headerlink\" title=\"部署testpage\"></a>部署testpage</h2><p>部署testpage<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl create -f service.yaml</span><br><span class=\"line\">$ kubectl create -f deployment.yaml</span><br><span class=\"line\">$ kubectl get po -o wide -l type=testpage</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running   0          4m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running   0          4m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running   0          4m        172.1.14.7    192.168.72.128</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"测试脚本\"><a href=\"#测试脚本\" class=\"headerlink\" title=\"测试脚本\"></a>测试脚本</h2><p>这个脚本每一秒访问一次testpage服务，返回访问到的pod的ip和version<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; curl http://192.168.72.2:21000; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<p>监控testpage的pod变化情况<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<p>监控testpage的deployment“滚动”发布状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl rollout status deployment testpage; sleep 1; done</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"开始测试\"><a href=\"#开始测试\" class=\"headerlink\" title=\"开始测试\"></a>开始测试</h2><p>我们模拟的testpage当前版本是0.8，这时我们需要将其升级到0.9</p>\n<ol>\n<li>根据上述dockerfile大家可以直接build出0.9的镜像: dean/testpage:0.9</li>\n<li><p>修改deployment.yaml（将镜像版本和VERSION变量修改成0.9）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: extensions/v1beta1</span><br><span class=\"line\">kind: Deployment</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: testpage</span><br><span class=\"line\">  namespace: default</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    type: testpage</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  replicas: 3</span><br><span class=\"line\">  revisionHistoryLimit: 3</span><br><span class=\"line\">  strategy:</span><br><span class=\"line\">    type: RollingUpdate</span><br><span class=\"line\">    rollingUpdate:</span><br><span class=\"line\">      maxSurge: 1</span><br><span class=\"line\">      maxUnavailable: 0</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        type: testpage</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: testpage</span><br><span class=\"line\">        image: dean/testpage:0.9</span><br><span class=\"line\">        imagePullPolicy: Always</span><br><span class=\"line\">        resources:</span><br><span class=\"line\">          limits:</span><br><span class=\"line\">            cpu: 10m</span><br><span class=\"line\">            memory: 20Mi</span><br><span class=\"line\">        env:</span><br><span class=\"line\">        - name: VERSION</span><br><span class=\"line\">          value: &quot;0.9&quot;</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">        - containerPort: 8080</span><br><span class=\"line\">        readinessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 8080</span><br><span class=\"line\">        livenessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 8080</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>执行测试脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; curl http://192.168.72.2:21000; sleep 1; done</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>该脚本持续返回：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:39:09 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:10 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:11 CST 2017</span><br><span class=\"line\">I am: 172.1.14.7, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:12 CST 2017</span><br><span class=\"line\">I am: 172.1.60.5, the version is: 0.8</span><br><span class=\"line\">Thu Mar  9 10:39:13 CST 2017</span><br><span class=\"line\">I am: 172.1.62.10, the version is: 0.8</span><br></pre></td></tr></table></figure></p>\n<p>从上面的输出我们可以看到总共有三个pod对应的ip地址</p>\n<p>这里会返回三个ip，对应的三个pod，版本是0.5</p>\n<ol start=\"4\">\n<li><p>执行两个监控脚本</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">while true; do date; kubectl get po -l type=testpage -o wide; sleep 1; done</span><br><span class=\"line\">while true; do date; kubectl rollout status deployment testpage; sleep 1; done</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>开始“滚动”发布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f deployment.yaml</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>观察测试脚本和监控脚本的变化（因为都是动态的就不录制视频了）：</p>\n</li>\n</ol>\n<p>rollout status结果如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:42:24 CST 2017</span><br><span class=\"line\">deployment &quot;testpage&quot; successfully rolled out</span><br><span class=\"line\">Thu Mar  9 10:42:25 CST 2017</span><br><span class=\"line\">Waiting for deployment spec update to be observed...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class=\"line\">Waiting for rollout to finish: 1 old replicas are pending termination...</span><br><span class=\"line\">Waiting for rollout to finish: 1 old replicas are pending termination...</span><br><span class=\"line\">deployment &quot;testpage&quot; successfully rolled out</span><br></pre></td></tr></table></figure></p>\n<p>上述过程可以看到kuberentes的pod是一个一个更新的。直到最后老pod被清理掉。升级完成</p>\n<p>kubectl get po输出, 这里可以直观看出整个过程，下列的输出我是截取了关键的变化点。</p>\n<p>我在几个关键点后面写上（标注N）用于进行过程解释</p>\n<p>标注1，因为配置文件中strategy的配置，不运行kuberentes进行同位替换，所有kuberentes创建了一个新pod<br>标注2，新pod创建成功，但是因为readinessProbe的配置kubernetes需要检测到8080端口的可访问才算是READY状态，所以状态READY为0/1（这里数字的原因是一个pod中可以有多个容器）<br>标注3，新pod创建成功，并且READY处于1/1（说明一个pod中的容器全部ready）这是该新pod会被添加到service中接受访问，我们结合testpage的访问数据可以看到在43:09时新pod被访问到<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:43:09 CST 2017</span><br><span class=\"line\">I am: 172.1.65.4, the version is: 0.9</span><br></pre></td></tr></table></figure></p>\n<p>标注4，包括新pod因为有3个pod可以提供服务，所有这是kubernetes下令中止一个老pod<br>标注5，因老pod已被中止（这里是异步的），kuberentes下令开始再创建一个新pod用于进行下一个老pod的替换<br>循环这个过程直到所有老pod替换完成，升级结束</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Thu Mar  9 10:42:39 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running             0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   0/1       ContainerCreating   0          14s       &lt;none&gt;        192.168.72.190   (标注1)</span><br><span class=\"line\">Thu Mar  9 10:42:40 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Running   0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running   0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running   0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   0/1       Running   0          15s       172.1.65.4    192.168.72.190    (标注2)</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:06 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Terminating         0          7m        172.1.62.10   192.168.72.233   (标注4)</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running             0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running             0          41s       172.1.65.4    192.168.72.190   (标注3)</span><br><span class=\"line\">testpage-515849216-1sf6b   0/1       ContainerCreating   0          1s        &lt;none&gt;        192.168.72.233   (标注5)</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:18 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-fm0m1   1/1       Terminating   0          7m        172.1.62.10   192.168.72.233</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Running       0          7m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running       0          7m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          53s       172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   0/1       Running       0          13s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:36 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS              RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Terminating         0          8m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running             0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running             0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running             0          31s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   0/1       ContainerCreating   0          0s        &lt;none&gt;        192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:43:49 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-glvbz   1/1       Terminating   0          8m        172.1.60.5    192.168.72.2</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Running       0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running       0          44s       172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   0/1       Running       0          13s       172.1.60.11   192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:44:07 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS        RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-230767614-hpx9s   1/1       Terminating   0          8m        172.1.14.7    192.168.72.128</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running       0          1m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running       0          1m        172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   1/1       Running       0          31s       172.1.60.11   192.168.72.2</span><br><span class=\"line\">...</span><br><span class=\"line\">Thu Mar  9 10:47:46 CST 2017</span><br><span class=\"line\">NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class=\"line\">testpage-515849216-059b0   1/1       Running   0          5m        172.1.65.4    192.168.72.190</span><br><span class=\"line\">testpage-515849216-1sf6b   1/1       Running   0          4m        172.1.62.11   192.168.72.233</span><br><span class=\"line\">testpage-515849216-5jfb5   1/1       Running   0          4m        172.1.60.11   192.168.72.2</span><br></pre></td></tr></table></figure>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><ol>\n<li>策略配置让kubernetes必须先创建一个新pod去替换老pod（而不是删除老pod再创建新pod，这个策略可自行配置）</li>\n<li>每次仅替换一个pod，并进行业务的无缝切换</li>\n<li>整个过程readinessProbe非常重要，确定着业务是否会被中断（必须确保readinessProbe的验证是对业务的准确验证，因为测试这里仅进行端口验证）</li>\n</ol>\n<h1 id=\"kubernetes支持其他rollout操作\"><a href=\"#kubernetes支持其他rollout操作\" class=\"headerlink\" title=\"kubernetes支持其他rollout操作\"></a>kubernetes支持其他rollout操作</h1><p>Read the deployment history</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout history deployment testpage</span><br><span class=\"line\">kubectl rollout history deployment testpage --revision 42</span><br></pre></td></tr></table></figure>\n<p>Rollback to the previous deployed version</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout undo deployment testpage</span><br><span class=\"line\">kubectl rollout undo deployment testpage --to-revision 21</span><br></pre></td></tr></table></figure>\n<p>Pause and Resume</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl rollout pause deployment testpage</span><br><span class=\"line\">kubectl rollout resume deployment testpage</span><br></pre></td></tr></table></figure>\n"},{"title":"kuberntes heapster cannot write to influxdb","date":"2017-02-05T16:00:00.000Z","_content":"\nkubernetes 1.5\nheapster 1.2\ninfluxdb 1.2\n\nheapster启动后自动在influxdb中创建了k8s的数据库，但是没有任何数据写入\n\nheapster未见任何错误日志\n\ninfluxdb日志如下：\n\n```\n[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] \"POST /write?consistency=&db=k8s&precision=&rp=default HTTP/1.1\" 500 72 \"-\" \"heapster/v1.3.0-beta.0\" 2ece120b-df51-11e6-808e-000000000000 15630\n[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] \"GET /ping HTTP/1.1\" 204 0 \"-\" \"heapster/v1.3.0-beta.0\" 2ed087a5-df51-11e6-808f-000000000000 14\n```\n\n该问题为heapster和influxdb版本不兼容导致，\n\nheapster在创建k8s后默认使用default 的retention，因为influxdb1.2默认retention是autogen导致heapster无法使用到default导致数据写入失败\n\n解决办法：\n在k8s库下面创建default的retention\n\n```\ncreate retention policy \"default\" on \"k8s\" duration 0d replication 1 default\n```\n\nlink: \n[heapster issues 1474](https://github.com/kubernetes/heapster/issues/1474)\n","source":"_posts/kuberntes-heapster-cannot-write-to-influxdb.md","raw":"title: kuberntes heapster cannot write to influxdb\ndate: 2017-02-06\ntags:\n- kubernetes\n- heapster\n---\n\nkubernetes 1.5\nheapster 1.2\ninfluxdb 1.2\n\nheapster启动后自动在influxdb中创建了k8s的数据库，但是没有任何数据写入\n\nheapster未见任何错误日志\n\ninfluxdb日志如下：\n\n```\n[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] \"POST /write?consistency=&db=k8s&precision=&rp=default HTTP/1.1\" 500 72 \"-\" \"heapster/v1.3.0-beta.0\" 2ece120b-df51-11e6-808e-000000000000 15630\n[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] \"GET /ping HTTP/1.1\" 204 0 \"-\" \"heapster/v1.3.0-beta.0\" 2ed087a5-df51-11e6-808f-000000000000 14\n```\n\n该问题为heapster和influxdb版本不兼容导致，\n\nheapster在创建k8s后默认使用default 的retention，因为influxdb1.2默认retention是autogen导致heapster无法使用到default导致数据写入失败\n\n解决办法：\n在k8s库下面创建default的retention\n\n```\ncreate retention policy \"default\" on \"k8s\" duration 0d replication 1 default\n```\n\nlink: \n[heapster issues 1474](https://github.com/kubernetes/heapster/issues/1474)\n","slug":"kuberntes-heapster-cannot-write-to-influxdb","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48uf000f2tsbwf0off31","content":"<p>kubernetes 1.5<br>heapster 1.2<br>influxdb 1.2</p>\n<p>heapster启动后自动在influxdb中创建了k8s的数据库，但是没有任何数据写入</p>\n<p>heapster未见任何错误日志</p>\n<p>influxdb日志如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] &quot;POST /write?consistency=&amp;db=k8s&amp;precision=&amp;rp=default HTTP/1.1&quot; 500 72 &quot;-&quot; &quot;heapster/v1.3.0-beta.0&quot; 2ece120b-df51-11e6-808e-000000000000 15630</span><br><span class=\"line\">[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] &quot;GET /ping HTTP/1.1&quot; 204 0 &quot;-&quot; &quot;heapster/v1.3.0-beta.0&quot; 2ed087a5-df51-11e6-808f-000000000000 14</span><br></pre></td></tr></table></figure>\n<p>该问题为heapster和influxdb版本不兼容导致，</p>\n<p>heapster在创建k8s后默认使用default 的retention，因为influxdb1.2默认retention是autogen导致heapster无法使用到default导致数据写入失败</p>\n<p>解决办法：<br>在k8s库下面创建default的retention</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create retention policy &quot;default&quot; on &quot;k8s&quot; duration 0d replication 1 default</span><br></pre></td></tr></table></figure>\n<p>link:<br><a href=\"https://github.com/kubernetes/heapster/issues/1474\" target=\"_blank\" rel=\"noopener\">heapster issues 1474</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>kubernetes 1.5<br>heapster 1.2<br>influxdb 1.2</p>\n<p>heapster启动后自动在influxdb中创建了k8s的数据库，但是没有任何数据写入</p>\n<p>heapster未见任何错误日志</p>\n<p>influxdb日志如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] &quot;POST /write?consistency=&amp;db=k8s&amp;precision=&amp;rp=default HTTP/1.1&quot; 500 72 &quot;-&quot; &quot;heapster/v1.3.0-beta.0&quot; 2ece120b-df51-11e6-808e-000000000000 15630</span><br><span class=\"line\">[httpd] 192.168.56.207 - root [20/Jan/2017:20:44:05 +0000] &quot;GET /ping HTTP/1.1&quot; 204 0 &quot;-&quot; &quot;heapster/v1.3.0-beta.0&quot; 2ed087a5-df51-11e6-808f-000000000000 14</span><br></pre></td></tr></table></figure>\n<p>该问题为heapster和influxdb版本不兼容导致，</p>\n<p>heapster在创建k8s后默认使用default 的retention，因为influxdb1.2默认retention是autogen导致heapster无法使用到default导致数据写入失败</p>\n<p>解决办法：<br>在k8s库下面创建default的retention</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create retention policy &quot;default&quot; on &quot;k8s&quot; duration 0d replication 1 default</span><br></pre></td></tr></table></figure>\n<p>link:<br><a href=\"https://github.com/kubernetes/heapster/issues/1474\" target=\"_blank\" rel=\"noopener\">heapster issues 1474</a></p>\n"},{"title":"Tuning The Linux Connection Tracking System","date":"2017-05-15T16:00:00.000Z","_content":"\n我们将大量的api部署在k8s上，有一次一个app后端的服务器进行了重启引起几十万的连接到api上重新验证，导致k8s的ConntrackTable满的告警，分析发现node-exporter(Prometheus node-exporter)监控的node_nf_conntrack_entries / node_nf_conntrack_entries_limit满。既然告警了就应该了解下这个告警是什么意思。\n\n下面是google到的内容，看看Conntract table做什么的：\n\n# The Connection Tracking/Conntrack Modules\n\nIt is a tracking technique of the connections. It is used to know how the packets that pass through the system are related to their connections. The connection tracking does NOT manipulate the packets and It works independently of the NAT module. The conntrack entry looks like:\n\nudp 17 170 src=192.168.1.2 dst=192.168.1.5 sport=137 dport=1025 src=192.168.1.5 dst=192.168.1.2 sport=1025 dport=137 [ASSURED] use=1\n\nThe conntrack entry is stored into two separate tuples (one for the original direction (red) and another for the reply direction (blue)). Tuples could belong to different linked lists/buckets in conntrack hash table. The connection tracking modules is responsible for creating and removing the tuples.\n\nNote: The tracking of the connections is ALSO used by iptables to do packet matching based on the connection state.\n\n# 验证\n\n要查询Conntract table的内容可以通过/proc\n\n```\n# cat /proc/net/nf_conntrack | head -5\nipv4     2 tcp      6 86385 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=41230 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=41230 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86389 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=33294 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=33294 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86398 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=55688 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=55688 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86393 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=57118 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=57118 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86386 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=39722 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=39722 [ASSURED] mark=0 zone=0 use=2\n```\n\n从上面的内容看出nf_conntrack记录的是ip和port的映射情况\n\n\n统计nf_conntrack_entries可以通过wc\n\n```\ncat /proc/net/nf_conntrack | wc -l\n```\n\n因为k8s的proxy目前采用的是iptables，所以必然需要存放大量的nf_conntrack链接信息，如果k8s节点的通讯量太大就可能导致nf_conntrack表的信息满，所以这个参数应该是k8s中相对比较重要的一个监控指标\n\n\n查看nf_conntrack table的限制\n\n```\n# sysctl -a|grep -i nf_conntrack_max\nnet.netfilter.nf_conntrack_max = 262144\nnet.nf_conntrack_max = 262144\n```\n\nconntrack bucket\n\n```\n# sysctl -a|grep -i nf_conntrack_buckets\nnet.netfilter.nf_conntrack_buckets = 65536\n```\n\nbucket size = 4 (262144/65536)\n\n\n\n修改max\n\n```\necho \"net.netfilter.nf_conntrack_max = 131072\" >> /etc/sysctl.conf\nsysct -p\n```\n\n其他配置参数：\n\n```\n# sysctl -a|grep -i nf_conntrack\nnet.netfilter.nf_conntrack_acct = 0\nnet.netfilter.nf_conntrack_buckets = 65536\nnet.netfilter.nf_conntrack_checksum = 1\nnet.netfilter.nf_conntrack_count = 431\nnet.netfilter.nf_conntrack_events = 1\nnet.netfilter.nf_conntrack_events_retry_timeout = 15\nnet.netfilter.nf_conntrack_expect_max = 1024\nnet.netfilter.nf_conntrack_frag6_high_thresh = 4194304\nnet.netfilter.nf_conntrack_frag6_low_thresh = 3145728\nnet.netfilter.nf_conntrack_frag6_timeout = 60\nnet.netfilter.nf_conntrack_generic_timeout = 600\nnet.netfilter.nf_conntrack_helper = 1\nnet.netfilter.nf_conntrack_icmp_timeout = 30\nnet.netfilter.nf_conntrack_icmpv6_timeout = 30\nnet.netfilter.nf_conntrack_log_invalid = 0\nnet.netfilter.nf_conntrack_max = 262144\nnet.netfilter.nf_conntrack_tcp_be_liberal = 0\nnet.netfilter.nf_conntrack_tcp_loose = 1\nnet.netfilter.nf_conntrack_tcp_max_retrans = 3\nnet.netfilter.nf_conntrack_tcp_timeout_close = 10\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600\nnet.netfilter.nf_conntrack_tcp_timeout_established = 86400\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_last_ack = 30\nnet.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300\nnet.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60\nnet.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300\nnet.netfilter.nf_conntrack_timestamp = 0\nnet.netfilter.nf_conntrack_udp_timeout = 30\nnet.netfilter.nf_conntrack_udp_timeout_stream = 180\nnet.nf_conntrack_max = 262144\n```\n\n\nref:\n[https://voipmagazine.wordpress.com/tag/nf_conntrack/](https://voipmagazine.wordpress.com/tag/nf_conntrack/)\n[https://voipmagazine.wordpress.com/tag/conntrack-entry/](https://voipmagazine.wordpress.com/tag/conntrack-entry/)\n","source":"_posts/tuning_conntrack.md","raw":"title:  Tuning The Linux Connection Tracking System\ndate: 2017-05-16\ntags:\n- kubernetes\n- iptables\n- conntrack\n---\n\n我们将大量的api部署在k8s上，有一次一个app后端的服务器进行了重启引起几十万的连接到api上重新验证，导致k8s的ConntrackTable满的告警，分析发现node-exporter(Prometheus node-exporter)监控的node_nf_conntrack_entries / node_nf_conntrack_entries_limit满。既然告警了就应该了解下这个告警是什么意思。\n\n下面是google到的内容，看看Conntract table做什么的：\n\n# The Connection Tracking/Conntrack Modules\n\nIt is a tracking technique of the connections. It is used to know how the packets that pass through the system are related to their connections. The connection tracking does NOT manipulate the packets and It works independently of the NAT module. The conntrack entry looks like:\n\nudp 17 170 src=192.168.1.2 dst=192.168.1.5 sport=137 dport=1025 src=192.168.1.5 dst=192.168.1.2 sport=1025 dport=137 [ASSURED] use=1\n\nThe conntrack entry is stored into two separate tuples (one for the original direction (red) and another for the reply direction (blue)). Tuples could belong to different linked lists/buckets in conntrack hash table. The connection tracking modules is responsible for creating and removing the tuples.\n\nNote: The tracking of the connections is ALSO used by iptables to do packet matching based on the connection state.\n\n# 验证\n\n要查询Conntract table的内容可以通过/proc\n\n```\n# cat /proc/net/nf_conntrack | head -5\nipv4     2 tcp      6 86385 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=41230 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=41230 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86389 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=33294 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=33294 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86398 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=55688 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=55688 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86393 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=57118 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=57118 [ASSURED] mark=0 zone=0 use=2\nipv4     2 tcp      6 86386 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=39722 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=39722 [ASSURED] mark=0 zone=0 use=2\n```\n\n从上面的内容看出nf_conntrack记录的是ip和port的映射情况\n\n\n统计nf_conntrack_entries可以通过wc\n\n```\ncat /proc/net/nf_conntrack | wc -l\n```\n\n因为k8s的proxy目前采用的是iptables，所以必然需要存放大量的nf_conntrack链接信息，如果k8s节点的通讯量太大就可能导致nf_conntrack表的信息满，所以这个参数应该是k8s中相对比较重要的一个监控指标\n\n\n查看nf_conntrack table的限制\n\n```\n# sysctl -a|grep -i nf_conntrack_max\nnet.netfilter.nf_conntrack_max = 262144\nnet.nf_conntrack_max = 262144\n```\n\nconntrack bucket\n\n```\n# sysctl -a|grep -i nf_conntrack_buckets\nnet.netfilter.nf_conntrack_buckets = 65536\n```\n\nbucket size = 4 (262144/65536)\n\n\n\n修改max\n\n```\necho \"net.netfilter.nf_conntrack_max = 131072\" >> /etc/sysctl.conf\nsysct -p\n```\n\n其他配置参数：\n\n```\n# sysctl -a|grep -i nf_conntrack\nnet.netfilter.nf_conntrack_acct = 0\nnet.netfilter.nf_conntrack_buckets = 65536\nnet.netfilter.nf_conntrack_checksum = 1\nnet.netfilter.nf_conntrack_count = 431\nnet.netfilter.nf_conntrack_events = 1\nnet.netfilter.nf_conntrack_events_retry_timeout = 15\nnet.netfilter.nf_conntrack_expect_max = 1024\nnet.netfilter.nf_conntrack_frag6_high_thresh = 4194304\nnet.netfilter.nf_conntrack_frag6_low_thresh = 3145728\nnet.netfilter.nf_conntrack_frag6_timeout = 60\nnet.netfilter.nf_conntrack_generic_timeout = 600\nnet.netfilter.nf_conntrack_helper = 1\nnet.netfilter.nf_conntrack_icmp_timeout = 30\nnet.netfilter.nf_conntrack_icmpv6_timeout = 30\nnet.netfilter.nf_conntrack_log_invalid = 0\nnet.netfilter.nf_conntrack_max = 262144\nnet.netfilter.nf_conntrack_tcp_be_liberal = 0\nnet.netfilter.nf_conntrack_tcp_loose = 1\nnet.netfilter.nf_conntrack_tcp_max_retrans = 3\nnet.netfilter.nf_conntrack_tcp_timeout_close = 10\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600\nnet.netfilter.nf_conntrack_tcp_timeout_established = 86400\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_last_ack = 30\nnet.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300\nnet.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60\nnet.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300\nnet.netfilter.nf_conntrack_timestamp = 0\nnet.netfilter.nf_conntrack_udp_timeout = 30\nnet.netfilter.nf_conntrack_udp_timeout_stream = 180\nnet.nf_conntrack_max = 262144\n```\n\n\nref:\n[https://voipmagazine.wordpress.com/tag/nf_conntrack/](https://voipmagazine.wordpress.com/tag/nf_conntrack/)\n[https://voipmagazine.wordpress.com/tag/conntrack-entry/](https://voipmagazine.wordpress.com/tag/conntrack-entry/)\n","slug":"tuning_conntrack","published":1,"updated":"2018-06-05T15:07:18.002Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cji1u48ui000i2tsbve6mlfzh","content":"<p>我们将大量的api部署在k8s上，有一次一个app后端的服务器进行了重启引起几十万的连接到api上重新验证，导致k8s的ConntrackTable满的告警，分析发现node-exporter(Prometheus node-exporter)监控的node_nf_conntrack_entries / node_nf_conntrack_entries_limit满。既然告警了就应该了解下这个告警是什么意思。</p>\n<p>下面是google到的内容，看看Conntract table做什么的：</p>\n<h1 id=\"The-Connection-Tracking-Conntrack-Modules\"><a href=\"#The-Connection-Tracking-Conntrack-Modules\" class=\"headerlink\" title=\"The Connection Tracking/Conntrack Modules\"></a>The Connection Tracking/Conntrack Modules</h1><p>It is a tracking technique of the connections. It is used to know how the packets that pass through the system are related to their connections. The connection tracking does NOT manipulate the packets and It works independently of the NAT module. The conntrack entry looks like:</p>\n<p>udp 17 170 src=192.168.1.2 dst=192.168.1.5 sport=137 dport=1025 src=192.168.1.5 dst=192.168.1.2 sport=1025 dport=137 [ASSURED] use=1</p>\n<p>The conntrack entry is stored into two separate tuples (one for the original direction (red) and another for the reply direction (blue)). Tuples could belong to different linked lists/buckets in conntrack hash table. The connection tracking modules is responsible for creating and removing the tuples.</p>\n<p>Note: The tracking of the connections is ALSO used by iptables to do packet matching based on the connection state.</p>\n<h1 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h1><p>要查询Conntract table的内容可以通过/proc</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># cat /proc/net/nf_conntrack | head -5</span><br><span class=\"line\">ipv4     2 tcp      6 86385 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=41230 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=41230 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86389 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=33294 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=33294 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86398 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=55688 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=55688 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86393 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=57118 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=57118 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86386 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=39722 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=39722 [ASSURED] mark=0 zone=0 use=2</span><br></pre></td></tr></table></figure>\n<p>从上面的内容看出nf_conntrack记录的是ip和port的映射情况</p>\n<p>统计nf_conntrack_entries可以通过wc</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /proc/net/nf_conntrack | wc -l</span><br></pre></td></tr></table></figure>\n<p>因为k8s的proxy目前采用的是iptables，所以必然需要存放大量的nf_conntrack链接信息，如果k8s节点的通讯量太大就可能导致nf_conntrack表的信息满，所以这个参数应该是k8s中相对比较重要的一个监控指标</p>\n<p>查看nf_conntrack table的限制</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack_max</span><br><span class=\"line\">net.netfilter.nf_conntrack_max = 262144</span><br><span class=\"line\">net.nf_conntrack_max = 262144</span><br></pre></td></tr></table></figure>\n<p>conntrack bucket</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack_buckets</span><br><span class=\"line\">net.netfilter.nf_conntrack_buckets = 65536</span><br></pre></td></tr></table></figure>\n<p>bucket size = 4 (262144/65536)</p>\n<p>修改max</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;net.netfilter.nf_conntrack_max = 131072&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">sysct -p</span><br></pre></td></tr></table></figure>\n<p>其他配置参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack</span><br><span class=\"line\">net.netfilter.nf_conntrack_acct = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_buckets = 65536</span><br><span class=\"line\">net.netfilter.nf_conntrack_checksum = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_count = 431</span><br><span class=\"line\">net.netfilter.nf_conntrack_events = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_events_retry_timeout = 15</span><br><span class=\"line\">net.netfilter.nf_conntrack_expect_max = 1024</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_high_thresh = 4194304</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_low_thresh = 3145728</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_timeout = 60</span><br><span class=\"line\">net.netfilter.nf_conntrack_generic_timeout = 600</span><br><span class=\"line\">net.netfilter.nf_conntrack_helper = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_icmp_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_icmpv6_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_log_invalid = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_max = 262144</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_be_liberal = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_loose = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_max_retrans = 3</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_close = 10</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_established = 86400</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300</span><br><span class=\"line\">net.netfilter.nf_conntrack_timestamp = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_udp_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_udp_timeout_stream = 180</span><br><span class=\"line\">net.nf_conntrack_max = 262144</span><br></pre></td></tr></table></figure>\n<p>ref:<br><a href=\"https://voipmagazine.wordpress.com/tag/nf_conntrack/\" target=\"_blank\" rel=\"noopener\">https://voipmagazine.wordpress.com/tag/nf_conntrack/</a><br><a href=\"https://voipmagazine.wordpress.com/tag/conntrack-entry/\" target=\"_blank\" rel=\"noopener\">https://voipmagazine.wordpress.com/tag/conntrack-entry/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>我们将大量的api部署在k8s上，有一次一个app后端的服务器进行了重启引起几十万的连接到api上重新验证，导致k8s的ConntrackTable满的告警，分析发现node-exporter(Prometheus node-exporter)监控的node_nf_conntrack_entries / node_nf_conntrack_entries_limit满。既然告警了就应该了解下这个告警是什么意思。</p>\n<p>下面是google到的内容，看看Conntract table做什么的：</p>\n<h1 id=\"The-Connection-Tracking-Conntrack-Modules\"><a href=\"#The-Connection-Tracking-Conntrack-Modules\" class=\"headerlink\" title=\"The Connection Tracking/Conntrack Modules\"></a>The Connection Tracking/Conntrack Modules</h1><p>It is a tracking technique of the connections. It is used to know how the packets that pass through the system are related to their connections. The connection tracking does NOT manipulate the packets and It works independently of the NAT module. The conntrack entry looks like:</p>\n<p>udp 17 170 src=192.168.1.2 dst=192.168.1.5 sport=137 dport=1025 src=192.168.1.5 dst=192.168.1.2 sport=1025 dport=137 [ASSURED] use=1</p>\n<p>The conntrack entry is stored into two separate tuples (one for the original direction (red) and another for the reply direction (blue)). Tuples could belong to different linked lists/buckets in conntrack hash table. The connection tracking modules is responsible for creating and removing the tuples.</p>\n<p>Note: The tracking of the connections is ALSO used by iptables to do packet matching based on the connection state.</p>\n<h1 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h1><p>要查询Conntract table的内容可以通过/proc</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># cat /proc/net/nf_conntrack | head -5</span><br><span class=\"line\">ipv4     2 tcp      6 86385 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=41230 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=41230 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86389 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=33294 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=33294 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86398 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=55688 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=55688 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86393 ESTABLISHED src=192.168.10.1 dst=192.168.10.1 sport=57118 dport=2379 src=192.168.10.1 dst=192.168.10.1 sport=2379 dport=57118 [ASSURED] mark=0 zone=0 use=2</span><br><span class=\"line\">ipv4     2 tcp      6 86386 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=39722 dport=8080 src=127.0.0.1 dst=127.0.0.1 sport=8080 dport=39722 [ASSURED] mark=0 zone=0 use=2</span><br></pre></td></tr></table></figure>\n<p>从上面的内容看出nf_conntrack记录的是ip和port的映射情况</p>\n<p>统计nf_conntrack_entries可以通过wc</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /proc/net/nf_conntrack | wc -l</span><br></pre></td></tr></table></figure>\n<p>因为k8s的proxy目前采用的是iptables，所以必然需要存放大量的nf_conntrack链接信息，如果k8s节点的通讯量太大就可能导致nf_conntrack表的信息满，所以这个参数应该是k8s中相对比较重要的一个监控指标</p>\n<p>查看nf_conntrack table的限制</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack_max</span><br><span class=\"line\">net.netfilter.nf_conntrack_max = 262144</span><br><span class=\"line\">net.nf_conntrack_max = 262144</span><br></pre></td></tr></table></figure>\n<p>conntrack bucket</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack_buckets</span><br><span class=\"line\">net.netfilter.nf_conntrack_buckets = 65536</span><br></pre></td></tr></table></figure>\n<p>bucket size = 4 (262144/65536)</p>\n<p>修改max</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;net.netfilter.nf_conntrack_max = 131072&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">sysct -p</span><br></pre></td></tr></table></figure>\n<p>其他配置参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># sysctl -a|grep -i nf_conntrack</span><br><span class=\"line\">net.netfilter.nf_conntrack_acct = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_buckets = 65536</span><br><span class=\"line\">net.netfilter.nf_conntrack_checksum = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_count = 431</span><br><span class=\"line\">net.netfilter.nf_conntrack_events = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_events_retry_timeout = 15</span><br><span class=\"line\">net.netfilter.nf_conntrack_expect_max = 1024</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_high_thresh = 4194304</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_low_thresh = 3145728</span><br><span class=\"line\">net.netfilter.nf_conntrack_frag6_timeout = 60</span><br><span class=\"line\">net.netfilter.nf_conntrack_generic_timeout = 600</span><br><span class=\"line\">net.netfilter.nf_conntrack_helper = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_icmp_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_icmpv6_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_log_invalid = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_max = 262144</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_be_liberal = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_loose = 1</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_max_retrans = 3</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_close = 10</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_established = 86400</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120</span><br><span class=\"line\">net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300</span><br><span class=\"line\">net.netfilter.nf_conntrack_timestamp = 0</span><br><span class=\"line\">net.netfilter.nf_conntrack_udp_timeout = 30</span><br><span class=\"line\">net.netfilter.nf_conntrack_udp_timeout_stream = 180</span><br><span class=\"line\">net.nf_conntrack_max = 262144</span><br></pre></td></tr></table></figure>\n<p>ref:<br><a href=\"https://voipmagazine.wordpress.com/tag/nf_conntrack/\" target=\"_blank\" rel=\"noopener\">https://voipmagazine.wordpress.com/tag/nf_conntrack/</a><br><a href=\"https://voipmagazine.wordpress.com/tag/conntrack-entry/\" target=\"_blank\" rel=\"noopener\">https://voipmagazine.wordpress.com/tag/conntrack-entry/</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cji1u48to00002tsb4eia7l0d","tag_id":"cji1u48tx00022tsb1wkdz0oj","_id":"cji1u48ue000e2tsbz5wpfoot"},{"post_id":"cji1u48to00002tsb4eia7l0d","tag_id":"cji1u48u400062tsbsczm3tuk","_id":"cji1u48uh000g2tsblydziaf3"},{"post_id":"cji1u48to00002tsb4eia7l0d","tag_id":"cji1u48u700092tsbq7q7oc1u","_id":"cji1u48uj000j2tsba4mm2y7l"},{"post_id":"cji1u48tv00012tsbrxvyae6x","tag_id":"cji1u48ua000c2tsbytec9m5o","_id":"cji1u48um000m2tsbz71sybyl"},{"post_id":"cji1u48tv00012tsbrxvyae6x","tag_id":"cji1u48uh000h2tsb8sbh5nce","_id":"cji1u48um000n2tsb4loz1u9r"},{"post_id":"cji1u48tv00012tsbrxvyae6x","tag_id":"cji1u48ul000k2tsbr6k1tqx2","_id":"cji1u48um000p2tsbhytkfnqe"},{"post_id":"cji1u48u000032tsb0k880g86","tag_id":"cji1u48ul000l2tsbac8ffhq1","_id":"cji1u48un000r2tsbehpiox1d"},{"post_id":"cji1u48u000032tsb0k880g86","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48un000s2tsbl3mmvx5p"},{"post_id":"cji1u48u200042tsb8bnmcq5u","tag_id":"cji1u48un000q2tsblfn4p49c","_id":"cji1u48uq000v2tsb2umpaqa4"},{"post_id":"cji1u48u200042tsb8bnmcq5u","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48uq000w2tsb5h2bvukq"},{"post_id":"cji1u48u300052tsb73xcaosx","tag_id":"cji1u48ul000l2tsbac8ffhq1","_id":"cji1u48ur000y2tsb2rrfk8mx"},{"post_id":"cji1u48u500072tsbu7ag8s9a","tag_id":"cji1u48ul000l2tsbac8ffhq1","_id":"cji1u48ut00102tsbuh1bkxoh"},{"post_id":"cji1u48u600082tsb6yofe24b","tag_id":"cji1u48ul000l2tsbac8ffhq1","_id":"cji1u48uv00122tsbl190od9q"},{"post_id":"cji1u48u8000a2tsby4nxiect","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48uw00162tsbjaj64l6m"},{"post_id":"cji1u48u8000a2tsby4nxiect","tag_id":"cji1u48uv00132tsba2glwhhx","_id":"cji1u48uw00172tsb2ius4gk5"},{"post_id":"cji1u48u8000a2tsby4nxiect","tag_id":"cji1u48uv00142tsb6jguo7pr","_id":"cji1u48uw00192tsbys2ae2ym"},{"post_id":"cji1u48u9000b2tsbz1afm1z0","tag_id":"cji1u48uw00152tsbivx1xoa4","_id":"cji1u48ux001a2tsb8wdi6zt1"},{"post_id":"cji1u48ub000d2tsbo2cd9kx2","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48ux001d2tsbm0ae0kxi"},{"post_id":"cji1u48ub000d2tsbo2cd9kx2","tag_id":"cji1u48ux001b2tsbea4voh7i","_id":"cji1u48uy001e2tsbsnsvawmm"},{"post_id":"cji1u48uf000f2tsbwf0off31","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48uy001h2tsbonhqx6ih"},{"post_id":"cji1u48uf000f2tsbwf0off31","tag_id":"cji1u48uy001f2tsbbj2wcjhu","_id":"cji1u48uy001i2tsbzzc07i4w"},{"post_id":"cji1u48ui000i2tsbve6mlfzh","tag_id":"cji1u48um000o2tsbjnleloen","_id":"cji1u48uz001l2tsbzsod2chf"},{"post_id":"cji1u48ui000i2tsbve6mlfzh","tag_id":"cji1u48uy001j2tsbirpyq1e1","_id":"cji1u48v0001m2tsb86rh9cpd"},{"post_id":"cji1u48ui000i2tsbve6mlfzh","tag_id":"cji1u48uz001k2tsbaehpjb60","_id":"cji1u48v0001n2tsbvpoe62wx"}],"Tag":[{"name":"arch","_id":"cji1u48tx00022tsb1wkdz0oj"},{"name":"linux","_id":"cji1u48u400062tsbsczm3tuk"},{"name":"hibernate","_id":"cji1u48u700092tsbq7q7oc1u"},{"name":"github","_id":"cji1u48ua000c2tsbytec9m5o"},{"name":"hexo","_id":"cji1u48uh000h2tsb8sbh5nce"},{"name":"blog","_id":"cji1u48ul000k2tsbr6k1tqx2"},{"name":"docker","_id":"cji1u48ul000l2tsbac8ffhq1"},{"name":"kubernetes","_id":"cji1u48um000o2tsbjnleloen"},{"name":"traefik","_id":"cji1u48un000q2tsblfn4p49c"},{"name":"harbor","_id":"cji1u48uv00132tsba2glwhhx"},{"name":"registry","_id":"cji1u48uv00142tsb6jguo7pr"},{"name":"markdown","_id":"cji1u48uw00152tsbivx1xoa4"},{"name":"rollout","_id":"cji1u48ux001b2tsbea4voh7i"},{"name":"heapster","_id":"cji1u48uy001f2tsbbj2wcjhu"},{"name":"iptables","_id":"cji1u48uy001j2tsbirpyq1e1"},{"name":"conntrack","_id":"cji1u48uz001k2tsbaehpjb60"}]}}